model:
  model_name: deepseek-ai/deepseek-coder-1.3b-base
  dtype: bfloat16                 # [bfloat16, float16, float32]
  fp32_matmul_precision: high     # [highest, high, medium]
  attention: flash_attention_2    # [flash_attention_2, sdpa, eager]

dataset:
  path: JetBrains-Research/lca-codegen-train
  data_dir: train
  split: train

chunk:
  start: 10500
  stop: 11200

preprocessor:
  max_seq_len: 16384
  context_tokens: 8192
  loss_ratio: 1
