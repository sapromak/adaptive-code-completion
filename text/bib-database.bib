% TODO: remove unused references
% TODO: check the consistency of the citation appearances

@misc{izadi2024,
  title         = {Language Models for Code Completion: A Practical Evaluation},
  author        = {Maliheh Izadi and Jonathan Katzy and Tim van Dam and Marc Otten and Razvan Mihai Popescu and Arie van Deursen},
  year          = {2024},
  eprint        = {2402.16197},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2402.16197}
}

@inproceedings{izadi2022,
  series     = {ICSE ’22},
  title      = {CodeFill: multi-token code completion by jointly learning from structure and naming sequences},
  url        = {http://dx.doi.org/10.1145/3510003.3510172},
  doi        = {10.1145/3510003.3510172},
  booktitle  = {Proceedings of the 44th International Conference on Software Engineering},
  publisher  = {ACM},
  author     = {Izadi, Maliheh and Gismondi, Roberta and Gousios, Georgios},
  year       = {2022},
  month      = may,
  collection = {ICSE ’22}
}

@misc{lu2021,
  title         = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  author        = {Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
  year          = {2021},
  eprint        = {2102.04664},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2102.04664}
}

@article{ciniselli2021,
  title     = {An Empirical Study on the Usage of Transformer Models for Code Completion},
  issn      = {2326-3881},
  url       = {http://dx.doi.org/10.1109/TSE.2021.3128234},
  doi       = {10.1109/tse.2021.3128234},
  journal   = {IEEE Transactions on Software Engineering},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Ciniselli, Matteo and Cooper, Nathan and Pascarella, Luca and Mastropaolo, Antonio and Aghajani, Emad and Poshyvanyk, Denys and Di Penta, Massimiliano and Bavota, Gabriele},
  year      = {2021},
  pages     = {1–1}
}

@misc{wang2021,
  title         = {Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs},
  author        = {Yanlin Wang and Hui Li},
  year          = {2021},
  eprint        = {2103.09499},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2103.09499}
}

@article{husein2025,
  title    = {Large language models for code completion: A systematic literature review},
  journal  = {Computer Standards & Interfaces},
  volume   = {92},
  pages    = {103917},
  year     = {2025},
  issn     = {0920-5489},
  doi      = {https://doi.org/10.1016/j.csi.2024.103917},
  url      = {https://www.sciencedirect.com/science/article/pii/S0920548924000862},
  author   = {Rasha Ahmad Husein and Hala Aburajouh and Cagatay Catal},
  keywords = {Code completion, Large language models, Deep learning, Transformers},
  abstract = {Code completion serves as a fundamental aspect of modern software development, improving developers' coding processes. Integrating code completion tools into an Integrated Development Environment (IDE) or code editor enhances the coding process and boosts productivity by reducing errors and speeding up code writing while reducing cognitive load. This is achieved by predicting subsequent tokens, such as keywords, variable names, types, function names, operators, and more. Different techniques can achieve code completion, and recent research has focused on Deep Learning methods, particularly Large Language Models (LLMs) utilizing Transformer algorithms. While several research papers have focused on the use of LLMs for code completion, these studies are fragmented, and there is no systematic overview of the use of LLMs for code completion. Therefore, we aimed to perform a Systematic Literature Review (SLR) study to investigate how LLMs have been applied for code completion so far. We have formulated several research questions to address how LLMs have been integrated for code completion-related tasks and to assess the efficacy of these LLMs in the context of code completion. To achieve this, we retrieved 244 papers from scientific databases using auto-search and specific keywords, finally selecting 23 primary studies based on an SLR methodology for in-depth analysis. This SLR study categorizes the granularity levels of code completion achieved by utilizing LLMs in IDEs, explores the existing issues in current code completion systems, how LLMs address these challenges, and the pre-training and fine-tuning methods employed. Additionally, this study identifies open research problems and outlines future research directions. Our analysis reveals that LLMs significantly enhance code completion performance across several programming languages and contexts, and their capability to predict relevant code snippets based on context and partial input boosts developer productivity substantially.}
}

@inproceedings{hindle2012,
  author    = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
  title     = {On the naturalness of software},
  year      = {2012},
  isbn      = {9781467310673},
  publisher = {IEEE Press},
  abstract  = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations---and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's completion capability. We conclude the paper by laying out a vision for future research in this area.},
  booktitle = {Proceedings of the 34th International Conference on Software Engineering},
  pages     = {837–847},
  numpages  = {11},
  location  = {Zurich, Switzerland},
  series    = {ICSE '12}
}

@misc{bhoopchand2016,
      title={Learning Python Code Suggestion with a Sparse Pointer Network}, 
      author={Avishkar Bhoopchand and Tim Rocktäschel and Earl Barr and Sebastian Riedel},
      year={2016},
      eprint={1611.08307},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1611.08307}, 
}

@misc{peng2023,
      title={The Impact of AI on Developer Productivity: Evidence from GitHub Copilot}, 
      author={Sida Peng and Eirini Kalliamvakou and Peter Cihon and Mert Demirer},
      year={2023},
      eprint={2302.06590},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2302.06590}, 
}

@article{weber2024,
  author     = {Weber, Thomas and Brandmaier, Maximilian and Schmidt, Albrecht and Mayer, Sven},
  title      = {Significant Productivity Gains through Programming with Large Language Models},
  year       = {2024},
  issue_date = {June 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {8},
  number     = {EICS},
  url        = {https://doi.org/10.1145/3661145},
  doi        = {10.1145/3661145},
  abstract   = {Large language models like GPT and Codex drastically alter many daily tasks, including programming, where they can rapidly generate code from natural language or informal specifications. Thus, they will change what it means to be a programmer and how programmers act during software development. This work explores how AI assistance for code generation impacts productivity. In our user study (N=24), we asked programmers to complete Python programming tasks supported by a) an auto-complete interface using GitHub Copilot, b) a conversational system using GPT-3, and c) traditionally with just the web browser. Aside from significantly increasing productivity metrics, participants displayed distinctive usage patterns and strategies, highlighting that the form of presentation and interaction affects how users engage with these systems. Our findings emphasize the benefits of AI-assisted coding and highlight the different design challenges for these systems.},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  month      = jun,
  articleno  = {256},
  numpages   = {29},
  keywords   = {github copilot, gpt, language models, programming, software development, user study}
}

@misc{bakal2025,
      title={Experience with GitHub Copilot for Developer Productivity at Zoominfo}, 
      author={Gal Bakal and Ali Dasdan and Yaniv Katz and Michael Kaufman and Guy Levin},
      year={2025},
      eprint={2501.13282},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2501.13282}, 
}

@misc{takerngsaksiri2024,
      title={Students' Perspective on AI Code Completion: Benefits and Challenges}, 
      author={Wannita Takerngsaksiri and Cleshan Warusavitarne and Christian Yaacoub and Matthew Hee Keng Hou and Chakkrit Tantithamthavorn},
      year={2024},
      eprint={2311.00177},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2311.00177}, 
}

@misc{giagnorio2025,
      title={Why Personalizing Deep Learning-Based Code Completion Tools Matters}, 
      author={Alessandro Giagnorio and Alberto Martin-Lopez and Gabriele Bavota},
      year={2025},
      eprint={2503.14201},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2503.14201}, 
}

@inproceedings{mandelin2005,
  author    = {Mandelin, David and Xu, Lin and Bod\'{\i}k, Rastislav and Kimelman, Doug},
  title     = {Jungloid mining: helping to navigate the API jungle},
  year      = {2005},
  isbn      = {1595930566},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1065010.1065018},
  doi       = {10.1145/1065010.1065018},
  abstract  = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined from a corpus of sample client programs.We implemented a tool, prospector, based on these techniques. prospector is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested prospector on a set of real programming problems involving APIs; prospector found the desired solution for 18 of 20 problems. We also evaluated prospector in a user study, finding that programmers solved programming problems more quickly and with more reuse when using prospector than without prospector.},
  booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {48–61},
  numpages  = {14},
  keywords  = {mining, program synthesis, reuse},
  location  = {Chicago, IL, USA},
  series    = {PLDI '05}
}

@inproceedings{hill2004,
  author    = {Hill, Rosco and Rideout, Joe},
  title     = {Automatic Method Completion},
  year      = {2004},
  isbn      = {0769521312},
  publisher = {IEEE Computer Society},
  address   = {USA},
  abstract  = {Modern software development environments include tools to help programmers write code efficiently and accurately. For example many integrated development environments include variable name completion, method name completion and recently refactoring tools have been added to some environments. This paper extends the idea of automatic completion to include completion of the body of a method by employing machine learning algorithms on the near duplicate code segments that frequently exist in large software projects.},
  booktitle = {Proceedings of the 19th IEEE International Conference on Automated Software Engineering},
  pages     = {228–235},
  numpages  = {8},
  series    = {ASE '04}
}

@inproceedings{han2009,
  author    = {Han, Sangmok and Wallace, David R. and Miller, Robert C.},
  booktitle = {2009 IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Code Completion from Abbreviated Input},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {332-343},
  keywords  = {Hidden Markov models;Lead;Programming profession;Acceleration;Software engineering;Feedback;User interfaces;System testing;Sampling methods;Data mining;Code Completion;Hidden Markov Model;Abbreviation;Multiple Keywords;Code Assistants;Data Mining},
  doi       = {10.1109/ASE.2009.64}
}

@inproceedings{raychev2014,
  author    = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
  title     = {Code completion with statistical language models},
  year      = {2014},
  isbn      = {9781450327848},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2594291.2594321},
  doi       = {10.1145/2594291.2594321},
  abstract  = {We address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls.Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments.Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90\% of the cases.},
  booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {419–428},
  numpages  = {10},
  location  = {Edinburgh, United Kingdom},
  series    = {PLDI '14}
}

@inproceedings{asaduzzaman2014,
  author    = {Asaduzzaman, Muhammad and Roy, Chanchal K. and Schneider, Kevin A. and Hou, Daqing},
  booktitle = {2014 IEEE International Conference on Software Maintenance and Evolution},
  title     = {Context-Sensitive Code Completion Tool for Better API Usability},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {621-624},
  keywords  = {Context;Proposals;Java;Context modeling;Receivers;Libraries;Databases;API methods;Code Completion;Eclipse plugin},
  doi       = {10.1109/ICSME.2014.110}
}

@article{proksch2015,
  author     = {Proksch, Sebastian and Lerch, Johannes and Mezini, Mira},
  title      = {Intelligent Code Completion with Bayesian Networks},
  year       = {2015},
  issue_date = {December 2015},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {25},
  number     = {1},
  issn       = {1049-331X},
  url        = {https://doi.org/10.1145/2744200},
  doi        = {10.1145/2744200},
  abstract   = {Code completion is an integral part of modern Integrated Development Environments (IDEs). Developers often use it to explore Application Programming Interfaces (APIs). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items.This work extends one of these existing approaches, the Best Matching Neighbor (BMN) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, Pattern-based Bayesian Networks (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed.Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that PBN can obtain comparable prediction quality to BMN, while model size and inference speed scale better with large input sizes.},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  month      = dec,
  articleno  = {3},
  numpages   = {31},
  keywords   = {Content assist, code completion, code recommender, evaluation, integrated development environments, machine learning, productivity}
}

@inproceedings{nguyen2015,
  author    = {Nguyen, Anh Tuan and Nguyen, Tien N.},
  booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  title     = {Graph-Based Statistical Language Model for Code},
  year      = {2015},
  volume    = {1},
  number    = {},
  pages     = {858-868},
  keywords  = {Context;Syntactics;Probability;Engines;Programming;Accuracy;Computational modeling;Graph-based Language Model;API Suggestion;Syntactic Template Suggestion},
  doi       = {10.1109/ICSE.2015.336}
}

@inproceedings{bielik2016,
  title     = {PHOG: Probabilistic Model for Code},
  author    = {Bielik, Pavol and Raychev, Veselin and Vechev, Martin},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  pages     = {2933--2942},
  year      = {2016},
  editor    = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v48/bielik16.pdf},
  url       = {https://proceedings.mlr.press/v48/bielik16.html},
  abstract  = {We introduce a new generative model for code called probabilistic higher order grammar (PHOG). PHOG generalizes probabilistic context free grammars (PCFGs) by allowing conditioning of a production rule beyond the parent non-terminal, thus capturing rich contexts relevant to programs. Even though PHOG is more powerful than a PCFG, it can be learned from data just as efficiently. We trained a PHOG model on a large JavaScript code corpus and show that it is more precise than existing models, while similarly fast. As a result, PHOG can immediately benefit existing programming tools based on probabilistic models of code.}
}

@inproceedings{ginzberg2017,
  title={Automatic Code Completion},
  author={Adam Ginzberg and Lindsey Kostas and Tara Balakrishnan},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:35359813}
}

@misc{karampatsis2019,
      title={Maybe Deep Neural Networks are the Best Choice for Modeling Source Code}, 
      author={Rafael-Michael Karampatsis and Charles Sutton},
      year={2019},
      eprint={1903.05734},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/1903.05734}, 
}

@misc{liu2020,
      title={Multi-task Learning based Pre-trained Language Model for Code Completion}, 
      author={Fang Liu and Ge Li and Yunfei Zhao and Zhi Jin},
      year={2020},
      eprint={2012.14631},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2012.14631}, 
}

@misc{alon2020,
      title={Structural Language Models of Code}, 
      author={Uri Alon and Roy Sadaka and Omer Levy and Eran Yahav},
      year={2020},
      eprint={1910.00577},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.00577}, 
}

@misc{svyatkovskiy2020,
      title={IntelliCode Compose: Code Generation Using Transformer}, 
      author={Alexey Svyatkovskiy and Shao Kun Deng and Shengyu Fu and Neel Sundaresan},
      year={2020},
      eprint={2005.08025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.08025}, 
}

@misc{kim2021,
      title={Code Prediction by Feeding Trees to Transformers}, 
      author={Seohyun Kim and Jinman Zhao and Yuchi Tian and Satish Chandra},
      year={2021},
      eprint={2003.13848},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2003.13848}, 
}

@article{estoup1916,
  added-at  = {2007-05-03T17:19:57.000+0200},
  address   = {Paris},
  author    = {Estoup, J. B.},
  biburl    = {https://www.bibsonomy.org/bibtex/272fb4f23ac99287d6a46a2be5fd0f01f/andreab},
  interhash = {492f9278bea34585716c1c1326c224ca},
  intrahash = {72fb4f23ac99287d6a46a2be5fd0f01f},
  journal   = {Institut Stenographique de France},
  keywords  = {d4.1 distribution estoup first frequency mandelbrot tagora word zipf},
  publisher = {Gauthier-Villars},
  timestamp = {2007-05-03T17:19:57.000+0200},
  title     = {Les Gammes Stenographiques},
  year      = 1916
}

@article{gage1994,
  title={A new algorithm for data compression},
  author={Philip Gage},
  journal={The C Users Journal archive},
  year={1994},
  volume={12},
  pages={23-38},
  url={https://api.semanticscholar.org/CorpusID:59804030}
}

@misc{sennrich2016,
      title={Neural Machine Translation of Rare Words with Subword Units}, 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1508.07909}, 
}

@inproceedings{harris1954,
  title={Distributional Structure},
  author={Zellig S. Harris},
  year={1954},
  url={https://api.semanticscholar.org/CorpusID:86680084}
}
