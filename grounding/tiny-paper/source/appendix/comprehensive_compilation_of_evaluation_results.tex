\section{Comprehensive Compilation of Evaluation Results}

We present results of our experiments in Table \ref{tab:extended-results}. They can be used as baselines for further research.

We additionally include results for the base model with RoPE's base frequencies ($\theta$) being 10,000 and 500,000, results for pretraining with file-level composer for the same values of $\theta$. These results demonstrate that RoPE adjustments impact model quality, and that the model with initial base frequency performs on zero-level for long contexts even after finetuning.

\input{Repo-level Completion/figures/extended-results}

% \todo{Write somethin here from commented}



% \subsection{Analysis and Discussion}

When using FL-4K composer, the model successfully recovers its quality after RoPE adjustments, suggesting that file-level data alone is sufficient to restore performance. The initial model shows strong in-context learning capabilities for the PD-4K composer, with it outperforming file-level inference. This advantage persists after repository-level pretraining, indicating that training on the collected data effectively retains model's ability to utilize relevant context for shorter context size.

For the PD-16K composer, the initial model, without RoPE adaptation, fails completely, but RoPE scaling alone improves Exact Match scores. Further pretraining yields gains of +19 for file-level training and +22 for the best composer in \textit{inproject} category, with all final scores being slightly higher than file-level pretraining performance. This suggests that adapting to the longer context window, rather than the specific sequence composition, is the primary factor in repository-level code completion, with context composers contributing only marginally for suggested approaches (+3 points for \textit{inproject} category).

% In the original pretraining composer with 16K truncation setting (column 16K+O), results cluster into two groups: one matching baseline composer performance and another aligning with file-level scores. This indicates that certain composers fail to retrieve meaningful repository-wide context and behave similarly to file-level approaches.

Overall, our findings emphasize that RoPE adaptation is the dominant factor in long-context performance gains, while sequence composition plays a secondary role. Future work should explore more effective retrieval-based strategies to maximize repository-level context utilization.


