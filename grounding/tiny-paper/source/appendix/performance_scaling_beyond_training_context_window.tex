\section{Performance Scaling Beyond Training Context Window}
\label{appendix:context-scaling}

% \todo{Need to Fix the Figure, because it does not satisfy requirements}
The repository-level pretraining with File-level composer and Path Distant composer for maximum sequence lengths of 4K and 16K. However, pretrained checkpoints extrapolate beyond these lengths up to 16K and 32K as snown on Figure \ref{fig:beyound_training_context_window}.

Similar behavior was observed in \citet{codellama} and it needs additional research.

\input{Repo-level Completion/figures/beyond-training-context-window}
