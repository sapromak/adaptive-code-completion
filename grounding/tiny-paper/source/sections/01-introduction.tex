\section{Introduction and Motivation}

Large Language Models (LLMs) trained on source code, commonly known as Code LLMs, have demonstrated impressive capabilities on a variety of code-related tasks \citep{codexglue, swe-bench, LLM-SE-review, CodeLLM-survey}. Traditionally, these models are pretrained on individual files, effectively capturing local context but often missing broader, project-level information. To address this limitation, several recent works have incorporated a \textit{repository-level pretraining} phase, \ie a stage of pretraining during which the model gets training examples from entire repositories to learn context spanning multiple files, shared dependencies, and cohesive development patterns. For example, models such as DeepSeek Coder, Starcoder 2, Qwen2.5 Coder and CodeGemma \citep{deepseek-coder, starcoder2, qwen2p5coder, codegemma} incorporate repository-level pretraining to extend their context windows and capture cross-file relationships. Beyond repository-level pretraining, other techniques have been investigated \citep{long-coder, repocoder, hirope}.

While repository-level pretraining enhances long-context capabilities, it also introduces significant challenges. Firstly, it requires huge amounts of data, \eg Qwen2.5 Coder’s repository-level pretraining leverages approximately 300B tokens of repository data. Secondly, long sequences can strain computational resources due to the quadratic complexity of traditional transformer architecture. Recent advances in efficient attention mechanisms (for example, Flash Attention and Ring Attention \citep{flash-attention-2, ring-attention}) have enabled training with context lengths typically in the tens of thousands of tokens, and even millions of tokens for smaller models. However, effective utilization of repository-level information remains challenging both for training and inference \citep{cocomic, repobench, cross-code-eval, better-context-better-completion}.

In this work, we focus on a single line repository-level code completion and study context extension pretraining for various repository preprocessing approaches. Following the Long Code Arena benchmark \citep{LCA} terminology, we evaluate the impact of different \textit{context composers}, \ie processors that transform repository files into context strings. Our approach builds on the OpenCoder base model \citep{opencoder}, originally configured with a 4,096 (4K) context window, by training on repository-level input sequences of up to 16,384 (16K) tokens. This extension results in significantly improved performance on 16K token sequences compared to the initial configuration.

We assess our methods using the Project-Level Code Completion task from the Long Code Arena benchmark, which effectively estimates a model’s ability to handle cross-file dependencies in realistic settings. By isolating the impact of repository-level pretraining and comparing different context composer strategies, our study provides practical insights for enhancing long-context code completion performance.

The main contributions of this paper are:
\begin{enumerate}
    \item We boost the project-level code completion performance of OpenCoder 1.5B to state-of-the-art levels using only 1B tokens of training data;
    \item Our experiments reveal that the choice of context composer during pretraining has only a marginal impact on final model quality, with performance scores ranging from 45.2 to 48.8 (out of 100) on the chosen metric.
\end{enumerate}
