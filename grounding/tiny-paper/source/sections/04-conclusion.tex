\section{Conclusion}
In this paper, we address the challenge of project-level code completion by evaluating pretraining for code LLM on various data extracted from repository. 
Our extensive experiments demonstrate that even relatively small training dataset and simple context composer (\eg file-level or path distance) is enough to get a model comparable to the latest state-of-the-art code LLMs. 
This insight reduces the complexity of repository-level pretraining, which effectively minimizes the technical complexities and encourages to broadly research the topic.

Although our findings are promising, they have certain limitations. Our experiments are limited to the OpenCoder model, and it remains unclear whether they generalize to other LLMs. A key direction for future work is to apply our approach on a broader range of Code LLMs. However, recent Code LLMs were released after the repository-level pretraining stage, which may introduce inconsistencies in evaluation.
