@article{LCA,
  title={Long Code Arena: a Set of Benchmarks for Long-Context Code Models},
  author={Bogomolov, Egor and Eliseeva, Aleksandra and Galimzyanov, Timur and Glukhov, Evgeniy and Shapkin, Anton and Tigina, Maria and Golubev, Yaroslav and Kovrigin, Alexander and van Deursen, Arie and Izadi, Maliheh and Bryksin, Timofey},
  journal={arXiv preprint arXiv:2406.11612},
  year={2024}
}

% Models

@article{opencoder,
  title={{OpenCoder}: The open cookbook for top-tier code large language models},
  author={Huang, Siming and Cheng, Tianhao and Liu, Jason Klein and Hao, Jiaran and Song, Liuyihan and Xu, Yang and Yang, J and Liu, JH and Zhang, Chenchen and Chai, Linzheng and others},
  journal={arXiv preprint arXiv:2411.04905},
  year={2024}
}

@article{deepseek-coder,
  title={{DeepSeek-Coder}: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}

@article{starcoder2,
  title={{{StarCoder} 2 and The Stack v2: The Next Generation}},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@article{qwen2p5coder,
  title={{Qwen2.5-Coder} Technical Report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@article{codegemma,
  title={{CodeGemma:} Open code models based on gemma},
  author={{CodeGemma Team} and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others},
  journal={arXiv preprint arXiv:2406.11409},
  year={2024}
}

% Context Extension

@article{RoPE-ABF,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}

@article{extend-context-survey,
  title={Beyond the limits: A survey of techniques to extend the context length in large language models},
  author={Wang, Xindi and Salmani, Mahsa and Omidi, Parsa and Ren, Xiangyu and Rezagholizadeh, Mehdi and Eshaghi, Armaghan},
  journal={arXiv preprint arXiv:2402.02244},
  year={2024}
}

@article{rope-extensions,
  title={Understanding the {RoPE} Extensions of Long-Context {LLM}s: An Attention Perspective},
  author={Zhong, Meizhi and Zhang, Chen and Lei, Yikun and Liu, Xikai and Gao, Yan and Hu, Yao and Chen, Kehai and Zhang, Min},
  journal={arXiv preprint arXiv:2406.13282},
  year={2024}
}

@article{positional-interpolation,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{yarn,
  title={{YaRN:} Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}

@article{rope-scaling-laws,
  title={Scaling laws of rope-based extrapolation},
  author={Liu, Xiaoran and Yan, Hang and Zhang, Shuo and An, Chenxin and Qiu, Xipeng and Lin, Dahua},
  journal={arXiv preprint arXiv:2310.05209},
  year={2023}
}


% Effective Long Context

@article{flash-attention-2,
  title={{FlashAttention}-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{ring-attention,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

% Code Benchmarks

@article{swe-bench,
  title={{{SWE}-bench: Can language models resolve real-world GitHub issues?}},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{codexglue,
  title={{{CodeXGLUE}: A machine learning benchmark dataset for code understanding and generation}},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021}
}



% Code Bencmarks Repo-level

@article{cross-code-eval,
  title={{CrossCodeEval}: A diverse and multilingual benchmark for cross-file code completion},
  author={Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi and Ding, Hantian and Tan, Ming and Jain, Nihal and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{repobench,
  title={{RepoBench: Benchmarking} Repository-Level Code Auto-Completion Systems},
  author={Liu, Tianyang and Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2306.03091},
  year={2023}
}

@article{cocomic,
  title={{CoCoMIC}: Code completion by jointly modeling in-file and cross-file context},
  author={Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
  journal={arXiv preprint arXiv:2212.10007},
  year={2022}
}

@article{better-context-better-completion,
  title={Better context makes better code language models: A case study on function call argument completion},
  author={Pei, Hengzhi and Zhao, Jinman and Lausen, Leonard and Zha, Sheng and Karypis, George},
  journal={arXiv preprint arXiv:2306.00381},
  year={2023}
}

% Repo-level Code LLMs

@article{long-coder,
  title={{LongCoder}: A Long-Range Pre-trained Language Model for Code Completion},
  author={Guo, Daya and Xu, Canwen and Duan, Nan and Yin, Jian and McAuley, Julian},
  journal={arXiv preprint arXiv:2306.14893},
  year={2023}
}

@inproceedings{repocoder,
  title={{RepoCoder:} Repository-Level Code Completion Through Iterative Retrieval and Generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Keung, Jacky and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={2471--2484},
  year={2023}
}

@inproceedings{hirope,
  title={{HiRoPE}: Length Extrapolation for Code Models Using Hierarchical Position},
  author={Zhang, Kechi and Li, Ge and Zhang, Huangzhao and Jin, Zhi},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13615--13627},
  year={2024}
}





@article{long-context-finetuning,
  title={Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models},
  author={Chen, Longze and Liu, Ziqiang and He, Wanwei and Li, Yunshui and Luo, Run and Yang, Min},
  journal={arXiv preprint arXiv:2405.17915},
  year={2024}
}


@article{LLM-SE-review,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={8},
  pages={1--79},
  year={2024},
  publisher={ACM New York, NY}
}

@article{CodeLLM-survey,
  title={A Survey on Large Language Models for Code Generation},
  author={Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}

@article{codellama,
  title={{Code Llama:} Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}