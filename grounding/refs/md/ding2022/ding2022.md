# arXiv:2212.10007v2 [cs.CL] 24 May 2023

#### COCOMIC: ✿✿✿✿ Code ✿✿✿✿ Completion By Jointly Modeling ✿✿ ✿ In-file and ✿✿Cross-file Context

Yangruibo Ding<sup>1</sup>,∗,† Zijian Wang<sup>2</sup>,∗,‡ Wasi Uddin Ahmad<sup>2</sup>,<sup>∗</sup> Murali Krishna Ramanathan<sup>2</sup> Ramesh Nallapati<sup>2</sup> Parminder Bhatia<sup>2</sup> Dan Roth<sup>2</sup> Bing Xiang<sup>2</sup>

<sup>1</sup>Columbia University <sup>2</sup>AWS AI Labs

yrbding@cs.columbia.edu {zijwan,wuahmad}@amazon.com

{mkraman,rnallapa,parmib,drot,bxiang}@amazon.com

## Abstract

While pre-trained language models (LM) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, *i.e., in-file context*, but ignore the rich semantics in other files within the same project, *i.e., cross-file context*, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code language models' capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we develop a cross-file context finder, CCFINDER, that locates and retrieves the most relevant cross-file context. We propose CO-COMIC, a framework that incorporates crossfile context to learn the in-file and cross-file context jointly on top of pretrained code LMs. COCOMIC successfully improves the existing code LM with a 33.94% relative increase in exact match and a 28.69% relative increase in identifier matching for code completion when the cross-file context is provided.

## 1 Introduction

In recent years, language models for source code like Codex [\(Chen et al.,](#page-9-0) [2021\)](#page-9-0) and CodeGen [\(Ni](#page-10-0)[jkamp et al.,](#page-10-0) [2022\)](#page-10-0) have shown promising performance in code completion tasks and have great potential to improve developer productivity [\(Barke](#page-8-0) [et al.,](#page-8-0) [2022\)](#page-8-0). These code LMs are typically trained with causal language modeling loss and complete the code conditioning on the previous code tokens in the same file, which we refer to as *in-file context*.

Modular programming [\(Parnas,](#page-10-1) [1972;](#page-10-1) [Parnas](#page-10-2) [et al.,](#page-10-2) [1985;](#page-10-2) [Sullivan et al.,](#page-10-3) [2001\)](#page-10-3) is a software design strategy that divides the complex software functionality into several independent, interchangeable components (*e.g.,* files, classes, and functions), such that each component implements only one aspect of the desired functionality and consequently

<span id="page-0-0"></span>![](_page_0_Figure_11.jpeg)

Figure 1: CodeGen-2B-mono fails to complete a Python program correctly as *in-file context* does not provide sufficient information. The model needs to know TagHandler takes an argument raw\_tags, which could be obtained through the function list\_tags of git. Generating the correct code requires the presence of class and function definitions as part of the context, which cannot be derived from the current file alone.

becomes easily reusable and testable. It has already been a well-adapted paradigm in modern software development and maintenance. Developing under the modular programming paradigm requires knowledge from the current file and the whole project, to which we refer as *cross-file context*. As shown in Figure [1,](#page-0-0) the *cross-file context* is critical for code completion: the CodeGen Python model [\(Nijkamp et al.,](#page-10-0) [2022\)](#page-10-0) with 2 billion parameters fails to generate the correct code since it only considers *in-file context* and lacks visibility to various crucial references for code completion, *e.g.,* member functions of imported classes and arguments of imported functions.

In this work, we argue that code LMs should generate code conditioned jointly on *in-file context* and *cross-file context*. However, there are challenges in developing such models. First, the project defines its individual and complex hierarchy and could be of varied sizes. Thus, given a piece of code, it is critical yet challenging to identify the most relevant and useful cross-file context. Second, we must carefully design a framework for aggregating the

information from the in-file and cross-file context. Naïvely concatenating code from in-file and crossfile context is not feasible for three reasons. (1) They represent distinct types of contextual information, as the former presents the local dependencies and human intentions (*e.g.,* code comments) for code completion, while the latter compensates for the project-level dependencies that do not exist in the surrounding lines. Thus, the model should not always treat them equally. (2) Unlike third-party packages which are mostly available in the pretraining dataset of LLMs, the same project context are likely to be private to the model, i.e., the model didn't see it during pre-training. This makes code completion that requires same project context very difficult if without the right context at inference time. (3) The model's input length is limited, so concatenating all context as input would exceed its context length capacity.

To address the aforementioned challenges, we build CCFINDER, a cross-file context finder that effectively retrieves the most relevant cross-file context given a code snippet to be completed. Furthermore, we propose COCOMIC, a novel framework that jointly learns in-file and cross-file context to improve code completion.

Cross-file Context Finder We design and implement CCFINDER, a static code analysis tool, to retrieve the most relevant cross-file context for code completion. CCFINDER parses the project hierarchy and code components to extract project information. CCFINDER further builds a project context graph to represent the details of each component (*i.e.,* entity) and the interactions among them (*i.e.,* relation). When an incomplete program requests completion, the tool will first analyze its import statements and pinpoint the related entities from the built context graph. Then, the tool will retrieve the neighbors of the pinpointed entities from the graph as the cross-file context of the current file.

Jointly Modeling In-file and Cross-file Context We propose COCOMIC, a novel framework built on top of existing code LMs with joint attention to in-file and retrieved cross-file context. We realize this in two steps: First, the model will compress cross-file context and build its representations. Second, when generating code completion, the model will attend to both the compressed cross-file context and the concrete in-file context.

We evaluate the effectiveness of CCFINDER and COCOMIC on a code completion dataset we built from the Python Package Index (PyPI), a repository of open-source Python projects. We show that CCFINDER is able to retrieve 27.07% more relevant context for code completion than in-file context. Experiments show that COCOMIC with access to relevant cross-file context improves the backbone pretrained code LM, CodeGen [\(Nijkamp](#page-10-0) [et al.,](#page-10-0) [2022\)](#page-10-0), by 33.94% in exact match and 28.69% identifier matches relatively.

Our main contributions are:

- 1. We present COCOMIC, a novel framework built on top of code LMs that jointly learns in-file and cross-file context to enhance code completion. To power COCOMIC, we develop CCFINDER, an effective tool at harvesting cross-file context, a critical yet overlooked resource for code completion in the era of modern software development.
- 2. We show that COCOMIC with cross-file context from CCFINDER significantly outperforms baselines by up to +33.94% in exact match. We additionally conduct extensive ablation studies to show the contribution of different components.
- 3. We release a diverse and high-quality dataset on statement-level code completion that tests model's ability on making use of cross-file context to facilitate further research.[1](#page-1-0)

# <span id="page-1-1"></span>2 Preliminaries

For the convenience of discussion, we define concepts that will be used throughout the paper.

Project Entities Project entities are code components that constitute the skeleton of software projects; developers frequently import and reuse these entities as cross-file context. We focus on four types of entities: *file, function, class, and global variable*. In particular, *file* contains the file name and file docstring; *class* contains the class signature, docstring, and attributes; *function* contains function signature, docstring, and body; *global variable* contains the variable name and its value.

Entity Relations Entity relations represent the interactions among project entities. We consider two categories of relations: *intra-file* and *inter-file*. Intra-file relations describe the in-file code hierarchies pre-defined by the programming language grammar. For example, a class is at the first level of the hierarchy while its member functions are at the

<span id="page-1-0"></span><sup>1</sup> <https://github.com/amazon-science/cocomic>

<span id="page-2-2"></span>![](_page_2_Figure_0.jpeg)

Figure 2: Overview of CCFINDER. First, CCFINDER builds the project context graph, including the bird's-eye view of the whole project and the code details of each module. Then, given the incomplete program, CCFINDER retrieves a set of the most relevant project entities as cross-file context from the graph.

second level. Inter-file relations define the file-tofile dependencies. Under each category, we further define several types of relations (Appendix [A\)](#page-11-0).

Locale We define *locale* as the entity's relative code location within the software project. For example, the locale of class entities is defined as file\_name.class\_name. The locale is assigned a unique name according to the specific location of a project entity, so we maintain the one-to-one mapping between each entity and its locale. The locale benefits COCOMIC in two ways: (1) when we construct cross-file context, the locale efficiently maps the relative path of a code snippet to its project entity CCFINDER builds ([§3.2\)](#page-3-0), and (2) it indicates hierarchical relations among project entities and helps model with code completion ([§6.4\)](#page-7-0).

In-file & Cross-file Context For an incomplete source file S, we define two types of context: *in-file* and *cross-file*. In-file context represents code snippets included in the current file, *i.e.,* code tokens before the predicting position. Cross-file context C represents the relevant code information (*e.g.,* classes, functions) from the same project that is out of but imported by the current file. Concretely, *cross-file context* refers to a collection of relevant project entities that might assist with the missing code prediction but are not in S.

## <span id="page-2-3"></span>3 Cross-file Context Finder: CCFINDER

Software projects typically have complex structures [\(Parnas et al.,](#page-10-2) [1985\)](#page-10-2) representing the dependencies among distinct code components. To retrieve the most relevant code snippets given a code, we need a tool with two main characteristics. First, the tool should be able to navigate the project structure to identify the file and module dependencies. Second, the tool can zoom into the dependencies and extract detailed code components. Off-the-shelf tools do not meet the requisites. For example, module dependency analysis tools[2,](#page-2-0)[3](#page-2-1) can only provide the module interactions while missing the hierarchical details inside each module and cannot directly output the concrete code. Therefore, we develop a new tool, CCFINDER, to aggregate cross-file context.

CCFINDER's overall workflow is shown in Figure [2.](#page-2-2) It has two main steps: (1) Analyze the program dependencies to build a bird's-eye view of the whole project and parse the source code to extract code details of each module. With these, CCFINDER builds the *project context graph*: graph nodes represent code components that constitute the project's backbone, and edges indicate the relations among components. (2) Given an incomplete program, the tool retrieves the most relevant crossfile context from the built graph. In this work, we focus on Python as the proof-of-concept to showcase our main arguments. However, CCFINDER's conceptual design is extensible to other languages.

#### <span id="page-2-4"></span>3.1 Project Context Graph

CCFINDER parses the project structure and corresponding source files to identify the project entities and entity relations. Then, CCFINDER uses entities and entity relations to build graph nodes and directed edges, respectively. The context graph is built top-down. First, we create a root node for the project and connect it with all file nodes. Sec-

<span id="page-2-0"></span><sup>2</sup> <https://github.com/google/importlab>

<span id="page-2-1"></span><sup>3</sup> <https://github.com/thebjorn/pydeps>

ond, each file node will build its own sub-graph, wrapping code components within the file, and also build connections with other files that it depends on, *i.e.,* it imports code from these files. Third, nodes will link to others within the file-level sub-graph based on the dependencies or scope. For example, a class node will have edges to its member functions.

Formally, CCFINDER builds the multi-relational, directed context graph G = (V, E) for the project, where V is the set of nodes representing code components, and E ⊆ V ×R×V is the set of edges that indicate the interactions among code components, where R is the set of edge types (Appendix [A\)](#page-11-0).

Note that the project context graph generated by CCFINDER differs from the traditional program dependence graph [\(Ferrante et al.,](#page-9-1) [1987\)](#page-9-1) and code property graph [\(Yamaguchi et al.,](#page-10-4) [2014\)](#page-10-4), which are built to estimate and analyze the program execution behaviors statically. These graphs focus on data flows and control flows, while our graph represents the dependencies of different modules within the project. CCFINDER-generated graph is also different from the code knowledge graph [\(Abdelaziz](#page-8-1) [et al.,](#page-8-1) [2021,](#page-8-1) [2022\)](#page-8-2) as the latter combines API usage knowledge such as third-party documentation and StackOverflow questions and answers.

#### <span id="page-3-0"></span>3.2 Cross-file Context Retrieval

The project context graph represents the project hierarchies and interactions among code components, so the closer a graph neighbor to a specific node, the more relevant that neighbor is. For example, if the input code imports a class, the most useful information regarding this class, such as its member functions and the global variables it depends on, should be only 1 or 2 hops away. Thus, given an input code, we retrieve a set of relevant nodes from the context graph as the cross-file context.

The workflow is presented in Algorithm [1.](#page-3-1) First, we extract the import statements from the incomplete code file (F) that only imports code snippets within the same project (GetLocalImportStmt). We iteratively use each import to identify and locate corresponding nodes in the project context graph (LocateNode). With the direct mapping between the code snippets and their locales ([§2\)](#page-1-1), we can locate the node given its relative path. We use such a node as the root node and retrieve its neighbors within k hops using the depth-first graph search (DepthF irstSearch). k is a configurable hyper-parameter in which increasing k

#### <span id="page-3-1"></span>Algorithm 1 Retrieve Cross-file context

- F: Incomplete code file G: Project context graph P: Parsed project information k: Maximum depth of graph search
- 1: ctx\_nodes ←Ø 2: I ← GetLocalImportStmt(F) 3: for stmt ∈ I do 4: root ← LocateNode(G, stmt) 5: N ← DepthF irstSearch(G, root, k) 6: for n ∈ N do 7: if n /∈ ctx\_nodes then 8: ctx\_nodes.add(n) 9: end if 10: end for 11: ReorderNode(ctx\_nodes,P) 12: end for 13: return ctx\_nodes

will retrieve a broader context. We set k = 2 for all the experiments in this work and empiricially justify the choice in [§6.2.](#page-6-0) Finally, once we collect the k-hop neighbors for all import statements, we re-order the nodes (ReorderNode), ensuring the nodes from the same source file follow the original code order, to maintain the naturalness [\(Hindle](#page-9-2) [et al.,](#page-9-2) [2012\)](#page-9-2) of human-written code.

## 4 Proposed Framework: COCOMIC

A high-level overview of the COCOMIC framework is presented in Figure [3.](#page-4-0) COCOMIC uses an autoregressive LM to encode (1) in-file code snippet and (2) retrieved cross-file context, and predicts the next code token conditioning on both.

#### <span id="page-3-2"></span>4.1 Input Representation

As shown in Figure [3,](#page-4-0) the model input includes two parts: source code sample S and its cross-file context C. Specifically, the source code sample S consists of a sequence of tokens x1, ..., x<sup>T</sup> , where xt is a code token and T is the length of S; the cross-file context, as introduced in [§3,](#page-2-3) is a list of entities, C = (c1, ..., cn), retrieved from the project context graph. Each entity, c<sup>i</sup> , is a short piece of code sequence describing the details of that entity, *i.e.,* c<sup>i</sup> = (locale<sup>i</sup> , w<sup>1</sup> i , ..., w<sup>m</sup> i , [SUM]), where w j i is a code token within the entity, locale<sup>i</sup> is the locale ([§2\)](#page-1-1) of c<sup>i</sup> , and [SUM] is a special token.

Representing Entity Relations with Locales As introduced in [§2,](#page-1-1) each project entity is paired with a locale that indicates its hierarchical relationship. We explore the benefits of prepending locales to

<span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)

Figure 3: The COCOMIC framework. Bottom: Given incomplete code, COCOMIC leverages CCFINDER to identify the corresponding entities in the project context graph ([§3.1\)](#page-2-4) and retrieve their k-hop neighbors as cross-file entities ([§3.2\)](#page-3-0). Up: COCOMIC first generates representations for cross-file entities using the appended [SUM] token ([§4.2\)](#page-4-1). Then it completes the current code by jointly attending to in-file and cross-file context ([§4.3\)](#page-4-2).

provide entities with such relational hints ([§6.4\)](#page-7-0). Specifically, for each cross-file entity, we prepend its locale to its code text as a comment, followed by a new line character: for the example in Figure [3,](#page-4-0) the retrieved entity def list\_tags() will be prepended with #git.list\_tags\n.

Better Entity Representation with **[SUM]** We append a special token [SUM] to entity descriptions. We expect [SUM] token to learn the summarization of the entity since the causal attention [\(Radford](#page-10-5) [et al.,](#page-10-5) [2019;](#page-10-5) [Brown et al.,](#page-9-3) [2020\)](#page-9-3) allows it to attend to all the previous tokens describing the entity. When completing code, the model will attend to the representations of the [SUM] tokens for each cross-file entity. We compare it with mean pooling in [§6.3](#page-6-1) and show that [SUM] works better.

#### <span id="page-4-1"></span>4.2 Encoding Cross-file Context

The computational cost of Transformers increases exponentially *w.r.t.* the input length, so it is impractical to prepend all the retrieved entities as plain text, as they typically contain thousands of tokens (Appendix [C\)](#page-11-1). Also, only a few keywords in an entity (e.g., identifiers) play an important role in assisting code completion. Thus, COCOMIC encodes each entity into a single token to balance the space limitation and the information need.

$$h\_{c\_i} = f\_\theta(c\_i) \in \mathbb{R}^{d\_h}, H\_\mathcal{C} = (h\_{c\_1}, \dots, h\_{c\_n}) \in \mathbb{R}^{n \times d\_h}$$

Specifically, for each entity c<sup>i</sup> , the model f<sup>θ</sup> will encode its code sequence into one representation hc<sup>i</sup> ∈ R <sup>d</sup><sup>h</sup> , where d<sup>h</sup> is the hidden dimension. Then, COCOMIC takes the hidden state of the last token, [SUM], as the entity representation. Finally, the model will output a list of entity embeddings, HC, representing the retrieved cross-file context.

# <span id="page-4-2"></span>4.3 Modeling In-file and Cross-file Context for Code Completion

After getting representations of cross-file context, COCOMIC continues to encode the in-file context and train the model to learn both context jointly.

In-file Context COCOMIC utilizes the causal language model setting to support the code completion task, where each token will consider its former texts as in-file context. Specifically, the infile context of source code S, at time step t, will be s<sup>t</sup> = (x1, ..., xt−1). We pass these tokens through the model and get the embeddings of each token to construct the representation of the in-file context.

$$H\_{\mathcal{S}}(t) = f\_{\theta}(s\_t) = f\_{\theta}(x\_1, \dots, x\_{t-1}) \in \mathbb{R}^{(t-1) \times d\_h}$$

Joint attention to In-file and Cross-file Context Different layers of a Transformer model have been shown to capture different language components (*e.g.,* lower layers learn language syntax or grammar while upper layers capture language semantics [\(Jawahar et al.,](#page-9-4) [2019\)](#page-9-4)). We hypothesize that both in-file and cross-file context contribute to forming the understanding of language components. Therefore, we fuse the in-file and cross-file context at each Transformer layer so that generating the next token's hidden state will always depend on both context. At each time step t, for the l-th layer, we

first compute the keys and values for cross-file and in-file context, using their (l − 1)-th hidden states.

$$\begin{aligned} K\_{\mathcal{C}} &= H\_{\mathcal{C}}^{[l-1]} \mathbf{W}^K, V\_{\mathcal{C}} = H\_{\mathcal{C}}^{[l-1]} \mathbf{W}^V \\ K\_{\mathcal{S}}(t) &= H\_{\mathcal{S}}(t)^{[l-1]} \mathbf{W}^K, V\_{\mathcal{S}}(t) = H\_{\mathcal{S}}(t)^{[l-1]} \mathbf{W}^V \end{aligned}$$

Then, we concatenate the keys and values from both context so that, at time step t, the generating token can jointly attend them.

$$K(t) = \mathsf{concat}(K\_{\mathcal{C}}, K\_{\mathcal{S}}(t)), V(t) = \mathsf{concat}(V\_{\mathcal{C}}, V\_{\mathcal{S}}(t))$$

$$Q(t) = f\_{\theta}(x\_t)^{[l-1]} \mathbf{W}^Q, Attn(t) = \mathsf{softmax}(\frac{Q(t)K(t)^\top}{\sqrt{d\_K}})V(t)$$

# 5 Experiment Setup

#### 5.1 Data

Our data stem from the Python Package Index.[4](#page-5-0) We collect permissively-licensed projects and filter out those with too few files (≤5 python files) or too memory-consuming to build the project context graph (≥5k nodes), ending up with 60,891 projects. Then, we divide the dataset into 80%/10%/10% train, validation, and test sets. We notice that popular packages, such as numpy, are used as dependencies by many packages and will cause potential information leakage if numpy is part of the test set. Thus, we only include projects that were not used as dependencies by any training projects in the test set. We create prompts by cutting the source file at the location where completion requires cross-file context. See Appendix [C](#page-11-1) for more details.

Figure [1](#page-0-0) shows an example prompt we create: it requires the details of TagHandler and git to complete the code accurately. In this work, we consider statement-level code completion, so the ground truth of the test sample is built accordingly. For the convenience of studying the model's prediction on local APIs (*i.e.,* APIs defined within the project), we further filter out the samples that either can not be parsed by the AST parser or do not include local API calls in the target statement (to be completed). Finally, we ended up with the 6,888 held-out prompts for evaluation.

#### 5.2 Implementation Details

Cross-file Context CCFINDER uses tree-sitter[5](#page-5-1) to parse source code files. Tree-sitter is a widely used source code parser that generates the abstract syntax tree (AST) given a program. CCFINDER will traverse the AST to extract information as described in [§3.](#page-2-3) Then, CCFINDER analyzes the import statements on top of import-dep[6](#page-5-2) to build the project context graph. In this work, we retrieve 2-hop neighbors with at max 128 project entities as cross-file context and each entity contains up to 128 tokens. These thresholds are data-driven to ensure the model input covers most of the relevant cross-file context (more details in Appendix [B\)](#page-11-2).

Model The backbone of COCOMIC is CodeGen [\(Nijkamp et al.,](#page-10-0) [2022\)](#page-10-0) and we use CodeGen-350M-Mono for all experiments. In all settings, we finetune the model for 5 epochs with max sequence length of 2,048 tokens and learning rate of 5e-5 with 5% warm-up steps then cosine annealing.

#### <span id="page-5-3"></span>5.3 Baselines & Evaluation Metrics

CodeGen We consider two variations of the vanilla CodeGen model with in-file context only: (1) zero-shot, where we directly evaluate the pretrained CodeGen model on our test dataset, and (2) finetuned, where we finetune CodeGen on our dataset first and then evaluate.

CodeGen w/ Cross-file Context We also consider a prompting baseline where we prepend the cross-file context to the input sequence and finetune. Similar to the configuration of COCOMIC, we reserve the first 128 tokens of the input for the code tokens from the cross-file context and use the rest tokens for the in-file context.

Evaluation Metrics We compute exact match (EM) and BLEU-4 [\(Papineni et al.,](#page-10-6) [2002\)](#page-10-6) to assess the accuracy of the generated code. While code match indicates the overall correctness of code completion, we want to zoom into the cases where cross-file context could most contribute, which is API usage. Therefore, we measure the identifier match to evaluate whether cross-file context improves the model's ability to predict the right APIs. To this end, we extract the identifiers from the model prediction and the ground truth, resulting in two ordered lists of identifiers. Then, we compare them and report the identifier match results in exact match, precision, and recall.

Besides, we compute the perplexity of all the tokens on the test set to study whether adding crossfile context degrades performance when the crossfile context is not explicitly required.

<span id="page-5-0"></span><sup>4</sup> <https://pypi.org/>

<span id="page-5-1"></span><sup>5</sup> <https://tree-sitter.github.io/>

<span id="page-5-2"></span><sup>6</sup> <https://pypi.org/project/import-deps/>

<span id="page-6-2"></span>

| Model                | Finetuned | Cross-file | Code Match |        | ID Match |       |       | PPL (↓) |
|----------------------|-----------|------------|------------|--------|----------|-------|-------|---------|
|                      |           | Entities   | EM         | BLEU-4 | EM       | Prec. | Rec.  |         |
| CodeGen              | ✗         | ✗          | 14.56      | 33.12  | 22.91    | 47.74 | 50.75 | 2.88    |
| + Finetune           | ✓         | ✗          | 15.97      | 35.11  | 24.29    | 50.46 | 53.07 | 2.87    |
| + Cross-file context | ✓         | ✓          | 17.00      | 36.34  | 25.80    | 48.91 | 54.76 | 2.77    |
| COCOMIC (Ours)       | ✓         | ✓          | 21.39      | 41.65  | 31.26    | 55.45 | 57.83 | 2.69    |

Table 1: Performance of COCOMIC compared with baselines. We show that using the text prompt for cross-file entities (row 3) helps marginally compared to the in-file-only baseline (row 2). On the contrary, COCOMIC with cross-file context (row 4) improves the performance by a large margin (+33.94% Code Match EM and +28.69% ID Match EM) compared to the in-file only baseline. In addition, we show that there is no degradation in perplexity (PPL) when evaluating all the tokens in the test set where the cross-file context is not always required, suggesting that adding cross-file context helps in general. See Appendix [F](#page-12-0) for additional case studies.

## 6 Results and Analysis

#### 6.1 COCOMIC Outperforms the Baselines

We present the results in Table [1.](#page-6-2) COCOMIC outperforms all baselines on all metrics with a clear margin, demonstrating the effectiveness of our proposed framework. We notice that when the crossfile context is prepended as a plain text prompt, CodeGen outperforms the other two baselines without cross-file context. However, limited by the maximum input length, it can only include a very limited amount of cross-file context, which significantly restricts its capacity. In contrast, COCOMIC encodes the code sequence of an entity into one single token, enabling the model to incorporate more cross-file context while saving the input length. We present additional ablations for the baseline model in Appendix [E.2,](#page-12-1) and case studies in Appendix [F.](#page-12-0)

Besides, we see no degradation when the crossfile context is not explicitly required. We calculate the perplexity of all tokens in the test samples, regardless of whether they require cross-file context. We see that COCOMIC achieves the lowest perplexity, indicating cross-file context in COCOMIC is generally beneficial for code completion.

# <span id="page-6-0"></span>6.2 CCFINDER Retrieves Relevant Cross-file Context

The objective of CCFINDER is to locate and retrieve relevant code context from other source files in the project. Identifiers (e.g., function names and parameters) are presumably one of the most critical API information. Therefore, we study the effectiveness of CCFINDER by assessing whether their retrieved context increases recall of the identifiers that appear in the ground truth.[7](#page-6-3)

<span id="page-6-4"></span>

| Code Context Type            | ID Recall (%) |
|------------------------------|---------------|
| In-file context              | 75.19         |
| In-file + Cross-file context | 95.55         |

Table 2: CCFINDER retrieves 27.07% more identifiers when compared to only in-file contexts.

<span id="page-6-5"></span>

| Entities From          | Code Match |        | ID Match |                   |      |  |
|------------------------|------------|--------|----------|-------------------|------|--|
|                        | EM         | BLEU-4 | EM       | Prec.             | Rec. |  |
| Random                 | 15.68      | 35.23  |          | 24.07 49.75 52.69 |      |  |
| CCFINDER (1-hop) 18.47 |            | 38.09  |          | 28.14 53.20 55.63 |      |  |
| CCFINDER (2-hop) 21.39 |            | 41.65  |          | 31.26 55.45 57.83 |      |  |

Table 3: Entities retrieved from CCFINDER are more useful than random entities, and 2-hop retrieval help achieve better performance.

Table [2](#page-6-4) shows that the in-file context covers (recall) 75.19% identifiers that appear in the ground truth. In comparison, prompts augmented with retrieved cross-file identifiers bring up identifier recall to 95.55%. This indicates that CCFINDER can retrieve most of the cross-file context that can help LM complete the input code. Note that while CCFINDER increases identifier recall by 27.07%, Table [1](#page-6-2) shows only a 7.08% improvement in identifier recall. This indicates that building intelligent prompting techniques or training LMs to use cross-file context can lead to better performances. Further, Table [3](#page-6-5) shows that random entities from the same project do not provide useful information since they are not necessarily related to the input code, and 2-hop retrieval outperforms 1-hop retrieval. These verify that CCFINDER retrieves relevant cross-file context and thus helps COCOMIC. Appendix [E.1](#page-12-2) presents additional analysis.

## <span id="page-6-1"></span>6.3 Entity Representation with **[SUM]** Token

We append a special token [SUM] to cross-file context to summarize their information (Figure [3\)](#page-4-0).

<span id="page-6-3"></span><sup>7</sup>We hypothesize that the inclusion of identifiers needed to complete a code is likely to benefit COCOMIC.

Now, we study the importance of the [SUM] token for a better representation of cross-file context. As a comparison, we apply the widely-used mean pooling that takes the mean over every cross-file token's embedding as the cross-file representation. We train a COCOMIC model with mean pooling and keep the rest of the settings the same. The result is in Table [4:](#page-7-1) our proposed [SUM] token effectively summarizes cross-file context and significantly outperforms the mean pooling strategy.

<span id="page-7-1"></span>

| COCOMIC            |       | Code Match | ID Match |                   |      |  |
|--------------------|-------|------------|----------|-------------------|------|--|
|                    | EM    | BLEU-4     | EM       | Prec.             | Rec. |  |
| Mean pooling 16.78 |       | 36.02      |          | 25.01 50.50 52.61 |      |  |
| [SUM]              | 21.39 | 41.65      |          | 31.26 55.45 57.83 |      |  |

Table 4: Representing cross-file context with [SUM] token significantly outperforms mean pooling alternative.

#### <span id="page-7-0"></span>6.4 Locales Help Learning Cross-file Context

As introduced in [§4.1,](#page-3-2) we prepend locales as relational hints for better entity representations. We study the effectiveness of such relational signals. As a comparison, we further study multi-task learning that encourages embedding relational information into entity representations.

Multi-task w/ Edge Prediction We use multitask learning (MTL) to encode cross-file relations. Specifically, we train the model with an auxiliary edge prediction task among cross-file entities. We take representations of two cross-file entities generated by the LM layers and ask the model to predict what edge type connects them.

Results Table [5](#page-7-2) presents the results. While MTL achieves 97.2% accuracy in the auxiliary edge prediction task, it hardly improves COCOMIC in code completion. Such a gap suggests that even if MTL fulfills the expectation of embedding edge information, this information is not directly useful for code completion. In contrast, adding locales consistently improves COCOMIC across all metrics. We hypothesize that this is due to locales providing an exact and direct signal as text (e.g., class\_name.method\_name). Thus the model could use them as *short-cut* in code completion.

# 7 Related Work

In the last couple of years, a significant effort has been made to pretrain Transformer language models using unlabeled source code [\(Feng et al.,](#page-9-5) [2020;](#page-9-5) [Ahmad et al.,](#page-8-3) [2021;](#page-8-3) [Wang et al.,](#page-10-7) [2021b;](#page-10-7) [Guo et al.,](#page-9-6)

<span id="page-7-2"></span>

| COCOMIC            |       | Code Match | ID Match |                   |  |
|--------------------|-------|------------|----------|-------------------|--|
|                    | EM    | BLEU-4     | EM       | Prec. Rec.        |  |
| No Relations       | 20.27 | 40.62      |          | 30.02 55.44 57.46 |  |
| MTL                | 20.01 | 40.00      |          | 29.53 55.51 56.68 |  |
| Locale             | 21.39 | 41.65      |          | 31.26 55.45 57.83 |  |
| Locale + MTL 21.25 |       | 41.44      |          | 31.05 55.83 58.03 |  |

Table 5: Locales improve performance while learning cross-file relations with multi-task learning does not provide COCOMIC more than marginal improvement.

[2022;](#page-9-6) [Ding et al.,](#page-9-7) [2022\)](#page-9-7) to facilitate software engineering applications [\(Husain et al.,](#page-9-8) [2019;](#page-9-8) [Iyer](#page-9-9) [et al.,](#page-9-9) [2018;](#page-9-9) [Tufano et al.,](#page-10-8) [2019;](#page-10-8) [Zhou et al.,](#page-10-9) [2019\)](#page-10-9). Among these efforts, developing code generation models is noteworthy [\(Chen et al.,](#page-9-0) [2021;](#page-9-0) [Xu et al.,](#page-10-10) [2022;](#page-10-10) [Wang and Komatsuzaki,](#page-10-11) [2021;](#page-10-11) [Black et al.,](#page-8-4) [2021,](#page-8-4) [2022;](#page-9-10) [Nijkamp et al.,](#page-10-0) [2022;](#page-10-0) [Fried et al.,](#page-9-11) [2022;](#page-9-11) [Li et al.,](#page-10-12) [2022\)](#page-10-12). Since most of these models are autoregressive language models, they can be directly used in code completion - given a code snippet as a prompt, generate the next N tokens. Until recently, existing works in the literature use code snippet from the current file (where the user is writing code) to prompt the code generation models. In a concurrent work, [Zhou et al.](#page-10-13) [\(2022\)](#page-10-13) proposed to retrieve API documentation given a natural language (NL) intent and generate code based on them. Our work has the same spirit as we propose to retrieve cross-file context (user-defined classes, functions from other project files) given a source code. The fundamental difference is that we utilize the *import statements* for structured retrieval.

While the use of in-file or class context is rigorously studied for software engineering applications in the literature, the use of cross-file context is relatively under-explored in code completion backed by code LMs. Earlier works [\(Henninger,](#page-9-12) [1991;](#page-9-12) [Rosson and Carroll,](#page-10-14) [1996;](#page-10-14) [Michail,](#page-10-15) [2001;](#page-10-15) [Ye et al.,](#page-10-16) [2000;](#page-10-16) [Ye and Fischer,](#page-10-17) [2002;](#page-10-17) [Cubranic](#page-9-13) [and Murphy,](#page-9-13) [2003;](#page-9-13) [Inoue et al.,](#page-9-14) [2003;](#page-9-14) [Hill and](#page-9-15) [Rideout,](#page-9-15) [2004;](#page-9-15) [Holmes and Murphy,](#page-9-16) [2005\)](#page-9-16) in software engineering literature focused on developing tools to extract information from software repositories to help developers complete code fragments (e.g., variable, method name or body completion). On the other hand, recent works focus on modeling cross-file information in neural approaches. [Wang et al.](#page-10-18) [\(2021a\)](#page-10-18) proposed to model intra- and inter-class context for code summarization by extracting the Unified Modeling Language (UML) class diagrams. A recent work [\(Shrivastava et al.,](#page-10-19)

[2022\)](#page-10-19) proposed a prompt engineering technique that learns a repository-level prompt generator to generate example-specific prompts. A concurrent work [\(Zhang et al.,](#page-10-20) [2023\)](#page-10-20) proposed an iterative retrieval-generation framework to augment prompt with cross-file context.

# 8 Conclusion

The absence of *cross-file context* for code language models (LMs) limits their practicality in modern software development. In this work, we propose COCOMIC, a framework that incorporates both in-file and cross-file context for code completion based on autoregressive code LMs. For this purpose, we build CCFINDER, a static code analysis tool that builds the project context graph, and find the most relevant cross-file context based on import statements. Empirical results show that CCFINDER successfully retrieves 27.07% more relevant context that are not in the current file, and our best COCOMIC model achieves 33.94% relative improvement over the in-file-context-only baseline.

# Limitations

Extension to other languages and third-party packages Our work focuses on Python language, which is widely used and has great availability of open-sourced software projects through PyPI. However, the main concept introduced in our work should be extensible to other languages. In addition, we focus on the project (repo) context in this work, and a potential extension is to incorporate third-party packages and building models to suggest the right third-party libraries to use. We leave these as future work.

Model performances with the absence of cross– file context In this work, we assumed that CO-COMIC could access the other source code files within the project to understand source code dependencies and utilize them accordingly to generate the target code completion. However, COCOMIC may not access the code files in many cases, *e.g.,* users do not want an AI code LM to read their private or sensitive project APIs. Therefore, it is valid to ask – how COCOMIC performs when the cross-file context is absent. We evaluate COCOMIC without access to cross-file context and compare with finetuned CodeGen model (second row in Table [1\)](#page-6-2). The results show that COCOMIC performs 5–7% lower (relative performance drop) than finetuned CodeGen model. Development of training strate-

gies to bridge this performance gap is needed, and we leave this as future work.

Impact on different sized language models Although we use CodeGen-350-mono model in this work which consists of 350M parameters, we hypothesize that larger LMs (*e.g.,* 2B, 6B, or 16B variants of CodeGen) would result in similar or higher performance boost due to modeling crossfile context. However, we acknowledge that our work does not substantiate that our proposed technique would boost the performance of language models of any size.

# Ethics Statement

Our work aims at improving code generation with cross-file context to improve the usability of code LMs. We highlight the limitations of our work above. We do not expect our work to have a negative broader impact, though using code LMs always comes with certain risks, e.g., generating biased, toxic, and insecure code. We refer readers to Sec. 7 in [\(Chen et al.,](#page-9-0) [2021\)](#page-9-0) for a detailed discussion on the broader impact of code LMs. In addition, we reported our usage of computational resources in Appendix [D.](#page-11-3)

# References

- <span id="page-8-1"></span>Ibrahim Abdelaziz, Julian Dolby, James P McCusker, and Kavitha Srinivas. 2021. A toolkit for generating code knowledge graphs. *The Eleventh International Conference on Knowledge Capture (K-CAP)*.
- <span id="page-8-2"></span>Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, and Kavitha Srinivas. 2022. Can machines read coding manuals yet? – a benchmark for building better language models for code understanding. In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI 2022)*.
- <span id="page-8-3"></span>Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. [Unified pre-training for pro](https://doi.org/10.18653/v1/2021.naacl-main.211)[gram understanding and generation.](https://doi.org/10.18653/v1/2021.naacl-main.211) In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 2655–2668, Online. Association for Computational Linguistics.
- <span id="page-8-0"></span>Shraddha Barke, Michael B James, and Nadia Polikarpova. 2022. [Grounded copilot: How programmers](https://arxiv.org/abs/2206.15000) [interact with code-generating models.](https://arxiv.org/abs/2206.15000) *ArXiv preprint*, abs/2206.15000.
- <span id="page-8-4"></span>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. *If you use this software, please cite it using these metadata*, 58.
- <span id="page-9-10"></span>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. [GPT-NeoX-20B: An open](https://doi.org/10.18653/v1/2022.bigscience-1.9)[source autoregressive language model.](https://doi.org/10.18653/v1/2022.bigscience-1.9) In *Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models*, pages 95–136, virtual+Dublin. Association for Computational Linguistics.
- <span id="page-9-3"></span>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. [Language models are few-shot learners.](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html) In *Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*.
- <span id="page-9-0"></span>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. [Evaluating large lan](https://arxiv.org/abs/2107.03374)[guage models trained on code.](https://arxiv.org/abs/2107.03374) *ArXiv preprint*, abs/2107.03374.
- <span id="page-9-13"></span>Davor Cubranic and Gail C Murphy. 2003. Hipikat: Recommending pertinent software development artifacts. In *25th International Conference on Software Engineering, 2003. Proceedings.*, pages 408– 418. IEEE.
- <span id="page-9-7"></span>Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray, and Saikat Chakraborty. 2022. [Towards learning \(dis\)-similarity of source](https://doi.org/10.18653/v1/2022.acl-long.436) [code from program contrasts.](https://doi.org/10.18653/v1/2022.acl-long.436) In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 6300–6312, Dublin, Ireland. Association for Computational Linguistics.
- <span id="page-9-5"></span>Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. [Code-](https://doi.org/10.18653/v1/2020.findings-emnlp.139)[BERT: A pre-trained model for programming and](https://doi.org/10.18653/v1/2020.findings-emnlp.139) [natural languages.](https://doi.org/10.18653/v1/2020.findings-emnlp.139) In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages 1536–1547, Online. Association for Computational Linguistics.
- <span id="page-9-1"></span>Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. 1987. [The program dependence graph and its use](https://doi.org/10.1145/24039.24041) [in optimization.](https://doi.org/10.1145/24039.24041) *ACM Trans. Program. Lang. Syst.*, 9(3):319–349.
- <span id="page-9-11"></span>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. [Incoder:](https://arxiv.org/abs/2204.05999) [A generative model for code infilling and synthesis.](https://arxiv.org/abs/2204.05999) *ArXiv preprint*, abs/2204.05999.
- <span id="page-9-6"></span>Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. [UniXcoder: Unified cross](https://doi.org/10.18653/v1/2022.acl-long.499)[modal pre-training for code representation.](https://doi.org/10.18653/v1/2022.acl-long.499) In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 7212–7225, Dublin, Ireland. Association for Computational Linguistics.
- <span id="page-9-12"></span>Scott Henninger. 1991. Retrieving software objects in an example-based programming environment. In *Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval*, pages 251–260.
- <span id="page-9-15"></span>Rosco Hill and Joe Rideout. 2004. Automatic method completion. In *Proceedings. 19th International Conference on Automated Software Engineering, 2004.*, pages 228–235. IEEE.
- <span id="page-9-2"></span>Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the naturalness of software. In *Proceedings of the 34th International Conference on Software Engineering*, ICSE '12, page 837–847. IEEE Press.
- <span id="page-9-16"></span>Reid Holmes and Gail C Murphy. 2005. Using structural context to recommend source code examples. In *Proceedings of the 27th international conference on Software engineering*, pages 117–125.
- <span id="page-9-8"></span>Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. [Code](https://arxiv.org/abs/1909.09436)[searchnet challenge: Evaluating the state of semantic](https://arxiv.org/abs/1909.09436) [code search.](https://arxiv.org/abs/1909.09436) *arXiv preprint arXiv:1909.09436*.
- <span id="page-9-14"></span>Katsuro Inoue, Reishi Yokomori, Hikaru Fujiwara, Tetsuo Yamamoto, Makoto Matsushita, and Shinji Kusumoto. 2003. Component rank: Relative significance rank for software component search. In *25th International Conference on Software Engineering, 2003. Proceedings.*, pages 14–24. IEEE.
- <span id="page-9-9"></span>Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. [Mapping language to code](https://doi.org/10.18653/v1/D18-1192) [in programmatic context.](https://doi.org/10.18653/v1/D18-1192) In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1643–1652, Brussels, Belgium. Association for Computational Linguistics.
- <span id="page-9-4"></span>Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019. [What does BERT learn about the structure of](https://doi.org/10.18653/v1/P19-1356) [language?](https://doi.org/10.18653/v1/P19-1356) In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 3651–3657, Florence, Italy. Association for Computational Linguistics.
- <span id="page-9-17"></span>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. Survey of hallucination in natural language generation. *ACM Computing Surveys*.
- <span id="page-10-12"></span>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. [Competition-level code generation with](https://arxiv.org/abs/2203.07814) [alphacode.](https://arxiv.org/abs/2203.07814) *ArXiv preprint*, abs/2203.07814.
- <span id="page-10-15"></span>Amir Michail. 2001. Codeweb: Data mining library reuse patterns. In *Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001*, pages 827–828. IEEE.
- <span id="page-10-0"></span>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. [Codegen: An open large language](https://arxiv.org/abs/2203.13474) [model for code with multi-turn program synthesis.](https://arxiv.org/abs/2203.13474) *ArXiv preprint*, abs/2203.13474.
- <span id="page-10-6"></span>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. [Bleu: A method for automatic evalu](https://doi.org/10.3115/1073083.1073135)[ation of machine translation.](https://doi.org/10.3115/1073083.1073135) In *Proceedings of the 40th Annual Meeting on Association for Computational Linguistics*, ACL '02, page 311–318, USA. Association for Computational Linguistics.
- <span id="page-10-1"></span>D. L. Parnas. 1972. [On the criteria to be used in de](https://doi.org/10.1145/361598.361623)[composing systems into modules.](https://doi.org/10.1145/361598.361623) *Commun. ACM*, 15(12):1053–1058.
- <span id="page-10-2"></span>D.L. Parnas, P.C. Clements, and D.M. Weiss. 1985. [The](https://doi.org/10.1109/TSE.1985.232209) [modular structure of complex systems.](https://doi.org/10.1109/TSE.1985.232209) *IEEE Transactions on Software Engineering*, SE-11(3):259–266.
- <span id="page-10-5"></span>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. [Language](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [models are unsupervised multitask learners.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) *OpenAI preprint*.
- <span id="page-10-14"></span>Mary Beth Rosson and John M Carroll. 1996. The reuse of uses in smalltalk programming. *ACM Transactions on Computer-Human Interaction (TOCHI)*, 3(3):219–253.
- <span id="page-10-19"></span>Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2022. [Repository-level prompt generation for](https://arxiv.org/abs/2206.12839) [large language models of code.](https://arxiv.org/abs/2206.12839) *ArXiv preprint*, abs/2206.12839.
- <span id="page-10-3"></span>Kevin J. Sullivan, William G. Griswold, Yuanfang Cai, and Ben Hallen. 2001. [The structure and value of](https://doi.org/10.1145/503209.503224) [modularity in software design.](https://doi.org/10.1145/503209.503224) ESEC/FSE-9, page 99–108, New York, NY, USA. Association for Computing Machinery.
- <span id="page-10-8"></span>Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019. [An empirical study on learning bug](https://doi.org/10.1145/3340544)[fixing patches in the wild via neural machine trans](https://doi.org/10.1145/3340544)[lation.](https://doi.org/10.1145/3340544) *ACM Transactions on Software Engineering and Methodology (TOSEM)*, 28(4):1–29.
- <span id="page-10-11"></span>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/](https://github.com/kingoflolz/mesh-transformer-jax) [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).
- <span id="page-10-18"></span>Yanlin Wang, Ensheng Shi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, Hongyu Zhang, and Dongmei Zhang. 2021a. [Cocosum: Contextual code summa](https://arxiv.org/abs/2107.01933)[rization with multi-relational graph neural network.](https://arxiv.org/abs/2107.01933) *ArXiv preprint*, abs/2107.01933.
- <span id="page-10-7"></span>Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. 2021b. [CodeT5: Identifier-aware unified pre](https://doi.org/10.18653/v1/2021.emnlp-main.685)[trained encoder-decoder models for code understand](https://doi.org/10.18653/v1/2021.emnlp-main.685)[ing and generation.](https://doi.org/10.18653/v1/2021.emnlp-main.685) In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 8696–8708, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
- <span id="page-10-21"></span>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. [Trans](https://doi.org/10.18653/v1/2020.emnlp-demos.6)[formers: State-of-the-art natural language processing.](https://doi.org/10.18653/v1/2020.emnlp-demos.6) In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.
- <span id="page-10-10"></span>Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In *Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming*, pages 1–10.
- <span id="page-10-4"></span>Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. 2014. [Modeling and discovering vulner](https://doi.org/10.1109/SP.2014.44)[abilities with code property graphs.](https://doi.org/10.1109/SP.2014.44) In *2014 IEEE Symposium on Security and Privacy*, pages 590–604.
- <span id="page-10-17"></span>Yunwen Ye and Gerhard Fischer. 2002. Supporting reuse by delivering task-relevant and personalized information. In *Proceedings of the 24th international conference on Software engineering*, pages 513–523.
- <span id="page-10-16"></span>Yunwen Ye, Gerhard Fischer, and Brent Reeves. 2000. Integrating active information delivery and reuse repository systems. *ACM SIGSOFT Software Engineering Notes*, 25(6):60–68.
- <span id="page-10-20"></span>Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023. Repocoder: Repository-level code completion through iterative retrieval and generation. *arXiv preprint arXiv:2303.12570*.
- <span id="page-10-13"></span>Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao JIang, and Graham Neubig. 2022. [Doccoder: Generating](https://arxiv.org/abs/2207.05987) [code by retrieving and reading docs.](https://arxiv.org/abs/2207.05987) *ArXiv preprint*, abs/2207.05987.
- <span id="page-10-9"></span>Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. [Devign: Effective vulnerability](https://proceedings.neurips.cc/paper/2019/file/49265d2447bc3bbfe9e76306ce40a31f-Paper.pdf) [identification by learning comprehensive program](https://proceedings.neurips.cc/paper/2019/file/49265d2447bc3bbfe9e76306ce40a31f-Paper.pdf) [semantics via graph neural networks.](https://proceedings.neurips.cc/paper/2019/file/49265d2447bc3bbfe9e76306ce40a31f-Paper.pdf) In *Advances in Neural Information Processing Systems*, volume 32, pages 10197–10207. Curran Associates, Inc.

## <span id="page-11-0"></span>A Edge Types of Project Context Graph

We provide details of edge types that we use to build our project context graph in Table [6.](#page-11-4) The edges of the project context graph are directional, so for each edge type, we further define the expected entity type of its tail (*i.e.,* the entity that the edge is "from") and head (*i.e.,* the entity that the edge is "to") for each edge type. We also consider the reverse edges for certain types so that CCFINDER could retrieve entity siblings conveniently: *e.g.,* when a function is imported, CCFINDER could retrieve global variables it depends on by visiting the function's parent, *i.e.,* the file, with an edge of Function Reverse type, and reach the global variable with an edge of Global Var. type.

<span id="page-11-4"></span>

| Edge Type           | Tail        | Head        |
|---------------------|-------------|-------------|
| Project File        | root        | file        |
| Import              | file        | file        |
| Global Var.         | file        | global var. |
| Global Var. Reverse | global var. | file        |
| Function            | file        | function    |
| Function Reverse    | function    | file        |
| Class               | file        | class       |
| Class Reverse       | class       | file        |
| Member Function     | class       | function    |

Table 6: The list of edge types we used to build the project context graph.

# <span id="page-11-2"></span>B Statistics of Retrieved Entities

This section presents the statistics of the retrieved entities of all samples in our dataset. Specifically, we hope to know two things that help us decide the experiment setup: (1) how many entities will be retrieved for the source file (2) how many tokens[8](#page-11-5) are there in the retrieved entity. Table [7](#page-11-6) shows the ratio of source files that will retrieve project entities more than a specific threshold, and Table [8](#page-11-7) ratio of entities that will contain tokens more than a specific threshold. COCOMIC uses 128 as the maximum number of retrieved entities to be included in the model input and tokens within each entity. Consequently, COCOMIC can always consider most of the cross-file context without causing too expensive computational overhead.

<span id="page-11-6"></span>

| Num of entities | > 32  | > 64  | > 128 | > 256 |
|-----------------|-------|-------|-------|-------|
| Ratio (%)       | 42.43 | 22.32 | 8.99  | 6.93  |

Table 7: The ratio of the number of retrieved entities for the source code file with different thresholds.

<span id="page-11-7"></span>

| Num of tokens | > 32  | > 64  | > 128 | > 256 |
|---------------|-------|-------|-------|-------|
| Ratio (%)     | 32.41 | 21.88 | 13.20 | 6.47  |

Table 8: The ratio of the number of tokens within the retrieved entity with different thresholds.

# C Additional Details on Data Preprocessing

As we introduced in [§4.1,](#page-3-2) the model input will be source code and its retrieved cross-file context. For training, we take the code files as samples. If a code file is too long, we split the code sequence into multiple chunks with the maximum length of the model input, and each chunk is paired with the same cross-file context. As the code files are from distinct PyPI projects, we assume the duplicated samples should be rare.

We follow the standard code completion setting for testing [\(Nijkamp et al.,](#page-10-0) [2022;](#page-10-0) [Xu et al.,](#page-10-10) [2022\)](#page-10-10), creating incomplete programs as prompts and asking the model to predict the following pieces of code. Specifically, we create prompts by cutting the source file at the location where completion requires cross-file context. We present the details of samples' sequence length, in terms of BPE subtokens, in Table [9.](#page-11-8) For cross-file context, we concatenate the text of all retrieved entities as a sequence and count the length. We could see that the cross-file context is typically long and could not be consumed as plain text together with the prompts. COCOMIC designs to compress the entities with the special token [SUM] that effectively alleviates this limitation.

<span id="page-11-8"></span>

|                    | Mean  | Max     | Median | Min |
|--------------------|-------|---------|--------|-----|
| Prompts            | 1,354 | 32,599  | 758    | 7   |
| Cross-file context | 4,485 | 186,339 | 1,928  | 22  |

Table 9: Length statistics of prompts and cross-file context of the test set.

## <span id="page-11-3"></span>D Additional Details on Experiments

Our code is based on Transformers [\(Wolf et al.,](#page-10-21) [2020\)](#page-10-21). We train our models on a machine with 8

<span id="page-11-5"></span><span id="page-11-1"></span><sup>8</sup> Practically, these will be BPE sub-tokens from CodeGen tokenizer.

Nvidia A100s. Each job takes around 50 hours (i.e., 400 GPU hours) to train. We perform one round of experiments only as it is very expensive to repeat the experiments many times. The hyperparameter used is from our initial small-scale grid search on hyperparameters, where we find that the final performance is relatively stable.

# E Additional Ablation Studies

# <span id="page-12-2"></span>E.1 k-hop Retrieval

As we see from Table [3,](#page-6-5) k = 1 underperforms comparing to k = 2. This is because k = 1 fetches less comprehensive context. For example, with the import statement import FileA as A, we can access class X's static member function Y as: A.classX.funcY through 2-hop retrieval, whereas 1-hop retrieval will not fetch. In fact, 1-hop retrieval won't fetch any class member function if only the file is imported, which happens frequently in Python. Given the great coverage of k = 2 (Table [2\)](#page-6-4) and given we found too many unrelated entities were retrieved if we use k > 2, we decided to use k = 2 throughout the work.

# <span id="page-12-1"></span>E.2 Additional Baseline Variants

In addition to the CodeGen w/ Cross-file Context baseline ([§5.3\)](#page-5-3) which uses the same cross-file context tokens as in COCOMIC, we experimented with a simplified setting that only takes the locales and the signature prototypes (name, arguments, and default return types, if present) to fit in more cross-file context within the limited token space.

|                                    | Code Match |        | ID Match |                   |      |
|------------------------------------|------------|--------|----------|-------------------|------|
|                                    | EM         | BLEU-4 | EM       | Prec.             | Rec. |
| CodeGen + Finetune                 |            |        |          |                   |      |
| + Cross-file context (full)        | 17.00      | 36.34  |          | 25.80 48.91 54.76 |      |
| + Cross-file context (simp.) 17.49 |            | 37.57  |          | 26.76 51.71 54.88 |      |
| COCOMIC                            | 21.39      | 41.65  |          | 31.26 55.45 57.83 |      |

Table 10: All baselines significantly outperforms CO-COMIC.

<span id="page-12-0"></span>We see the performance only improves marginally when using simplified cross-file context, and it still underperforms COCOMIC significantly. This suggests that baseline models have substantial limitations of sequence lengths that the performance is subpar even if we simplify the cross-file context, while COCOMIC is capable of compressing up to 16,384 (=128x128) tokens of cross-file context into only 128 vectors, making cross-file context readily available for the model to use.

# F Case Study

We present a case study by comparing COCOMIC with the finetuned CodeGen model with and without cross-file context.

# F.1 COCOMIC vs. CodeGen finetuned w/o cross-file context

We present two qualitative examples in Figure [4](#page-13-0) and [5](#page-13-1) where Baseline refers to the CodeGen model finetuned w/o cross-file context. From Figure [4,](#page-13-0) we see that Baseline calls get\_credentials() function of AssumeRoleExecutor class, which does not exist. On the other hand, being able to access cross-file context, COCOMIC predicts the correct function execute() that returns Credentials class instance. In Figure [5,](#page-13-1) we see similar behavior from Baseline as it predicts get\_key\_and\_secret method. In language generation literature, such an inaccurate or unfaithful generation given the input is known as *hallucination* [\(Ji et al.,](#page-9-17) [2022\)](#page-9-17).

# F.2 COCOMIC vs. CodeGen finetuned with cross-file context

We present two qualitative examples in Figure [6](#page-14-0) and [7](#page-15-0) where Baseline refers to the CodeGen model finetuned with cross-file context. From Figure [6,](#page-14-0) we see that Baseline calls parse() method of CommandLineArgs class, which does not exist. On the other hand, due to providing cross-file context, COCOMIC predicts the correct function get\_cli\_args() that returns CliArgs class instance. In Figure [7,](#page-15-0) we see similar behavior from Baseline as it predicts update\_from\_dict(schedule) method. In both cases, the Baseline fails to make an accurate prediction due to the truncation of cross-file context, while COCOMIC predicts correctly as it encodes each cross-file context entity individually and then utilizes their embedding in the self-attention mechanism. In contrast to cross-file context truncation, we could improve the Baseline by effectively selecting the most useful entities from the ordered list of retrieved cross-file entities. We leave this as our future work.

<span id="page-13-0"></span>

Figure 4: COCOMIC *vs.* CodeGen finetuned w/o cross-file context: qualitative example-1.

Figure 5: COCOMIC *vs.* CodeGen finetuned w/o cross-file context: qualitative example-2.

Figure 6: COCOMIC *vs.* CodeGen finetuned with cross-file context: qualitative example-1.

Figure 7: COCOMIC *vs.* CodeGen finetuned with cross-file context: qualitative example-2.