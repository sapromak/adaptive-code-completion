# BERTSCORE: EVALUATING TEXT GENERATION WITH BERT

Tianyi Zhang∗†‡, Varsha Kishore∗‡, Felix Wu∗‡, Kilian Q. Weinberger†‡ , and Yoav Artzi‡§

‡Department of Computer Science and §Cornell Tech, Cornell University {vk352, fw245, kilian}@cornell.edu {yoav}@cs.cornell.edu

ASAPP Inc. tzhang@asapp.com

#### ABSTRACT

We propose BERTSCORE, an automatic evaluation metric for text generation. Analogously to common metrics, BERTSCORE computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTSCORE correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTSCORE is more robust to challenging examples when compared to existing metrics.

#### 1 INTRODUCTION

Automatic evaluation of natural language generation, for example in machine translation and caption generation, requires comparing candidate sentences to annotated references. The goal is to evaluate semantic equivalence. However, commonly used methods rely on surface-form similarity only. For example, BLEU [\(Papineni et al., 2002\)](#page-11-0), the most common machine translation metric, simply counts n-gram overlap between the candidate and the reference. While this provides a simple and general measure, it fails to account for meaning-preserving lexical and compositional diversity.

In this paper, we introduce BERTSCORE, a language generation evaluation metric based on pretrained BERT contextual embeddings [\(Devlin et al., 2019\)](#page-9-0). BERTSCORE computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings.

BERTSCORE addresses two common pitfalls in n-gram-based metrics [\(Banerjee & Lavie, 2005\)](#page-9-1). First, such methods often fail to robustly match paraphrases. For example, given the reference *people like foreign cars*, BLEU and METEOR [\(Banerjee & Lavie, 2005\)](#page-9-1) incorrectly give a higher score to *people like visiting places abroad* compared to *consumers prefer imported cars*. This leads to performance underestimation when semantically-correct phrases are penalized because they differ from the surface form of the reference. In contrast to string matching (e.g., in BLEU) or matching heuristics (e.g., in METEOR), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection [\(Devlin et al., 2019\)](#page-9-0). Second, n-gram models fail to capture distant dependencies and penalize semantically-critical ordering changes [\(Isozaki](#page-10-0) [et al., 2010\)](#page-10-0). For example, given a small window of size two, BLEU will only mildly penalize swapping of cause and effect clauses (e.g. *A because B* instead of *B because A*), especially when the arguments A and B are long phrases. In contrast, contextualized embeddings are trained to effectively capture distant dependencies and ordering.

We experiment with BERTSCORE on machine translation and image captioning tasks using the outputs of 363 systems by correlating BERTSCORE and related metrics to available human judgments. Our experiments demonstrate that BERTSCORE correlates highly with human evaluations. In machine translation, BERTSCORE shows stronger system-level and segment-level correlations with human judgments than existing metrics on multiple common benchmarks and demonstrates

<sup>∗</sup>Equal contribution. † Work done at Cornell.

strong model selection performance compared to BLEU. We also show that BERTSCORE is well-correlated with human annotators for image captioning, surpassing SPICE, a popular taskspecific metric [\(Anderson et al., 2016\)](#page-9-2). Finally, we test the robustness of BERTSCORE on the adversarial paraphrase dataset PAWS [\(Zhang et al., 2019\)](#page-13-0), and show that it is more robust to adversarial examples than other metrics. The code for BERTSCORE is available at [https://github.com/Tiiiger/bert\\_score](https://github.com/Tiiiger/bert_score).

#### <span id="page-1-0"></span>2 PROBLEM STATEMENT AND PRIOR METRICS

Natural language text generation is commonly evaluated using annotated reference sentences. Given a reference sentence x tokenized to k tokens hx1, . . . , xki and a candidate xˆ tokenized to l tokens hxˆ1, . . . , xˆli, a generation evaluation metric is a function f(x, xˆ) ∈ R. Better metrics have a higher correlation with human judgments. Existing metrics can be broadly categorized into using n-gram matching, edit distance, embedding matching, or learned functions.

#### 2.1 n-GRAM MATCHING APPROACHES

The most commonly used metrics for generation count the number of n-grams that occur in the reference x and candidate xˆ. The higher the n is, the more the metric is able to capture word order, but it also becomes more restrictive and constrained to the exact form of the reference.

Formally, let S n x and S n xˆ be the lists of token n-grams (n ∈ Z+) in the reference x and candidate xˆ sentences. The number of matched n-grams is P <sup>w</sup>∈S<sup>n</sup> xˆ I[w ∈ S n x ], where I[·] is an indicator function. The exact match precision (Exact-Pn) and recall (Exact-Rn) scores are:

$$\text{Exact-P}\_n = \frac{\sum\_{w \in S\_x^n} \mathbb{I}[w \in S\_x^n]}{|S\_x^n|} \quad \text{and} \quad \text{Exact-R}\_n = \frac{\sum\_{w \in S\_x^n} \mathbb{I}[w \in S\_x^n]}{|S\_x^n|}.$$

Several popular metrics build upon one or both of these exact matching scores.

BLEU The most widely used metric in machine translation is BLEU [\(Papineni et al., 2002\)](#page-11-0), which includes three modifications to Exact-Pn. First, each n-gram in the reference can be matched at most once. Second, the number of exact matches is accumulated for all reference-candidate pairs in the corpus and divided by the total number of n-grams in all candidate sentences. Finally, very short candidates are discouraged using a brevity penalty. Typically, BLEU is computed for multiple values of n (e.g. n = 1, 2, 3, 4) and the scores are averaged geometrically. A smoothed variant, SENT-BLEU [\(Koehn et al., 2007\)](#page-10-1) is computed at the sentence level. In contrast to BLEU, BERTSCORE is not restricted to maximum n-gram length, but instead relies on contextualized embeddings that are able to capture dependencies of potentially unbounded length.

METEOR METEOR [\(Banerjee & Lavie, 2005\)](#page-9-1) computes Exact-P<sup>1</sup> and Exact-R<sup>1</sup> while allowing backing-off from exact unigram matching to matching word stems, synonyms, and paraphrases. For example, *running* may match *run* if no exact match is possible. Non-exact matching uses an external stemmer, a synonym lexicon, and a paraphrase table. METEOR 1.5 [\(Denkowski & Lavie, 2014\)](#page-9-3) weighs content and function words differently, and also applies importance weighting to different matching types. The more recent METEOR++ 2.0 [\(Guo & Hu, 2019\)](#page-10-2) further incorporates a learned external paraphrase resource. Because METEOR requires external resources, only five languages are supported with the full feature set, and eleven are partially supported. Similar to METEOR, BERTSCORE allows relaxed matches, but relies on BERT embeddings that are trained on large amounts of raw text and are currently available for 104 languages. BERTSCORE also supports importance weighting, which we estimate with simple corpus statistics.

Other Related Metrics NIST [\(Doddington, 2002\)](#page-9-4) is a revised version of BLEU that weighs each n-gram differently and uses an alternative brevity penalty. ∆BLEU [\(Galley et al., 2015\)](#page-9-5) modifies multi-reference BLEU by including human annotated negative reference sentences. CHRF [\(Popovic,´](#page-11-1) [2015\)](#page-11-1) compares character n-grams in the reference and candidate sentences. CHRF++ [\(Popovic,´](#page-11-2) [2017\)](#page-11-2) extends CHRF to include word bigram matching. ROUGE [\(Lin, 2004\)](#page-10-3) is a commonly used metric for summarization evaluation. ROUGE-n [\(Lin, 2004\)](#page-10-3) computes Exact-R<sup>n</sup> (usually n = 1, 2), while ROUGE-L is a variant of Exact-R<sup>1</sup> with the numerator replaced by the length of the longest common subsequence. CIDER [\(Vedantam et al., 2015\)](#page-12-0) is an image captioning metric that computes cosine similarity between tf–idf weighted n-grams. We adopt a similar approach to weigh tokens differently. Finally, [Chaganty et al.](#page-9-6) [\(2018\)](#page-9-6) and [Hashimoto et al.](#page-10-4) [\(2019\)](#page-10-4) combine automatic metrics with human judgments for text generation evaluation.

#### 2.2 EDIT-DISTANCE-BASED METRICS

Several methods use word edit distance or word error rate [\(Levenshtein, 1966\)](#page-10-5), which quantify similarity using the number of edit operations required to get from the candidate to the reference. TER [\(Snover et al., 2006\)](#page-12-1) normalizes edit distance by the number of reference words, and ITER [\(Panja & Naskar, 2018\)](#page-11-3) adds stem matching and better normalization. PER [\(Tillmann et al.,](#page-12-2) [1997\)](#page-12-2) computes position independent error rate, CDER [\(Leusch et al., 2006\)](#page-10-6) models block reordering as an edit operation. CHARACTER [\(Wang et al., 2016\)](#page-12-3) and EED [\(Stanchev et al., 2019\)](#page-12-4) operate on the character level and achieve higher correlation with human judgements on some languages.

#### 2.3 EMBEDDING-BASED METRICS

Word embeddings [\(Mikolov et al., 2013;](#page-11-4) [Pennington et al., 2014;](#page-11-5) [Grave et al., 2018;](#page-10-7) [Nguyen et al.,](#page-11-6) [2017;](#page-11-6) [Athiwaratkun et al., 2018\)](#page-9-7) are learned dense token representations. MEANT 2.0 [\(Lo, 2017\)](#page-10-8) uses word embeddings and shallow semantic parses to compute lexical and structural similarity. YISI-1 [\(Lo et al., 2018\)](#page-11-7) is similar to MEANT 2.0, but makes the use of semantic parses optional. Both methods use a relatively simple similarity computation, which inspires our approach, including using greedy matching [\(Corley & Mihalcea, 2005\)](#page-9-8) and experimenting with a similar importance weighting to YISI-1. However, we use contextual embeddings, which capture the specific use of a token in a sentence, and potentially capture sequence information. We do not use external tools to generate linguistic structures, which makes our approach relatively simple and portable to new languages. Instead of greedy matching, WMD [\(Kusner et al., 2015\)](#page-10-9), WMD<sup>O</sup> [\(Chow et al., 2019\)](#page-9-9), and SMS [\(Clark et al., 2019\)](#page-9-10) propose to use optimal matching based on earth mover's distance [\(Rubner](#page-11-8) [et al., 1998\)](#page-11-8). The tradeoff[1](#page-2-0) between greedy and optimal matching was studied by [Rus & Lintean](#page-11-9) [\(2012\)](#page-11-9). [Sharma et al.](#page-12-5) [\(2018\)](#page-12-5) compute similarity with sentence-level representations. In contrast, our token-level computation allows us to weigh tokens differently according to their importance.

### 2.4 LEARNED METRICS

Various metrics are trained to optimize correlation with human judgments. BEER [\(Stanojevic &´](#page-12-6) [Sima'an, 2014\)](#page-12-6) uses a regression model based on character n-grams and word bigrams. BLEND [\(Ma](#page-11-10) [et al., 2017\)](#page-11-10) uses regression to combine 29 existing metrics. RUSE [\(Shimanaka et al., 2018\)](#page-12-7) combines three pre-trained sentence embedding models. All these methods require costly human judgments as supervision for each dataset, and risk poor generalization to new domains, even within a known language and task [\(Chaganty et al., 2018\)](#page-9-6). [Cui et al.](#page-9-11) [\(2018\)](#page-9-11) and [Lowe et al.](#page-11-11) [\(2017\)](#page-11-11) train a neural model to predict if the input text is human-generated. This approach also has the risk of being optimized to existing data and generalizing poorly to new data. In contrast, the model underlying BERTSCORE is not optimized for any specific evaluation task.

# 3 BERTSCORE

Given a reference sentence x = hx1, . . . , xki and a candidate sentence xˆ = hxˆ1, . . . , xˆli, we use contextual embeddings to represent the tokens, and compute matching using cosine similarity, optionally weighted with inverse document frequency scores. Figure [1](#page-3-0) illustrates the computation.

Token Representation We use contextual embeddings to represent the tokens in the input sentences x and xˆ. In contrast to prior word embeddings [\(Mikolov et al., 2013;](#page-11-4) [Pennington et al.,](#page-11-5) [2014\)](#page-11-5), contextual embeddings, such as BERT [\(Devlin et al., 2019\)](#page-9-0) and ELMO [\(Peters et al., 2018\)](#page-11-12), can generate different vector representations for the same word in different sentences depending on the surrounding words, which form the context of the target word. The models used to generate these embeddings are most commonly trained using various language modeling objectives, such as masked word prediction [\(Devlin et al., 2019\)](#page-9-0).

<span id="page-2-0"></span><sup>1</sup>We provide an ablation study of this design choice in Appendix [C.](#page-18-0)

![](_page_3_Figure_1.jpeg)

<span id="page-3-0"></span>Figure 1: Illustration of the computation of the recall metric RBERT. Given the reference x and candidate xˆ, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy matching in red, and include the optional idf importance weighting.

We experiment with different models (Section [4\)](#page-4-0), using the tokenizer provided with each model. Given a tokenized reference sentence x = hx1, . . . , xki, the embedding model generates a sequence of vectors hx1, . . . , xki. Similarly, the tokenized candidate xˆ = hxˆ1, . . . , xˆmi is mapped to hˆx1, . . . , ˆxli. The main model we use is BERT, which tokenizes the input text into a sequence of word pieces [\(Wu et al., 2016\)](#page-12-8), where unknown words are split into several commonly observed sequences of characters. The representation for each word piece is computed with a Transformer encoder [\(Vaswani et al., 2017\)](#page-12-9) by repeatedly applying self-attention and nonlinear transformations in an alternating fashion. BERT embeddings have been shown to benefit various NLP tasks [\(Devlin](#page-9-0) [et al., 2019;](#page-9-0) [Liu, 2019;](#page-10-10) [Huang et al., 2019;](#page-10-11) [Yang et al., 2019a\)](#page-12-10).

Similarity Measure The vector representation allows for a soft measure of similarity instead of exact-string [\(Papineni et al., 2002\)](#page-11-0) or heuristic [\(Banerjee & Lavie, 2005\)](#page-9-1) matching. The cosine similarity of a reference token x<sup>i</sup> and a candidate token xˆ<sup>j</sup> is <sup>x</sup> > <sup>i</sup> ˆx<sup>j</sup> kxikkˆx<sup>j</sup> k . We use pre-normalized vectors, which reduces this calculation to the inner product x > <sup>i</sup> ˆx<sup>j</sup> . While this measure considers tokens in isolation, the contextual embeddings contain information from the rest of the sentence.

BERTSCORE The complete score matches each token in x to a token in xˆ to compute recall, and each token in xˆ to a token in x to compute precision. We use greedy matching to maximize the matching similarity score,[2](#page-3-1) where each token is matched to the most similar token in the other sentence. We combine precision and recall to compute an F1 measure. For a reference x and candidate xˆ, the recall, precision, and F1 scores are:

$$R\_{\rm BERT} = \frac{1}{|x|} \sum\_{x\_i \in x} \max\_{\hat{x}\_j \in \hat{x}} \mathbf{x}\_i^\top \hat{\mathbf{x}}\_j \quad , \quad P\_{\rm BERT} = \frac{1}{|\hat{x}|} \sum\_{\hat{x}\_j \in \hat{x}} \max\_{x\_i \in x} \mathbf{x}\_i^\top \hat{\mathbf{x}}\_j \quad , \quad F\_{\rm BERT} = 2 \frac{P\_{\rm BERT} \cdot R\_{\rm BERT}}{P\_{\rm BERT} + R\_{\rm BERT}}$$

.

Importance Weighting Previous work on similarity measures demonstrated that rare words can be more indicative for sentence similarity than common words [\(Banerjee & Lavie, 2005;](#page-9-1) [Vedantam](#page-12-0) [et al., 2015\)](#page-12-0). BERTSCORE enables us to easily incorporate importance weighting. We experiment with inverse document frequency (idf) scores computed from the test corpus. Given M reference sentences {x (i)}<sup>M</sup> <sup>i</sup>=1, the idf score of a word-piece token w is

$$\text{idf}(w) = -\log\frac{1}{M} \sum\_{i=1}^{M} \mathbb{I}[w \in x^{(i)}] \; ,$$

where I[·] is an indicator function. We do not use the full tf-idf measure because we process single sentences, where the term frequency (tf) is likely 1. For example, recall with idf weighting is

$$R\_{\text{BERT}} = \frac{\sum\_{x\_i \in x} \text{idf}(x\_i) \max\_{\hat{x}\_j \in \hat{x}} \mathbf{x}\_i^\top \hat{\mathbf{x}}\_j}{\sum\_{x\_i \in x} \text{idf}(x\_i)} \ . $$

Because we use reference sentences to compute idf, the idf scores remain the same for all systems evaluated on a specific test set. We apply plus-one smoothing to handle unknown word pieces.

<span id="page-3-1"></span><sup>2</sup>We compare greedy matching with optimal assignment in Appendix [C.](#page-18-0)

Baseline Rescaling Because we use pre-normalized vectors, our computed scores have the same numerical range of cosine similarity (between −1 and 1). However, in practice we observe scores in a more limited range, potentially because of the learned geometry of contextual embeddings. While this characteristic does not impact BERTSCORE's capability to rank text generation systems, it makes the actual score less readable. We address this by rescaling BERTSCORE with respect to its empirical lower bound b as a baseline. We compute b using Common Crawl monolingual datasets.[3](#page-4-1) For each language and contextual embedding model, we create 1M candidate-reference pairs by grouping two random sentences. Because of the random pairing and the corpus diversity, each pair has very low lexical and semantic overlapping.[4](#page-4-2) We compute b by averaging BERTSCORE computed on these sentence pairs. Equipped with baseline b, we rescale BERTSCORE linearly. For example, the rescaled value Rˆ BERT of RBERT is:

$$
\hat{R}\_{\text{BERT}} = \frac{R\_{\text{BERT}} - b}{1 - b} \ .
$$

After this operation Rˆ BERT is typically between 0 and 1. We apply the same rescaling procedure for PBERT and FBERT. This method does not affect the ranking ability and human correlation of BERTSCORE, and is intended solely to increase the score readability.

#### <span id="page-4-0"></span>4 EXPERIMENTAL SETUP

We evaluate our approach on machine translation and image captioning.

Contextual Embedding Models We evaluate twelve pre-trained contextual embedding models, including variants of BERT [\(Devlin et al., 2019\)](#page-9-0), RoBERTa [\(Liu et al., 2019b\)](#page-10-12), XLNet [\(Yang et al.,](#page-12-11) [2019b\)](#page-12-11), and XLM [\(Lample & Conneau, 2019\)](#page-10-13). We present the best-performing models in Section [5.](#page-5-0) We use the 24-layer RoBERTalarge model[5](#page-4-3) for English tasks, 12-layer BERTchinese model for Chinese tasks, and the 12-layer cased multilingual BERTmulti model for other languages.[6](#page-4-4) We show the performance of all other models in Appendix [F.](#page-22-0) Contextual embedding models generate embedding representations at every layer in the encoder network. Past work has shown that intermediate layers produce more effective representations for semantic tasks [\(Liu et al., 2019a\)](#page-10-14). We use the WMT16 dataset [\(Bojar et al., 2016\)](#page-9-12) as a validation set to select the best layer of each model (Appendix [B\)](#page-16-0).

Machine Translation Our main evaluation corpus is the WMT18 metric evaluation dataset [\(Ma](#page-11-13) [et al., 2018\)](#page-11-13), which contains predictions of 149 translation systems across 14 language pairs, gold references, and two types of human judgment scores. Segment-level human judgments assign a score to each reference-candidate pair. System-level human judgments associate each system with a single score based on all pairs in the test set. WMT18 includes translations from English to Czech, German, Estonian, Finnish, Russian, and Turkish, and from the same set of languages to English. We follow the WMT18 standard practice and use absolute Pearson correlation |ρ| and Kendall rank correlation τ to evaluate metric quality, and compute significance with the Williams test [\(Williams, 1959\)](#page-12-12) for |ρ| and bootstrap re-sampling for τ as suggested by [Graham & Baldwin](#page-9-13) [\(2014\)](#page-9-13). We compute systemlevel scores by averaging BERTSCORE for every reference-candidate pair. We also experiment with hybrid systems by randomly sampling one candidate sentence from one of the available systems for each reference sentence [\(Graham & Liu, 2016\)](#page-10-15). This enables system-level experiments with a higher number of systems. Human judgments of each hybrid system are created by averaging the WMT18 segment-level human judgments for the corresponding sentences in the sampled data. We compare BERTSCOREs to one canonical metric for each category introduced in Section [2,](#page-1-0) and include the comparison with all other participating metrics from WMT18 in Appendix [F.](#page-22-0)

In addition to the standard evaluation, we design model selection experiments. We use 10K hybrid systems super-sampled from WMT18. We randomly select 100 out of 10K hybrid systems, and rank them using the automatic metrics. We repeat this process 100K times. We report the percentage of the metric ranking agreeing with the human ranking on the best system (Hits@1). In Tables [23-](#page-35-0)[28,](#page-40-0)

<span id="page-4-1"></span><sup>3</sup><https://commoncrawl.org/>

<span id="page-4-3"></span><span id="page-4-2"></span><sup>4</sup>BLEU computed on these pairs is around zero.

<sup>5</sup>We use the tokenizer provided with each model. For all Hugging Face models that use the GPT-2 tokenizer, at the time of our experiments, the tokenizer adds a space to the beginning of each sentence.

<span id="page-4-4"></span><sup>6</sup>All the models used are from [https://github.com/huggingface/pytorch-transformers.](https://github.com/huggingface/pytorch-transformers)

| Metric         | en↔cs<br>(5/5) | en↔de<br>(16/16) | en↔et<br>(14/14) | en↔fi<br>(9/12) | en↔ru<br>(8/9) | en↔tr<br>(5/8) | en↔zh<br>(14/14) |
|----------------|----------------|------------------|------------------|-----------------|----------------|----------------|------------------|
| BLEU           | .970/.995      | .971/.981        | .986/.975        | .973/.962       | .979/.983      | .657/.826      | .978/.947        |
| ITER           | .975/.915      | .990/.984        | .975/.981        | .996/.973       | .937/.975      | .861/.865      | .980/<br>–       |
| RUSE           | .981/<br>–     | .997/<br>–       | .990/<br>–       | .991/<br>–      | .988/<br>–     | .853/<br>–     | .981/<br>–       |
| YiSi-1         | .950/.987      | .992/.985        | .979/.979        | .973/.940       | .991/.992      | .958/.976      | .951/.963        |
| PBERT          | .980/.994      | .998/.988        | .990/.981        | .995/.957       | .982/.990      | .791/.935      | .981/.954        |
| RBERT          | .998/.997      | .997/.990        | .986/.980        | .997/.980       | .995/.989      | .054/.879      | .990/.976        |
| FBERT          | .990/.997      | .999/.989        | .990/.982        | .998/.972       | .990/.990      | .499/.908      | .988/.967        |
| FBERT<br>(idf) | .985/.995      | .999/.990        | .992/.981        | .992/.972       | .991/.991      | .826/.941      | .989/.973        |

<span id="page-5-1"></span>Table 1: Absolute Pearson correlations with system-level human judgments on WMT18. For each language pair, the left number is the to-English correlation, and the right is the from-English. We bold correlations of metrics not significantly outperformed by any other metric under Williams Test for that language pair and direction. The numbers in parenthesis are the number of systems used for each language pair and direction.

| Metric         | en↔cs      | en↔de      | en↔et      | en↔fi      | en↔ru      | en↔tr      | en↔zh      |
|----------------|------------|------------|------------|------------|------------|------------|------------|
| BLEU           | .956/.993  | .969/.977  | .981/.971  | .962/.958  | .972/.977  | .586/.796  | .968/.941  |
| ITER           | .966/.865  | .990/.978  | .975/.982  | .989/.966  | .943/.965  | .742/.872  | .978/<br>– |
| RUSE           | .974/<br>– | .996/<br>– | .988/<br>– | .983/<br>– | .982/<br>– | .780/<br>– | .973/<br>– |
| YiSi-1         | .942/.985  | .991/.983  | .976/.976  | .964/.938  | .985/.989  | .881/.942  | .943/.957  |
| PBERT          | .965/.989  | .995/.983  | .990/.970  | .976/.951  | .976/.988  | .846/.936  | .975/.950  |
| RBERT          | .989/.995  | .997/.991  | .982/.979  | .989/.977  | .988/.989  | .540/.872  | .981/.980  |
| FBERT          | .978/.993  | .998/.988  | .989/.978  | .983/.969  | .985/.989  | .760/.910  | .981/.969  |
| FBERT<br>(idf) | .982/.995  | .998/.988  | .988/.979  | .989/.969  | .983/.987  | .453/.877  | .980/.963  |

Table 2: Absolute Pearson correlations with system-level human judgments on WMT18. We use 10K hybrid super-sampled systems for each language pair and direction. For each language pair, the left number is the to-English correlation, and the right is the from-English. Bolding criteria is the same as in Table [1.](#page-5-1)

we include two additional measures to the model selection study: (a) the mean reciprocal rank of the top metric-rated system according to the human ranking, and (b) the difference between the human score of the top human-rated system and that of the top metric-rated system.

Additionally, we report the same study on the WMT17 [\(Bojar et al., 2017\)](#page-9-14) and the WMT16 [\(Bojar](#page-9-12) [et al., 2016\)](#page-9-12) datasests in Appendix [F.](#page-22-0)[7](#page-5-2) This adds 202 systems to our evaluation.

Image Captioning We use the human judgments of twelve submission entries from the COCO 2015 Captioning Challenge. Each participating system generates a caption for each image in the COCO validation set [\(Lin et al., 2014\)](#page-10-16), and each image has approximately five reference captions. Following [Cui et al.](#page-9-11) [\(2018\)](#page-9-11), we compute the Pearson correlation with two system-level metrics: the percentage of captions that are evaluated as better or equal to human captions (M1) and the percentage of captions that are indistinguishable from human captions (M2). We compute BERTSCORE with multiple references by scoring the candidate with each available reference and returning the highest score. We compare with eight task-agnostic metrics: BLEU [\(Papineni et al.,](#page-11-0) [2002\)](#page-11-0), METEOR [\(Banerjee & Lavie, 2005\)](#page-9-1), ROUGE-L [\(Lin, 2004\)](#page-10-3), CIDER [\(Vedantam et al., 2015\)](#page-12-0), BEER [\(Stanojevic & Sima'an, 2014\)](#page-12-6), EED [\(Stanchev et al., 2019\)](#page-12-4), ´ CHRF++ [\(Popovic, 2017\)](#page-11-2), and ´ CHARACTER [\(Wang et al., 2016\)](#page-12-3). We also compare with two task-specific metrics: SPICE [\(Ander](#page-9-2)[son et al., 2016\)](#page-9-2) and LEIC [\(Cui et al., 2018\)](#page-9-11). SPICE is computed using the similarity of scene graphs parsed from the reference and candidate captions. LEIC is trained to predict if a caption is written by a human given the image.

<span id="page-5-2"></span><span id="page-5-0"></span><sup>7</sup> For WMT16, we only conduct segment-level experiments on to-English pairs due to errors in the dataset.

| en↔zh      |
|------------|
|            |
|            |
| .658/.515  |
| .673/<br>– |
| .670/<br>– |
| .613/.594  |
| .661/.551  |
| .677/.657  |
| .673/.629  |
| .678/.595  |
|            |

<span id="page-6-0"></span>Table 3: Model selection accuracies (Hits@1) on WMT18 hybrid systems. We report the average of 100K samples and the 0.95 confidence intervals are below 10−<sup>3</sup> . We bold the highest numbers for each language pair and direction.

| Metric         | en↔cs<br>(5k/5k) | en↔de<br>(78k/ 20k) | en↔et<br>(57k/32k) | en↔fi<br>(16k/10k) | en↔ru<br>(10k/22k) | en↔tr<br>(9k/1k) | en↔zh<br>(33k/29k) |
|----------------|------------------|---------------------|--------------------|--------------------|--------------------|------------------|--------------------|
| BLEU           | .233/.389        | .415/.620           | .285/.414          | .154/.355          | .228/.330          | .145/.261        | .178/.311          |
| ITER           | .198/.333        | .396/.610           | .235/.392          | .128/.311          | .139/.291          | -.029/.236       | .144/<br>–         |
| RUSE           | .347/<br>–       | .498/<br>–          | .368/<br>–         | .273/<br>–         | .311/<br>–         | .259/<br>–       | .218/<br>–         |
| YiSi-1         | .319/.496        | .488/.691           | .351/.546          | .231/.504          | .300/.407          | .234/.418        | .211/.323          |
| PBERT          | .387/.541        | .541/.715           | .389/.549          | .283/.486          | .345/.414          | .280/.328        | .248/.337          |
| RBERT          | .388/.570        | .546/.728           | .391/.594          | .304/.565          | .343/.420          | .290/.411        | .255/.367          |
| FBERT          | .404/.562        | .550/.728           | .397/.586          | .296/.546          | .353/.423          | .292/.399        | .264/.364          |
| FBERT<br>(idf) | .408/.553        | .550/.721           | .395/585           | .293/.537          | .346/.425          | .296/.406        | .260/.366          |

<span id="page-6-1"></span>Table 4: Kendall correlations with segment-level human judgments on WMT18. For each language pair, the left number is the to-English correlation, and the right is the from-English. We bold correlations of metrics not significantly outperformed by any other metric under bootstrap sampling for that language pair and direction. The numbers in parenthesis are the number of candidate-reference sentence pairs for each language pair and direction.

### 5 RESULTS

Machine Translation Tables [1–](#page-5-1)[3](#page-6-0) show system-level correlation to human judgements, correlations on hybrid systems, and model selection performance. We observe that BERTSCORE is consistently a top performer. In to-English results, RUSE [\(Shimanaka et al., 2018\)](#page-12-7) shows competitive performance. However, RUSE is a supervised method trained on WMT16 and WMT15 human judgment data. In cases where RUSE models were not made available, such as for our from-English experiments, it is not possible to use RUSE without additional data and training. Table [4](#page-6-1) shows segment-level correlations. We see that BERTSCORE exhibits significantly higher performance compared to the other metrics. The large improvement over BLEU stands out, making BERTSCORE particularly suitable to analyze specific examples, where SENTBLEU is less reliable. In Appendix [A,](#page-14-0) we provide qualitative examples to illustrate the segment-level performance difference between SENTBLEU and BERTSCORE. At the segment-level, BERTSCORE even significantly outperforms RUSE. Overall, we find that applying importance weighting using idf at times provides small benefit, but in other cases does not help. Understanding better when such importance weighting is likely to help is an important direction for future work, and likely depends on the domain of the text and the available test data. We continue without idf weighting for the rest of our experiments. While recall RBERT, precision PBERT, and F1 FBERT alternate as the best measure in different setting, F1 FBERT performs reliably well across all the different settings. Our overall recommendation is therefore to use F1. We present additional results using the full set of 351 systems and evaluation metrics in Tables [12](#page-24-0)[–28](#page-40-0) in the appendix, including for experiments with idf importance weighting, different contextual embedding models, and model selection.

Image Captioning Table [5](#page-7-0) shows correlation results for the COCO Captioning Challenge. BERTSCORE outperforms all task-agnostic baselines by large margins. Image captioning presents a challenging evaluation scenario, and metrics based on strict n-gram matching, including BLEU and ROUGE, show weak correlations with human judgments. idf importance weighting shows signifi-

| Metric         | M1      | M2      |  |
|----------------|---------|---------|--|
| BLEU           | -0.019∗ | -0.005∗ |  |
| METEOR         | 0.606∗  | 0.594∗  |  |
| ROUGE-L        | 0.090∗  | 0.096∗  |  |
| CIDER          | 0.438∗  | 0.440∗  |  |
| SPICE          | 0.759∗  | 0.750∗  |  |
| LEIC           | 0.939∗  | 0.949∗  |  |
| BEER           | 0.491   | 0.562   |  |
| EED            | 0.545   | 0.599   |  |
| CHRF++         | 0.702   | 0.729   |  |
| CHARACTER      | 0.800   | 0.801   |  |
| PBERT          | -0.105  | -0.041  |  |
| RBERT          | 0.888   | 0.863   |  |
| FBERT          | 0.322   | 0.350   |  |
| RBERT<br>(idf) | 0.917   | 0.889   |  |

| Type                                            | Method                                                          | QQP                                                         | PAWSQQP                                                     |
|-------------------------------------------------|-----------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|
| Trained on QQP<br>(supervised)                  | DecAtt<br>DIIN<br>BERT                                          | 0.939∗<br>0.952∗<br>0.963∗                                  | 0.263<br>0.324<br>0.351                                     |
| Trained on QQP<br>+ PAWSQQP<br>(supervised)     | DecAtt<br>DIIN<br>BERT                                          | -<br>-<br>-                                                 | 0.511<br>0.778<br>0.831                                     |
| Metric<br>(Not trained<br>on QQP or<br>PAWSQQP) | BLEU<br>METEOR<br>ROUGE-L<br>CHRF++<br>BEER<br>EED<br>CHARACTER | 0.707<br>0.755<br>0.740<br>0.577<br>0.741<br>0.743<br>0.698 | 0.527<br>0.532<br>0.536<br>0.608<br>0.564<br>0.611<br>0.650 |
|                                                 | PBERT<br>RBERT<br>FBERT<br>FBERT (idf)                          | 0.757<br>0.744<br>0.761<br>0.777                            | 0.687<br>0.685<br>0.685<br>0.693                            |

<span id="page-7-0"></span>Table 5: Pearson correlation on the 2015 COCO Captioning Challenge. The M1 and M2 measures are described in Section [4.](#page-4-0) LEIC uses images as additional inputs. Numbers with <sup>∗</sup> are cited from [Cui et al.](#page-9-11) [\(2018\)](#page-9-11). We bold the highest correlations of task-specific and task-agnostic metrics.

<span id="page-7-1"></span>Table 6: Area under ROC curve (AUC) on QQP and PAWSQQP datasets. The scores of trained DecATT [\(Parikh et al., 2016\)](#page-11-14), DIIN [\(Gong et al., 2018\)](#page-9-15), and fine-tuned BERT are reported by [Zhang et al.](#page-13-0) [\(2019\)](#page-13-0). Numbers with <sup>∗</sup> are scores on the held-out test set of QQP. We bold the highest correlations of taskspecific and task-agnostic metrics.

cant benefit for this task, suggesting people attribute higher importance to content words. Finally, LEIC [\(Cui et al., 2018\)](#page-9-11), a trained metric that takes images as additional inputs and is optimized specifically for the COCO data and this set of systems, outperforms all other methods.

Speed Despite the use of a large pre-trained model, computing BERTSCORE is relatively fast. We are able to process 192.5 candidate-reference pairs/second using a GTX-1080Ti GPU. The complete WMT18 en-de test set, which includes 2,998 sentences, takes 15.6sec to process, compared to 5.4sec with SacreBLEU [\(Post, 2018\)](#page-11-15), a common BLEU implementation. Given the sizes of commonly used test and validation sets, the increase in processing time is relatively marginal, and BERTSCORE is a good fit for using during validation (e.g., for stopping) and testing, especially when compared to the time costs of other development stages.

## <span id="page-7-2"></span>6 ROBUSTNESS ANALYSIS

We test the robustness of BERTSCORE using adversarial paraphrase classification. We use the Quora Question Pair corpus (QQP; [Iyer et al., 2017\)](#page-10-17) and the adversarial paraphrases from the Paraphrase Adversaries from Word Scrambling dataset (PAWS; [Zhang et al., 2019\)](#page-13-0). Both datasets contain pairs of sentences labeled to indicate whether they are paraphrases or not. Positive examples in QQP are real duplicate questions, while negative examples are related, but different questions. Sentence pairs in PAWS are generated through word swapping. For example, in PAWS, *Flights from New York to Florida* may be changed to *Flights from Florida to New York* and a good classifier should identify that these two sentences are not paraphrases. PAWS includes two parts: PAWSQQP, which is based on the QQP data, and PAWSWiki. We use the PAWSQQP development set which contains 667 sentences. For the automatic metrics, we use no paraphrase detection training data. We expect that pairs with higher scores are more likely to be paraphrases. To evaluate the automatic metrics on QQA, we use the first 5,000 sentences in the training set instead of the the test set because the test labels are not available. We treat the first sentence as the reference and the second sentence as the candidate.

Table [6](#page-7-1) reports the area under ROC curve (AUC) for existing models and automatic metrics. We observe that supervised classifiers trained on QQP perform worse than random guess on PAWSQQP, which shows these models predict the adversarial examples are more likely to be paraphrases. When adversarial examples are provided in training, state-of-the-art models like DIIN [\(Gong et al., 2018\)](#page-9-15) and fine-tuned BERT are able to identify the adversarial examples but their performance still decreases significantly from their performance on QQP. Most metrics have decent performance on QQP, but show a significant performance drop on PAWSQQP, almost down to chance performance. This suggests these metrics fail to to distinguish the harder adversarial examples. In contrast, the performance of BERTSCORE drops only slightly, showing more robustness than the other metrics.

## 7 DISCUSSION

We propose BERTSCORE, a new metric for evaluating generated text against gold standard references. BERTSCORE is purposely designed to be simple, task agnostic, and easy to use. Our analysis illustrates how BERTSCORE resolves some of the limitations of commonly used metrics, especially on challenging adversarial examples. We conduct extensive experiments with various configuration choices for BERTSCORE, including the contextual embedding model used and the use of importance weighting. Overall, our extensive experiments, including the ones in the appendix, show that BERTSCORE achieves better correlation than common metrics, and is effective for model selection. However, there is no one configuration of BERTSCORE that clearly outperforms all others. While the differences between the top configurations are often small, it is important for the user to be aware of the different trade-offs, and consider the domain and languages when selecting the exact configuration to use. In general, for machine translation evaluation, we suggest using FBERT, which we find the most reliable. For evaluating text generation in English, we recommend using the 24 layer RoBERTalarge model to compute BERTSCORE. For non-English language, the multilingual BERTmulti is a suitable choice although BERTSCORE computed with this model has less stable performance on low-resource languages. We report the optimal hyperparameter for all models we experimented with in Appendix [B](#page-16-0)

Briefly following our initial preprint publication, [Zhao et al.](#page-13-1) [\(2019\)](#page-13-1) published a concurrently developed method related to ours, but with a focus on integrating contextual word embeddings with earth mover's distance (EMD; [Rubner et al., 1998\)](#page-11-8) rather than our simple matching process. They also propose various improvements compared to our use of contextualized embeddings. We study these improvements in Appendix [C](#page-18-0) and show that integrating them into BERTSCORE makes it equivalent or better than the EMD-based approach. Largely though, the effect of the different improvements on BERTSCORE is more modest compared to their method. Shortly after our initial publication, YiSi-1 was updated to use BERT embeddings, showing improved performance [\(Lo, 2019\)](#page-10-18). This further corroborates our findings. Other recent related work includes training a model on top of BERT to maximize the correlation with human judgments [\(Mathur et al., 2019\)](#page-11-16) and evaluating generation with a BERT model fine-tuned on paraphrasing [\(Yoshimura et al., 2019\)](#page-12-13). More recent work shows the potential of using BERTSCORE for training a summarization system [\(Li et al., 2019\)](#page-10-19) and for domain-specific evaluation using SciBERT [\(Beltagy et al., 2019\)](#page-9-16) to evaluate abstractive text summarization [\(Gabriel et al., 2019\)](#page-9-17).

In future work, we look forward to designing new task-specific metrics that use BERTSCORE as a subroutine and accommodate task-specific needs, similar to how [Wieting et al.](#page-12-14) [\(2019\)](#page-12-14) suggests to use semantic similarity for machine translation training. Because BERTSCORE is fully differentiable, it also can be incorporated into a training procedure to compute a learning loss that reduces the mismatch between optimization and evaluation objectives.

#### ACKNOWLEDGEMENT

This research is supported in part by grants from the National Science Foundation (III-1618134, III-1526012, IIS1149882, IIS-1724282, TRIPODS-1740822, CAREER-1750499), the Office of Naval Research DOD (N00014-17-1-2175), and the Bill and Melinda Gates Foundation, SAP, Zillow, Workday, and Facebook Research. We thank Graham Neubig and David Grangier for for their insightful comments. We thank the Cornell NLP community including but not limited to Claire Cardie, Tianze Shi, Alexandra Schofield, Gregory Yauney, and Rishi Bommasani. We thank Yin Cui and Guandao Yang for their help with the COCO 2015 dataset.

#### REFERENCES

- <span id="page-9-2"></span>Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: Semantic propositional image caption evaluation. In *ECCV*, 2016.
- <span id="page-9-7"></span>Ben Athiwaratkun, Andrew Wilson, and Anima Anandkumar. Probabilistic fasttext for multi-sense word embeddings. In *ACL*, 2018.
- <span id="page-9-1"></span>Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In *IEEvaluation@ACL*, 2005.
- <span id="page-9-16"></span>Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. *ArXiv*, 2019.
- <span id="page-9-12"></span>Ondˇrej Bojar, Yvette Graham, Amir Kamran, and Miloš Stanojevic. Results of the WMT16 metrics ´ shared task. In *WMT*, 2016.
- <span id="page-9-14"></span>Ondˇrej Bojar, Yvette Graham, and Amir Kamran. Results of the WMT17 metrics shared task. In *WMT*, 2017.
- <span id="page-9-6"></span>Arun Chaganty, Stephen Mussmann, and Percy Liang. The price of debiasing automatic metrics in natural language evalaution. In *ACL*, 2018.
- <span id="page-9-9"></span>Julian Chow, Lucia Specia, and Pranava Madhyastha. WMDO: Fluency-based word mover's distance for machine translation evaluation. In *WMT*, 2019.
- <span id="page-9-10"></span>Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In *ACL*, 2019.
- <span id="page-9-8"></span>Courtney Corley and Rada Mihalcea. Measuring the semantic similarity of texts. In *ACL Workshop*, EMSEE '05, 2005.
- <span id="page-9-11"></span>Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, and Serge J. Belongie. Learning to evaluate image captioning. In *CVPR*, 2018.
- <span id="page-9-3"></span>Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation for any target language. In *WMT@ACL*, 2014.
- <span id="page-9-0"></span>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In *NAACL-HLT*, 2019.
- <span id="page-9-4"></span>George Doddington. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In *HLT*, 2002.
- <span id="page-9-19"></span>William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In *IWP*, 2005.
- <span id="page-9-17"></span>Saadia Gabriel, Antoine Bosselut, Ari Holtzman, Kyle Lo, Asli Çelikyilmaz, and Yejin Choi. Cooperative generator-discriminator networks for abstractive summarization with narrative flow. *ArXiv*, 2019.
- <span id="page-9-5"></span>Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, and William B. Dolan. deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets. In *ACL*, 2015.
- <span id="page-9-18"></span>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In *ICML*, 2017.
- <span id="page-9-15"></span>Yichen Gong, Heng Luo, and Jian Zhang. Natural language inference over interaction space. In *ICLR*, 2018.
- <span id="page-9-13"></span>Yvette Graham and Timothy Baldwin. Testing for significance of increased correlation with human judgment. In *EMNLP*, 2014.
- <span id="page-10-15"></span>Yvette Graham and Qun Liu. Achieving accurate conclusions in evaluation of automatic machine translation metrics. In *NAACL*, 2016.
- <span id="page-10-7"></span>Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word vectors for 157 languages. *arXiv preprint arXiv:1802.06893*, 2018.
- <span id="page-10-2"></span>Yinuo Guo and Junfeng Hu. Meteor++ 2.0: Adopt syntactic level paraphrase knowledge into machine translation evaluation. In *WMT*, 2019.
- <span id="page-10-4"></span>Tatsu Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical evaluation for natural language generation. In *NAACL-HLT*, 2019.
- <span id="page-10-11"></span>Chenyang Huang, Amine Trabelsi, and Osmar R Zaïane. ANA at semeval-2019 task 3: Contextual emotion detection in conversations through hierarchical LSTMs and BERT. *arXiv preprint arXiv:1904.00132*, 2019.
- <span id="page-10-0"></span>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. Automatic evaluation of translation quality for distant language pairs. In *EMNLP*, 2010.
- <span id="page-10-17"></span>Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First quora dataset release: Question pairs. <https://tinyurl.com/y2y8u5ed>, 2017.
- <span id="page-10-1"></span>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In *ACL*, 2007.
- <span id="page-10-9"></span>Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In *ICML*, 2015.
- <span id="page-10-13"></span>Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. *arXiv*, 2019.
- <span id="page-10-6"></span>Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDER: Efficient MT evaluation using block movements. In *EACL*, 2006.
- <span id="page-10-5"></span>Vladimir Iosifovich Levenshtein. Binary Codes Capable of Correcting Deletions, Insertions and Rever sals. *Soviet Physics Doklady*, 10, 1966.
- <span id="page-10-19"></span>Siyao Li, Deren Lei, Pengda Qin, and William Yang Wang. Deep reinforcement learning with distributional semantic rewards for abstractive summarization. In *EMNLP-IJCNLP*, 2019.
- <span id="page-10-3"></span>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In *ACL*, 2004.
- <span id="page-10-16"></span>Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In *ECCV*, 2014.
- <span id="page-10-14"></span>Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. *arXiv preprint arXiv:1903.08855*, 2019a.

<span id="page-10-10"></span>Yang Liu. Fine-tune BERT for extractive summarization. *arXiv preprint arXiv:1903.10318*, 2019.

<span id="page-10-12"></span>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. *arXiv*, abs/1907.11692, 2019b.

<span id="page-10-8"></span>Chi-kiu Lo. MEANT 2.0: Accurate semantic mt evaluation for any output language. In *WMT*, 2017.

<span id="page-10-18"></span>Chi-kiu Lo. YiSi - a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. In *WMT*, 2019.

- <span id="page-11-7"></span>Chi-kiu Lo, Michel Simard, Darlene Stewart, Samuel Larkin, Cyril Goutte, and Patrick Littell. Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric: The NRC supervised submissions to the parallel corpus filtering task. In *WMT*, 2018.
- <span id="page-11-11"></span>Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. Towards an automatic Turing test: Learning to evaluate dialogue responses. In *ACL*, 2017.
- <span id="page-11-10"></span>Qingsong Ma, Yvette Graham, Shugen Wang, and Qun Liu. Blend: a novel combined mt metric based on direct assessment – casict-dcu submission to WMT17 metrics task. In *WMT*, 2017.
- <span id="page-11-13"></span>Qingsong Ma, Ondrej Bojar, and Yvette Graham. Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance. In *WMT*, 2018.
- <span id="page-11-16"></span>Nitika Mathur, Timothy Baldwin, and Trevor Cohn. Putting evaluation in context: Contextual embeddings improve machine translation evaluation. In *ACL*, 2019.
- <span id="page-11-4"></span>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In *NIPS*, 2013.
- <span id="page-11-6"></span>Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. A mixture model for learning multi-sense word embeddings. In *ACL*, 2017.
- <span id="page-11-18"></span>Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In *WMT*, 2018.
- <span id="page-11-19"></span>Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. *arXiv preprint arXiv:1904.01038*, 2019.
- <span id="page-11-3"></span>Joybrata Panja and Sudip Kumar Naskar. Iter: Improving translation edit rate through optimizable edit costs. In *WMT*, 2018.
- <span id="page-11-0"></span>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In *ACL*, 2002.
- <span id="page-11-14"></span>Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In *EMNLP*, 2016.
- <span id="page-11-5"></span>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In *EMNLP*, 2014.
- <span id="page-11-12"></span>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke S. Zettlemoyer. Deep contextualized word representations. In *NAACL-HLT*, 2018.
- <span id="page-11-1"></span>Maja Popovic. chrf: character n-gram f-score for automatic mt evaluation. In ´ *WMT@ACL*, 2015.
- <span id="page-11-2"></span>Maja Popovic. chrf++: words helping character n-grams. In ´ *WMT*, 2017.
- <span id="page-11-15"></span>Matt Post. A call for clarity in reporting BLEU scores. In *WMT*, 2018.
- <span id="page-11-17"></span>Nils Reimers and Iryna Gurevych. Alternative weighting schemes for elmo embeddings. *arXiv preprint arXiv:1904.02954*, 2019.
- <span id="page-11-8"></span>Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. A metric for distributions with applications to image databases. In *ICCV*. IEEE, 1998.
- <span id="page-11-9"></span>Vasile Rus and Mihai Lintean. A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics. In *Proceedings of the Seventh Workshop on Building Educational Applications Using NLP*. ACL, 2012.
- <span id="page-12-16"></span>Andreas Rücklé, Steffen Eger, Maxime Peyrard, and Iryna Gurevych. Concatenated power mean word embeddings as universal cross-lingual sentence representations. *arXiv*, 2018.
- <span id="page-12-5"></span>Shikhar Sharma, Layla El Asri, Hannes Schulz, and Jeremie Zumer. Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. *arXiv preprint arXiv:1706.09799*, 2018.
- <span id="page-12-7"></span>Hiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru Komachi. Ruse: Regressor using sentence embeddings for automatic machine translation evaluation. In *WMT*, 2018.
- <span id="page-12-1"></span>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of translation edit rate with targeted human annotation. In *AMTA*, 2006.
- <span id="page-12-4"></span>Peter Stanchev, Weiyue Wang, and Hermann Ney. EED: Extended edit distance measure for machine translation. In *WMT*, 2019.
- <span id="page-12-6"></span>Miloš Stanojevic and Khalil Sima'an. Beer: Better evaluation as ranking. In ´ *WMT*, 2014.
- <span id="page-12-2"></span>Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf. Accelerated dp based search for statistical translation. In *EUROSPEECH*, 1997.
- <span id="page-12-17"></span>Kristina Toutanova, Chris Brockett, Ke M Tran, and Saleema Amershi. A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs. In *EMNLP*, 2016.
- <span id="page-12-9"></span>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *NIPS*, 2017.
- <span id="page-12-0"></span>Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image description evaluation. In *CVPR*, 2015.
- <span id="page-12-3"></span>Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. Character: Translation edit rate on character level. In *WMT*, 2016.
- <span id="page-12-14"></span>John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. Beyond BLEU:training neural machine translation with semantic similarity. In *ACL*, 2019.
- <span id="page-12-15"></span>Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In *ACL*, 2018.
- <span id="page-12-12"></span>Evan James Williams. *Regression analysis*. wiley, 1959.
- <span id="page-12-18"></span>Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In *ICLR*, 2019.
- <span id="page-12-8"></span>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint arXiv:1609.08144*, 2016.
- <span id="page-12-10"></span>Wei Yang, Haotian Zhang, and Jimmy Lin. Simple applications of BERT for ad hoc document retrieval. *arXiv preprint arXiv:1903.10972*, 2019a.
- <span id="page-12-11"></span>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. *arXiv*, 2019b.
- <span id="page-12-13"></span>Ryoma Yoshimura, Hiroki Shimanaka, Yukio Matsumura, Hayahide Yamagishi, and Mamoru Komachi. Filtering pseudo-references by paraphrasing for automatic evaluation of machine translation. In *WMT*, 2019.
- <span id="page-13-0"></span>Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. *arXiv preprint arXiv:1904.01130*, 2019.
- <span id="page-13-1"></span>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In *EMNLP*, 2019.

| Case                | No. | Reference and Candidate Pairs                                                                                                                                                               | Human | FBERT | BLEU |
|---------------------|-----|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|-------|------|
| BLEU<br>><br>FBERT  | 1.  | x: At the same time Kingfisher is closing 60 B&Q outlets across the country<br>xˆ: At the same time, Kingfisher will close 60 B & Q stores nationwide                                       | 38    | 125   | 530  |
|                     | 2.  | x: Hewlett-Packard to cut up to 30,000 jobs<br>xˆ: Hewlett-Packard will reduce jobs up to 30.000                                                                                            | 119   | 39    | 441  |
|                     | 3.  | x: According to opinion in Hungary, Serbia is "a safe third country".<br>xˆ: According to Hungarian view, Serbia is a "safe third country."                                                 | 23    | 96    | 465  |
|                     | 4.  | x: Experts believe November's Black Friday could be holding back spending.<br>xˆ: Experts believe that the Black Friday in November has put the brakes on spending                          | 73    | 147   | 492  |
|                     | 5.  | x: And it's from this perspective that I will watch him die.<br>xˆ: And from this perspective, I will see him die.                                                                          | 37    | 111   | 414  |
|                     | 6.  | x: In their view the human dignity of the man had been violated.<br>xˆ: Look at the human dignity of the man injured.                                                                       | 500   | 470   | 115  |
| FBERT<br>><br>BLEU  | 8.  | x: For example when he steered a shot from Ideye over the crossbar in the 56th minute.<br>xˆ: So, for example, when he steered a shot of Ideye over the latte (56th).                       | 516   | 524   | 185  |
|                     | 7.  | x: A good prank is funny, but takes moments to reverse.<br>xˆ: A good prank is funny, but it takes only moments before he becomes a boomerang.                                              | 495   | 424   | 152  |
|                     | 9.  | x: I will put the pressure on them and onus on them to make a decision.<br>xˆ: I will exert the pressure on it and her urge to make a decision.                                             | 507   | 471   | 220  |
|                     | 10. | x: Transport for London is not amused by this flyposting "vandalism."<br>xˆ: Transport for London is the Plaka animal "vandalism" is not funny.                                             | 527   | 527   | 246  |
| Human<br>><br>FBERT | 11. | x: One big obstacle to access to the jobs market is the lack of knowledge of the German language.<br>xˆ: A major hurdle for access to the labour market are a lack of knowledge of English. | 558   | 131   | 313  |
|                     | 12. | x: On Monday night Hungary closed its 175 km long border with Serbia.<br>xˆ: Hungary had in the night of Tuesday closed its 175 km long border with Serbia.                                 | 413   | 135   | 55   |
|                     | 13. | x: They got nothing, but they were allowed to keep the clothes.<br>xˆ: You got nothing, but could keep the clothes.                                                                         | 428   | 174   | 318  |
|                     | 14. | x: A majority of Republicans don't see Trump's temperament as a problem.<br>xˆ: A majority of Republicans see Trump's temperament is not a problem.                                         | 290   | 34    | 134  |
|                     | 15. | x:His car was still running in the driveway.<br>xˆ: His car was still in the driveway.                                                                                                      | 299   | 49    | 71   |
| FBERT<br>><br>Human | 16. | x: Currently the majority of staff are men.<br>xˆ: At the moment the men predominate among the staff.                                                                                       | 77    | 525   | 553  |
|                     | 17. | x: There are, indeed, multiple variables at play.<br>xˆ: In fact, several variables play a role.                                                                                            | 30    | 446   | 552  |
|                     | 18. | x: One was a man of about 5ft 11in tall.<br>xˆ: One of the men was about 1,80 metres in size.                                                                                               | 124   | 551   | 528  |
|                     | 19. | x: All that stuff sure does take a toll.<br>xˆ: All of this certainly exacts its toll.                                                                                                      | 90    | 454   | 547  |
|                     | 20. | x: Wage gains have shown signs of picking up.<br>xˆ: Increases of wages showed signs of a recovery.                                                                                         | 140   | 464   | 514  |

<span id="page-14-1"></span>Table 7: Examples sentences where similarity ranks assigned by Human, FBERT, and BLEU differ significantly on WMT16 German-to-English evaluation task. x: gold reference, xˆ: candidate outputs of MT systems. Rankings assigned by Human, FBERT, and BLEU are shown in the right three columns. The sentences are ranked by the similarity, *i.e.* rank 1 is the most similar pair assigned by a score. An ideal metric should rank similar to humans.

## <span id="page-14-0"></span>A QUALITATIVE ANALYSIS

We study BERTSCORE and SENTBLEU using WMT16 German-to-English [\(Bojar et al., 2016\)](#page-9-12). We rank all 560 candidate-reference pairs by human score, BERTSCORE, or SENTBLEU from most similar to least similar. Ideally, the ranking assigned by BERTSCORE and SENTBLEU should be similar to the ranking assigned by the human score.

Table [7](#page-14-1) first shows examples where BERTSCORE and SENTBLEU scores disagree about the ranking for a candidate-reference pair by a large number. We observe that BERTSCORE is effectively able to capture synonyms and changes in word order. For example, the reference and candidate sentences in pair 3 are almost identical except that the candidate replaces *opinion in Hungary* with *Hungarian view* and switches the order of the quotation mark (*"*) and *a*. While BERTSCORE ranks the pair relatively high, SENTBLEU judges the pair as dissimilar, because it cannot match synonyms and is sensitive to the small word order changes. Pair 5 shows a set of changes that preserve the semantic meaning: replacing *to cut* with *will reduce* and swapping the order of *30,000* and *jobs*. BERTSCORE ranks the candidate translation similar to the human judgment, whereas SENTBLEU ranks it much lower. We also see that SENTBLEU potentially over-rewards n-gram overlap, even when phrases are used very differently. In pair 6, both the candidate and the reference contain *the human dignity of the man*. Yet the two sentences convey very different meaning. BERTSCORE agrees with the human judgment and ranks the pair low. In contrast, SENTBLEU considers the pair as relatively similar because of the significant word overlap.

![](_page_15_Figure_1.jpeg)

<span id="page-15-0"></span>Figure 2: BERTSCORE visualization. The cosine similarity of each word matching in PBERT are color-coded.

The bottom half of Table [7](#page-14-1) shows examples where BERTSCORE and human judgments disagree about the ranking. We observe that BERTSCORE finds it difficult to detect factual errors. For example, BERTSCORE assigns high similarity to pair 11 when the translation replaces *German language* with *English* and pair 12 where the translation incorrectly outputs *Tuesday* when it is supposed to generate *Monday*. BERTSCORE also fails to identify that *5ft 11in* is equivalent with *1.80 metres* in pair 18. As a result, BERTSCORE assigns low similarity to the eighth pair in Table [7.](#page-14-1) SENTBLEU also suffers from these limitations.

Figure [2](#page-15-0) visualizes the BERTSCORE matching of two pairs of candidate and reference sentences. The figure illustrates how FBERT matches synonymous phrases, such as *imported cars* and *foreign cars*. We also see that FBERT effectively matches words even given a high ordering distortion, for example the token *people* in the figure.

# <span id="page-16-0"></span>B REPRESENTATION CHOICE

As suggested by previous works [\(Peters et al., 2018;](#page-11-12) [Reimers & Gurevych, 2019\)](#page-11-17), selecting a good layer or a good combination of layers from the BERT model is important. In designing BERTSCORE, we use WMT16 segment-level human judgment data as a development set to facilitate our representation choice. For Chinese models, we tune with the WMT17 "en-zh" data because the language pair "en-zh" is not available in WMT16. In Figure [3,](#page-17-0) we plot the change of human correlation of FBERT over different layers of BERT, RoBERTa, XLNet and XLM models. Based on results from different models, we identify a common trend that FBERT computed with the intermediate representations tends to work better. We tune the number of layer to use for a range of publicly available models.[8](#page-16-1) Table [8](#page-16-2) shows the results of our hyperparameter search.

| Model                          | Total Number of Layers | Best Layer |
|--------------------------------|------------------------|------------|
| bert-base-uncased              | 12                     | 9          |
| bert-large-uncased             | 24                     | 18         |
| bert-base-cased-finetuned-mrpc | 12                     | 9          |
| bert-base-multilingual-cased   | 12                     | 9          |
| bert-base-chinese              | 12                     | 8          |
| roberta-base                   | 12                     | 10         |
| roberta-large                  | 24                     | 17         |
| roberta-large-mnli             | 24                     | 19         |
| xlnet-base-cased               | 12                     | 5          |
| xlnet-large-cased              | 24                     | 7          |
| xlm-mlm-en-2048                | 12                     | 7          |
| xlm-mlm-100-1280               | 16                     | 11         |

<span id="page-16-2"></span>Table 8: Recommended layer of representation to use for BERTSCORE. The layers are chosen based on a held-out validation set (WMT16).

<span id="page-16-1"></span><sup>8</sup> [https://huggingface.co/pytorch-transformers/pretrained\\_models.html](https://huggingface.co/pytorch-transformers/pretrained_models.html)

![](_page_17_Figure_1.jpeg)

<span id="page-17-0"></span>Figure 3: Pearson correlation of FBERT computed with different models, across different layers, with segment-level human judgments on the WMT16 to-English machine translation task. The WMT17 English-Chinese data is used for the BERT Chinese model. Layer 0 corresponds to using BPE embeddings. Consistently, correlation drops significantly in the final layers.

## <span id="page-18-0"></span>C ABLATION STUDY OF MOVERSCORE

Word Mover's Distance (WMD; [Kusner et al., 2015\)](#page-10-9) is a semantic similarity metric that relies on word embeddings and optimal transport. MOVERSCORE [\(Zhao et al., 2019\)](#page-13-1) combines contextual embeddings and WMD for text generation evaluation. In contrast, BERTSCORE adopts a greedy approach to aggregate token-level information. In addition to using WMD for generation evaluation, [Zhao et al.](#page-13-1) [\(2019\)](#page-13-1) also introduce various other improvements. We do a detailed ablation study to understand the benefit of each improvement, and to investigate whether it can be applied to BERTSCORE. We use a 12-layer uncased BERT model on the WMT17 to-English segment-level data, the same setting as [Zhao et al.](#page-13-1) [\(2019\)](#page-13-1).

We identify several differences between MOVERSCORE and BERTSCORE by analyzing the released source code. We isolate each difference, and mark it with a bracketed tag for our ablation study:

- 1. [MNLI] Use a BERT model fine-tuned on MNLI [\(Williams et al., 2018\)](#page-12-15).
- 2. [PMEANS] Apply power means [\(Rücklé et al., 2018\)](#page-12-16) to aggregate the information of different layers.[9](#page-18-1)
- 3. [IDF-L] For reference sentences, instead of computing the idf scores on the 560 sentences in the segment-level data ([IDF-S]), compute the idf scores on the 3,005 sentences in the system-level data.
- 4. [SEP] For candidate sentences, recompute the idf scores on the candidate sentences. The weighting of reference tokens are kept the same as in [IDF-S]
- 5. [RM] Exclude punctuation marks and sub-word tokens except the first sub-word in each word from the matching.

We follow the setup of [Zhao et al.](#page-13-1) [\(2019\)](#page-13-1) and use their released fine-tuned BERT model to conduct the experiments. Table [9](#page-19-0) shows the results of our ablation study. We report correlations for the two variants of WMD [Zhao et al.](#page-13-1) [\(2019\)](#page-13-1) study: unigrams (WMD1) and bigrams (WMD2). Our FBERT corresponds to the vanilla setting and the importance weighted variant corresponds to the [IDF-S] setting. The complete MOVERSCORE metric corresponds to [IDF-S]+[SEP]+[PMEANS]+[MNLI]+[RM]. We make several observations. First, for all language pairs except fi-en and lv-en, we can replicate the reported performance. For these two language pairs, [Zhao et al.](#page-13-1) [\(2019\)](#page-13-1) did not release their implementations at the time of publication.[10](#page-18-2) Second, we confirm the effectiveness of [PMEANS] and [MNLI]. In Appendix [F,](#page-22-0) we study more pre-trained models and further corroborate this conclusion. However, the contribution of other techniques, including [RM] and [SEP], seems less stable. Third, replacing greedy matching with WMD does not lead to consistent improvement. In fact, oftentimes BERTSCORE is the better metric when given the same setup. In general, for any given language pair, BERTSCORE is always among the best performing ones. Given the current results, it is not clear tht WMD is better than greedy matching for text generation evaluation.

<span id="page-18-1"></span><sup>9</sup> [Zhao et al.](#page-13-1) [\(2019\)](#page-13-1) uses the embeddings from the last five layers from BERT and L2-normalizes the embedding vectors at each layer before computing the P-MEANs and L2-normalizing the concatenated P-MEANS.

<span id="page-18-2"></span><sup>10</sup>A public comment on the project page indicates that some of the techniques are not applied for these two language pairs [\(https://github.com/AIPHES/emnlp19-moverscore/issues/1\)](https://github.com/AIPHES/emnlp19-moverscore/issues/1).

| Ablation                       | Metric | cs-en | de-en | fi-en | lv-en | ru-en | tr-en | zh-en |
|--------------------------------|--------|-------|-------|-------|-------|-------|-------|-------|
|                                | WMD1   | 0.628 | 0.655 | 0.795 | 0.692 | 0.701 | 0.715 | 0.699 |
| Vanilla                        | WMD2   | 0.638 | 0.661 | 0.797 | 0.695 | 0.700 | 0.728 | 0.714 |
|                                | FBERT  | 0.659 | 0.680 | 0.817 | 0.702 | 0.719 | 0.727 | 0.717 |
|                                | WMD1   | 0.636 | 0.662 | 0.824 | 0.709 | 0.716 | 0.728 | 0.713 |
| IDF-S                          | WMD2   | 0.643 | 0.662 | 0.821 | 0.708 | 0.712 | 0.732 | 0.715 |
|                                | FBERT  | 0.657 | 0.681 | 0.823 | 0.713 | 0.725 | 0.718 | 0.711 |
|                                | WMD1   | 0.633 | 0.659 | 0.825 | 0.708 | 0.716 | 0.727 | 0.715 |
| IDF-L                          | WMD2   | 0.641 | 0.661 | 0.822 | 0.708 | 0.713 | 0.730 | 0.716 |
|                                | FBERT  | 0.655 | 0.682 | 0.823 | 0.713 | 0.726 | 0.718 | 0.712 |
|                                | WMD1   | 0.651 | 0.660 | 0.819 | 0.703 | 0.714 | 0.724 | 0.715 |
| IDF-L + SEP                    | WMD2   | 0.659 | 0.662 | 0.816 | 0.702 | 0.712 | 0.729 | 0.715 |
|                                | FBERT  | 0.664 | 0.681 | 0.818 | 0.709 | 0.724 | 0.716 | 0.710 |
|                                | WMD1   | 0.651 | 0.686 | 0.803 | 0.681 | 0.730 | 0.730 | 0.720 |
| IDF-L + SEP<br>+ RM            | WMD2   | 0.664 | 0.687 | 0.797 | 0.679 | 0.728 | 0.735 | 0.718 |
|                                | FBERT  | 0.659 | 0.695 | 0.800 | 0.683 | 0.734 | 0.722 | 0.712 |
|                                | WMD1   | 0.658 | 0.663 | 0.820 | 0.707 | 0.717 | 0.725 | 0.712 |
| IDF-L + SEP<br>+ PMEANS        | WMD2   | 0.667 | 0.665 | 0.817 | 0.707 | 0.717 | 0.727 | 0.712 |
|                                | FBERT  | 0.671 | 0.682 | 0.819 | 0.708 | 0.725 | 0.715 | 0.704 |
|                                | WMD1   | 0.659 | 0.679 | 0.822 | 0.732 | 0.718 | 0.746 | 0.725 |
| IDF-L + SEP<br>+ MNLI          | WMD2   | 0.664 | 0.682 | 0.819 | 0.731 | 0.715 | 0.748 | 0.722 |
|                                | FBERT  | 0.668 | 0.701 | 0.825 | 0.737 | 0.727 | 0.744 | 0.725 |
|                                | WMD1   | 0.672 | 0.686 | 0.831 | 0.738 | 0.725 | 0.753 | 0.737 |
| IDF-L + SEP<br>+ PMEANS + MNLI | WMD2   | 0.677 | 0.690 | 0.828 | 0.736 | 0.722 | 0.755 | 0.735 |
|                                | FBERT  | 0.682 | 0.707 | 0.836 | 0.741 | 0.732 | 0.751 | 0.736 |
| IDF-L + SEP                    | WMD1   | 0.670 | 0.708 | 0.821 | 0.717 | 0.738 | 0.762 | 0.744 |
| + PMEANS + MNLI                | WMD2   | 0.679 | 0.709 | 0.814 | 0.716 | 0.736 | 0.762 | 0.738 |
| + RM                           | FBERT  | 0.676 | 0.717 | 0.824 | 0.719 | 0.740 | 0.757 | 0.738 |

<span id="page-19-0"></span>Table 9: Ablation Study of MOVERSCORE and BERTSCORE using Pearson correlations on the WMT17 to-English segment-level data. Correlations that are not outperformed by others for that language pair under Williams Test are bolded. We observe that using WMD does not consistently improve BERTSCORE.

| Type                      | Metric                   | Meaning | Grammar | Combined |
|---------------------------|--------------------------|---------|---------|----------|
|                           | PBERT                    | 0.36    | 0.47    | 0.46     |
| BERTSCORE                 | RBERT                    | 0.64    | 0.29    | 0.52     |
|                           | FBERT                    | 0.58    | 0.41    | 0.56     |
|                           | BLEU                     | 0.46    | 0.13    | 0.33     |
|                           | METEOR                   | 0.53    | 0.11    | 0.36     |
| Common metrics            | ROUGE-L                  | 0.51    | 0.16    | 0.38     |
|                           | SARI                     | 0.50    | 0.15    | 0.37     |
| Best metrics according to | SKIP-2+RECALL+MULT-PROB  | 0.59    | N/A     | 0.51     |
|                           | PARSE-2+RECALL+MULT-MAX  | N/A     | 0.35    | 0.52     |
| Toutanova et al. (2016)   | PARSE-2+RECALL+MULT-PROB | 0.57    | 0.35    | 0.52     |

<span id="page-20-0"></span>Table 10: Pearson correlations with human judgments on the MSR Abstractive Text Compression Dataset.

## D ADDITIONAL EXPERIMENTS ON ABSTRACTIVE TEXT COMPRESSION

We use the human judgments provided from the MSR Abstractive Text Compression Dataset [\(Toutanova et al., 2016\)](#page-12-17) to illustrate the applicability of BERTSCORE to abstractive text compression evaluation. The data includes three types of human scores: (a) meaning: how well a compressed text preserve the meaning of the original text; (b) grammar: how grammatically correct a compressed text is; and (c) combined: the average of the meaning and the grammar scores. We follow the experimental setup of [Toutanova et al.](#page-12-17) [\(2016\)](#page-12-17) and report Pearson correlation between BERTSCORE and the three types of human scores. Table [10](#page-20-0) shows that RBERT has the highest correlation with human meaning judgments, and PBERT correlates highly with human grammar judgments. FBERT provides a balance between the two aspects.

| Task             | Model                                                                                                      | BLEU                    | PˆBERT | RˆBERT                                                                                                                              | FˆBERT | PBERT | RBERT | FBERT |
|------------------|------------------------------------------------------------------------------------------------------------|-------------------------|--------|-------------------------------------------------------------------------------------------------------------------------------------|--------|-------|-------|-------|
| WMT14<br>En-De   | ConvS2S (Gehring et al., 2017)<br>Transformer-big∗∗ (Ott et al., 2018)<br>DynamicConv∗∗∗ (Wu et al., 2019) | 0.266<br>0.298<br>0.297 |        | 0.6099 0.6055 0.6075 0.8499 0.8482 0.8488<br>0.6587 0.6528 0.6558 0.8687 0.8664 0.8674<br>0.6526 0.6464 0.6495 0.8664 0.8640 0.8650 |        |       |       |       |
| WMT14<br>En-Fr   | ConvS2S (Gehring et al., 2017)<br>Transformer-big (Ott et al., 2018)<br>DynamicConv (Wu et al., 2019)      | 0.408<br>0.432<br>0.432 |        | 0.6998 0.6821 0.6908 0.8876 0.8810 0.8841<br>0.7148 0.6978 0.7061 0.8932 0.8869 0.8899<br>0.7156 0.6989 0.7071 0.8936 0.8873 0.8902 |        |       |       |       |
| IWSLT14<br>De-En | Transformer-iwslt+ (Ott et al., 2019)<br>LightConv (Wu et al., 2019)<br>DynamicConv (Wu et al., 2019)      | 0.350<br>0.348<br>0.352 |        | 0.6749 0.6590 0.6672 0.9452 0.9425 0.9438<br>0.6737 0.6542 0.6642 0.9450 0.9417 0.9433<br>0.6770 0.6586 0.6681 0.9456 0.9425 0.9440 |        |       |       |       |

<span id="page-21-0"></span>Table 11: BLEU scores and BERTSCOREs of publicly available pre-trained MT models in fairseq [\(Ott et al., 2019\)](#page-11-19). We show both rescaled scores marked with ˆ and raw BERTSCOREs. ∗ : trained on unconfirmed WMT data version, ∗∗: trained on WMT16 + ParaCrawl, ∗∗∗: trained on WMT16, <sup>+</sup>: trained by us using fairseq.

# E BERTSCORE OF RECENT MT MODELS

[Table 11](#page-21-0) shows the BLEU scores and the BERTSCOREs of pre-trained machine translation models on WMT14 English-to-German, WMT14 English-to-French, IWSLT14 German-to-English task. We used publicly available pre-trained models from fairseq [\(Ott et al., 2019\)](#page-11-19).[11](#page-21-1) Because a pretrained Transformer model on IWSLT is not released, we trained our own using the fairseq library. We use multilingual cased BERTbase [12](#page-21-2) for English-to-German and English-to-French pairs, and English uncased BERTbase [13](#page-21-3) for German-to-English pairs. Interestingly, the gap between a DynamicConv [\(Wu et al., 2019\)](#page-12-18) trained on only WMT16 and a Transformer [\(Ott et al., 2018\)](#page-11-18) trained on WMT16 and ParaCrawl[14](#page-21-4) (about 30× more training data) becomes larger when evaluated with BERTSCORE rather than BLEU.

<span id="page-21-1"></span><sup>11</sup> Code and pre-trained model available at [https://github.com/pytorch/fairseq.](https://github.com/pytorch/fairseq)

<span id="page-21-2"></span><sup>12</sup>Hash code: bert-base-multilingual-cased\_L9\_version=0.2.0

<span id="page-21-3"></span><sup>13</sup>Hash code: roberta-large\_L17\_version=0.2.0

<span id="page-21-4"></span><sup>14</sup><http://paracrawl.eu/download.html>

## <span id="page-22-0"></span>F ADDITIONAL RESULTS

In this section, we present additional experimental results:

- 1. Segment-level and system-level correlation studies on three years of WMT metric evaluation task (WMT16–18)
- 2. Model selection study on WMT18 10K hybrid systems
- 3. System-level correlation study on 2015 COCO captioning challenge
- 4. Robustness study on PAWS-QQP.

Following BERT [\(Devlin et al., 2019\)](#page-9-0), a variety of Transformer-based [\(Vaswani et al., 2017\)](#page-12-9) pretrained contextual embeddings have been proposed and released. We conduct additional experiments with four types of pre-trained embeddings: BERT, XLM [\(Lample & Conneau, 2019\)](#page-10-13), XLNet [\(Yang](#page-12-11) [et al., 2019b\)](#page-12-11), and RoBERTa [\(Liu et al., 2019b\)](#page-10-12). XLM (Cross-lingual Language Model) is a Transformer pre-trained on the translation language modeling of predicting masked tokens from a pair of sentence in two different languages and masked language modeling tasks using multi-lingual training data. [Yang et al.](#page-12-11) [\(2019b\)](#page-12-11) modify the Transformer architecture and pre-train it on a permutation language modeling task resulting in some improvement on top of the original BERT when fine-tuned on several downstream tasks. [Liu et al.](#page-10-12) [\(2019b\)](#page-10-12) introduce RoBERTa (Robustly optimized BERT approach) and demonstrate that an optimized BERT model is comparable to or sometimes outperforms an XLNet on downstream tasks.

We perform a comprehensive study with the following pre-trained contextual embedding models:[15](#page-22-1)

- BERT models: bert-base-uncased, bert-large-uncased, bert-based-chinese, bert-base-multilingual-cased, and bert-base-cased-mrpc
- RoBERTa models: roberta-base, roberta-large, and roberta-large-mnli
- XLNet models: xlnet-base-cased and xlnet-base-large
- XLM models: xlm-mlm-en-2048 and xlm-mlm-100-1280

#### F.1 WMT CORRELATION STUDY

Experimental setup Because of missing data in the released WMT16 dataset [\(Bojar et al., 2016\)](#page-9-12), we are only able to experiment with to-English segment-level data, which contains the outputs of 50 different systems on 6 language pairs. We use this data as the validation set for hyperparameter tuning (Appendix [B\)](#page-16-0). Table [12](#page-24-0) shows the Pearson correlations of all participating metrics and BERTSCOREs computed with different pre-trained models. Significance testing for this dataset does not include the baseline metrics because the released dataset does not contain the original outputs from the baseline metrics. We conduct significance testing between BERTSCORE results only.

The WMT17 dataset [\(Bojar et al., 2017\)](#page-9-14) contains outputs of 152 different translations on 14 language pairs. We experiment on the segment-level and system-level data on both to-English and from-English language pairs. We exclude fi-en data from the segment-level experiment due to an error in the released data. We compare our results to all participating metrics and perform standard significance testing as done by [Bojar et al.](#page-9-14) [\(2017\)](#page-9-14). Tables [13](#page-25-0)[–16](#page-28-0) show the results.

The WMT18 dataset [\(Ma et al., 2018\)](#page-11-13) contains outputs of 159 translation systems on 14 language pairs. In addition to the results in Tables [1](#page-5-1)[–4,](#page-6-1) we complement the study with the correlations of all participating metrics in WMT18 and results from using different contextual models for BERTSCORE.

Results Table [12–](#page-24-0)[22](#page-34-0) collectively showcase the effectiveness of BERTSCORE in correlating with human judgments. The improvement of BERTSCORE is more pronounced on the segment-level than on the system-level. We also see that more optimized or larger BERT models can produce better contextual representations (e.g., comparing FRoBERTa–Large and FBERT–Large). In contrast, the smaller XLNet performs better than a large one. Based on the evidence in Figure [8](#page-16-2) and Tables [12–](#page-24-0)[22,](#page-34-0) we

<span id="page-22-1"></span><sup>15</sup>Denoted by names specified at [https://huggingface.co/pytorch-transformers/pretrained\\_models.html.](https://huggingface.co/pytorch-transformers/pretrained_models.html)

hypothesize that the permutation language task, though leading to a good set of model weights for fine-tuning on downstream tasks, does not necessarily produce informative pre-trained embeddings for generation evaluation. We also observe that fine-tuning pre-trained models on a related task, such as natural language inference [\(Williams et al., 2018\)](#page-12-15), can lead to better human correlation in evaluating text generation. Therefore, for evaluating English sentences, we recommend computing BERTSCORE with a 24-layer RoBERTa model fine-tuned on the MNLI dataset. For evaluating Non-English sentences, both the multilingual BERT model and the XLM model trained on 100 languages are suitable candidates. We also recommend using domain- or language-specific contextual embeddings when possible, such as using BERT Chinese models for evaluating Chinese tasks. In general, we advise users to consider the target domain and languages when selecting the exact configuration to use.

#### F.2 MODEL SELECTION STUDY

Experimental setup Similar to Section [4,](#page-4-0) we use the 10K hybrid systems super-sampled from WMT18. We randomly select 100 out of 10K hybrid systems, rank them using automatic metrics, and repeat this process 100K times. We add to the results in the main paper (Table [3\)](#page-6-0) performance of all participating metrics in WMT18 and results from using different contextual embedding models for BERTSCORE. We reuse the hybrid configuration and metric outputs released in WMT18. In addition to the Hits@1 measure, we evaluate the metrics using (a) mean reciprocal rank (MRR) of the top metric-rated system in human rankings, and (b) the absolute human score difference (Diff) between the top metric- and human-rated systems. Hits@1 captures a metric's ability to select the best system. The other two measures quantify the amount of error a metric makes in the selection process. Tables [23–](#page-35-0)[28](#page-40-0) show the results from these experiments.

Results The additional results further support our conclusion from Table [3:](#page-6-0) BERTSCORE demonstrates better model selection performance. We also observe that the supervised metric RUSE displays strong model selection ability.

#### F.3 IMAGE CAPTIONING ON COCO

We follow the experimental setup described in Section [4.](#page-4-0) Table [29](#page-41-0) shows the correlations of several pre-trained contextual embeddings. We observe that precision-based methods such as BLEU and PBERT are weakly correlated with human judgments on image captioning tasks. We hypothesize that this is because human judges prefer captions that capture the main objects in a picture for image captioning. In general, RBERT has a high correlation, even surpassing the task-specific metric SPICE [Anderson et al.](#page-9-2) [\(2016\)](#page-9-2). While the fine-tuned RoBERTa-Large model does not result in the highest correlation, it is one of the best metrics.

#### F.4 ROBUSTNESS ANALYSIS ON PAWS-QQP

We present the full results of the robustness study described in Section [6](#page-7-2) in Table [30.](#page-42-0) In general, we observe that BERTSCORE is more robust than other commonly used metrics. BERTSCORE computed with the 24-layer RoBERTa model performs the best. Fine-tuning RoBERTa-Large on MNLI [\(Williams et al., 2018\)](#page-12-15) can significantly improve the robustness against adversarial sentences. However, a fine-tuned BERT on MRPC (Microsoft Research Paraphrasing Corpus) [\(Dolan & Brock](#page-9-19)[ett, 2005\)](#page-9-19) performs worse than its counterpart.

| Setting      | Metric                                           | cs-en<br>560   | de-en<br>560   | fi-en<br>560   | ro-en<br>560   | ru-en<br>560   | tr-en<br>560   |
|--------------|--------------------------------------------------|----------------|----------------|----------------|----------------|----------------|----------------|
|              | DPMFCOMB                                         | 0.713          | 0.584          | 0.598          | 0.627          | 0.615          | 0.663          |
|              | METRICS-F                                        | 0.696          | 0.601          | 0.557          | 0.662          | 0.618          | 0.649          |
|              | COBALT-F.                                        | 0.671          | 0.591          | 0.554          | 0.639          | 0.618          | 0.627          |
|              | UPF-COBA.                                        | 0.652          | 0.550          | 0.490          | 0.616          | 0.556          | 0.626          |
|              | MPEDA                                            | 0.644          | 0.538          | 0.513          | 0.587          | 0.545          | 0.616          |
|              | CHRF2<br>CHRF3                                   | 0.658<br>0.660 | 0.457<br>0.455 | 0.469<br>0.472 | 0.581<br>0.582 | 0.534<br>0.535 | 0.556<br>0.555 |
| Unsupervised | CHRF1                                            | 0.644          | 0.454          | 0.452          | 0.570          | 0.522          | 0.551          |
|              | UOW-REVAL                                        | 0.577          | 0.528          | 0.471          | 0.547          | 0.528          | 0.531          |
|              | WORDF3                                           | 0.599          | 0.447          | 0.473          | 0.525          | 0.504          | 0.536          |
|              | WORDF2                                           | 0.596          | 0.445          | 0.471          | 0.522          | 0.503          | 0.537          |
|              | WORDF1<br>SENTBLEU                               | 0.585<br>0.557 | 0.435<br>0.448 | 0.464<br>0.484 | 0.508<br>0.499 | 0.497<br>0.502 | 0.535<br>0.532 |
|              | DTED                                             | 0.394          | 0.254          | 0.361          | 0.329          | 0.375          | 0.267          |
| Supervised   | BEER                                             | 0.661          | 0.462          | 0.471          | 0.551          | 0.533          | 0.545          |
|              | PBERT–Base                                       | 0.729          | 0.617          | 0.719          | 0.651          | 0.684          | 0.678          |
|              | RBERT–Base                                       | 0.741          | 0.639          | 0.616          | 0.693          | 0.660          | 0.660          |
|              | FBERT–Base<br>PBERT–Base (no idf)                | 0.747<br>0.723 | 0.640<br>0.638 | 0.661<br>0.662 | 0.723<br>0.700 | 0.672<br>0.633 | 0.688<br>0.696 |
|              | RBERT–Base (no idf)                              | 0.745          | 0.656          | 0.638          | 0.697          | 0.653          | 0.674          |
|              | FBERT–Base (no idf)                              | 0.747          | 0.663          | 0.666          | 0.714          | 0.662          | 0.703          |
|              | PBERT–Base–MRPC                                  | 0.697          | 0.618          | 0.614          | 0.676          | 0.62           | 0.695          |
|              | RBERT–Base–MRPC                                  | 0.723          | 0.636          | 0.587          | 0.667          | 0.648          | 0.664          |
|              | FBERT–Base–MRPC<br>PBERT–Base–MRPC (idf)         | 0.725<br>0.713 | 0.644<br>0.613 | 0.617<br>0.630 | 0.691<br>0.693 | 0.654<br>0.635 | 0.702<br>0.691 |
|              | RBERT–Base–MRPC (idf)                            | 0.727          | 0.631          | 0.573          | 0.666          | 0.642          | 0.662          |
|              | FBERT–Base–MRPC (idf)                            | 0.735          | 0.637          | 0.620          | 0.700          | 0.658          | 0.697          |
|              | PBERT–Large                                      | 0.756          | 0.671          | 0.701          | 0.723          | 0.678          | 0.706          |
|              | RBERT–Large                                      | 0.768          | 0.684          | 0.677          | 0.720          | 0.686          | 0.699          |
|              | FBERT–Large                                      | 0.774          | 0.693          | 0.705          | 0.736          | 0.701          | 0.717          |
|              | PBERT–Large (idf)<br>RBERT–Large (idf)           | 0.758<br>0.771 | 0.653<br>0.680 | 0.704<br>0.661 | 0.734<br>0.718 | 0.685<br>0.687 | 0.705<br>0.692 |
|              | FBERT–Large (idf)                                | 0.774          | 0.678          | 0.700          | 0.740          | 0.701          | 0.711          |
|              | PRoBERTa–Base                                    | 0.738          | 0.642          | 0.671          | 0.712          | 0.669          | 0.671          |
|              | RRoBERTa–Base                                    | 0.745          | 0.669          | 0.645          | 0.698          | 0.682          | 0.653          |
|              | FRoBERTa–Base                                    | 0.761          | 0.674          | 0.686          | 0.732          | 0.697          | 0.689          |
| Pre-Trained  | PRoBERTa–Base (idf)<br>RRoBERTa–Base (idf)       | 0.751<br>0.744 | 0.626<br>0.652 | 0.678<br>0.638 | 0.723<br>0.699 | 0.685<br>0.685 | 0.668<br>0.657 |
|              | FRoBERTa–Base (idf)                              | 0.767          | 0.653          | 0.688          | 0.737          | 0.705          | 0.685          |
|              | PRoBERTa–Large                                   | 0.757          | 0.702          | 0.709          | 0.735          | 0.721          | 0.676          |
|              | RRoBERTa–Large                                   | 0.765          | 0.713          | 0.686          | 0.718          | 0.714          | 0.676          |
|              | FRoBERTa–Large                                   | 0.780          | 0.724          | 0.728          | 0.753          | 0.738          | 0.709          |
|              | PRoBERTa–Large (idf)<br>RRoBERTa–Large (idf)     | 0.771<br>0.762 | 0.682<br>0.695 | 0.705<br>0.683 | 0.727<br>0.711 | 0.714<br>0.708 | 0.681<br>0.678 |
|              | FRoBERTa–Large (idf)                             | 0.786          | 0.704          | 0.727          | 0.747          | 0.732          | 0.711          |
|              | PRoBERTa–Large–MNLI                              | 0.777          | 0.718          | 0.733          | 0.744          | 0.729          | 0.747          |
|              | RRoBERTa–Large–MNLI                              | 0.790          | 0.731          | 0.702          | 0.741          | 0.727          | 0.732          |
|              | FRoBERTa–Large–MNLI<br>PRoBERTa–Large–MNLI (idf) | 0.795<br>0.794 | 0.736<br>0.695 | 0.733<br>0.731 | 0.757<br>0.752 | 0.744<br>0.732 | 0.756<br>0.747 |
|              | RRoBERTa–Large–MNLI (idf)                        | 0.792          | 0.706          | 0.694          | 0.737          | 0.724          | 0.733          |
|              | FRoBERTa–Large–MNLI (idf)                        | 0.804          | 0.710          | 0.729          | 0.760          | 0.742          | 0.754          |
|              | PXLNet–Base                                      | 0.708          | 0.612          | 0.639          | 0.650          | 0.606          | 0.690          |
|              | RXLNet–Base                                      | 0.728          | 0.630          | 0.617          | 0.645          | 0.621          | 0.675          |
|              | FXLNet–Base                                      | 0.727<br>0.726 | 0.631<br>0.618 | 0.640<br>0.655 | 0.659<br>0.678 | 0.626<br>0.629 | 0.695<br>0.700 |
|              | PXLNet–Base (idf)<br>RXLNet–Base (idf)           | 0.734          | 0.633          | 0.618          | 0.66           | 0.635          | 0.682          |
|              | FXLNet–Base (idf)                                | 0.739          | 0.633          | 0.649          | 0.681          | 0.643          | 0.702          |
|              | PXL-NET–LARGE                                    | 0.710          | 0.577          | 0.643          | 0.647          | 0.616          | 0.684          |
|              | RXL-NET–LARGE                                    | 0.732          | 0.600          | 0.610          | 0.636          | 0.627          | 0.668          |
|              | FXL-NET–LARGE                                    | 0.733          | 0.600          | 0.643          | 0.655          | 0.637          | 0.691          |
|              | PXL-NET–LARGE (idf)<br>RXL-NET–LARGE (idf)       | 0.728<br>0.735 | 0.574<br>0.592 | 0.652<br>0.597 | 0.669<br>0.642 | 0.633<br>0.629 | 0.681<br>0.662 |
|              | FXL-NET–LARGE (idf)                              | 0.742          | 0.592          | 0.643          | 0.670          | 0.645          | 0.685          |
|              | PXLM–En                                          | 0.688          | 0.569          | 0.613          | 0.645          | 0.583          | 0.659          |
|              | RXLM–En                                          | 0.715          | 0.603          | 0.577          | 0.645          | 0.609          | 0.644          |
|              | FXLM–En                                          | 0.713          | 0.597          | 0.610          | 0.657          | 0.610          | 0.668          |
|              | PXLM–En (idf)<br>RXLM–En (idf)                   | 0.728<br>0.730 | 0.576<br>0.597 | 0.649<br>0.591 | 0.681<br>0.659 | 0.604<br>0.622 | 0.683<br>0.669 |
|              | FXLM–En (idf)                                    | 0.739          | 0.594          | 0.636          | 0.682          | 0.626          | 0.691          |

<span id="page-24-0"></span>Table 12: Pearson correlations with segment-level human judgments on WMT16 to-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples.

| Setting      | Metric                                                                                                         | cs-en<br>560                                       | de-en<br>560                                       | fi-en<br>560                                       | lv-en<br>560                                       | ru-en<br>560                                       | tr-en<br>560                                       | zh-en<br>560                                       |
|--------------|----------------------------------------------------------------------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|
| Unsupervised | CHRF                                                                                                           | 0.514                                              | 0.531                                              | 0.671                                              | 0.525                                              | 0.599                                              | 0.607                                              | 0.591                                              |
|              | CHRF++                                                                                                         | 0.523                                              | 0.534                                              | 0.678                                              | 0.520                                              | 0.588                                              | 0.614                                              | 0.593                                              |
|              | MEANT 2.0                                                                                                      | 0.578                                              | 0.565                                              | 0.687                                              | 0.586                                              | 0.607                                              | 0.596                                              | 0.639                                              |
|              | MEANT 2.0-NOSRL                                                                                                | 0.566                                              | 0.564                                              | 0.682                                              | 0.573                                              | 0.591                                              | 0.582                                              | 0.630                                              |
|              | SENTBLEU                                                                                                       | 0.435                                              | 0.432                                              | 0.571                                              | 0.393                                              | 0.484                                              | 0.538                                              | 0.512                                              |
|              | TREEAGGREG                                                                                                     | 0.486                                              | 0.526                                              | 0.638                                              | 0.446                                              | 0.555                                              | 0.571                                              | 0.535                                              |
|              | UHH_TSKM                                                                                                       | 0.507                                              | 0.479                                              | 0.600                                              | 0.394                                              | 0.465                                              | 0.478                                              | 0.477                                              |
| Supervised   | AUTODA                                                                                                         | 0.499                                              | 0.543                                              | 0.673                                              | 0.533                                              | 0.584                                              | 0.625                                              | 0.583                                              |
|              | BEER                                                                                                           | 0.511                                              | 0.530                                              | 0.681                                              | 0.515                                              | 0.577                                              | 0.600                                              | 0.582                                              |
|              | BLEND                                                                                                          | 0.594                                              | 0.571                                              | 0.733                                              | 0.577                                              | 0.622                                              | 0.671                                              | 0.661                                              |
|              | BLEU2VEC                                                                                                       | 0.439                                              | 0.429                                              | 0.590                                              | 0.386                                              | 0.489                                              | 0.529                                              | 0.526                                              |
|              | NGRAM2VEC                                                                                                      | 0.436                                              | 0.435                                              | 0.582                                              | 0.383                                              | 0.490                                              | 0.538                                              | 0.520                                              |
|              | PBERT–Base                                                                                                     | 0.625                                              | 0.659                                              | 0.808                                              | 0.688                                              | 0.698                                              | 0.713                                              | 0.675                                              |
|              | RBERT–Base                                                                                                     | 0.653                                              | 0.645                                              | 0.782                                              | 0.662                                              | 0.678                                              | 0.716                                              | 0.715                                              |
|              | FBERT–Base                                                                                                     | 0.654                                              | 0.671                                              | 0.811                                              | 0.692                                              | 0.707                                              | 0.731                                              | 0.714                                              |
|              | PBERT–Base (idf)                                                                                               | 0.626                                              | 0.668                                              | 0.819                                              | 0.708                                              | 0.719                                              | 0.702                                              | 0.667                                              |
|              | RBERT–Base (idf)                                                                                               | 0.652                                              | 0.658                                              | 0.789                                              | 0.678                                              | 0.696                                              | 0.703                                              | 0.712                                              |
|              | FBERT–Base (idf)                                                                                               | 0.657                                              | 0.680                                              | 0.823                                              | 0.712                                              | 0.725                                              | 0.718                                              | 0.711                                              |
|              | PBERT–Base–MRPC                                                                                                | 0.599                                              | 0.630                                              | 0.788                                              | 0.657                                              | 0.659                                              | 0.710                                              | 0.681                                              |
|              | RBERT–Base–MRPC                                                                                                | 0.613                                              | 0.620                                              | 0.754                                              | 0.616                                              | 0.650                                              | 0.685                                              | 0.705                                              |
|              | FBERT–Base–MRPC                                                                                                | 0.627                                              | 0.647                                              | 0.792                                              | 0.656                                              | 0.676                                              | 0.717                                              | 0.712                                              |
|              | PBERT–Base–MRPC (idf)                                                                                          | 0.609                                              | 0.630                                              | 0.801                                              | 0.680                                              | 0.676                                              | 0.712                                              | 0.682                                              |
|              | RBERT–Base–MRPC (idf)                                                                                          | 0.611                                              | 0.628                                              | 0.759                                              | 0.633                                              | 0.665                                              | 0.687                                              | 0.703                                              |
|              | FBERT–Base–MRPC (idf)                                                                                          | 0.633                                              | 0.649                                              | 0.803                                              | 0.678                                              | 0.690                                              | 0.719                                              | 0.713                                              |
|              | PBERT–Large                                                                                                    | 0.638                                              | 0.685                                              | 0.816                                              | 0.717                                              | 0.719                                              | 0.746                                              | 0.693                                              |
|              | RBERT–Large                                                                                                    | 0.661                                              | 0.676                                              | 0.782                                              | 0.693                                              | 0.705                                              | 0.744                                              | 0.730                                              |
|              | FBERT–Large                                                                                                    | 0.666                                              | 0.701                                              | 0.814                                              | 0.723                                              | 0.730                                              | 0.760                                              | 0.731                                              |
|              | PBERT–Large (idf)                                                                                              | 0.644                                              | 0.692                                              | 0.827                                              | 0.728                                              | 0.729                                              | 0.734                                              | 0.689                                              |
|              | RBERT–Large (idf)                                                                                              | 0.665                                              | 0.686                                              | 0.796                                              | 0.712                                              | 0.729                                              | 0.733                                              | 0.730                                              |
|              | FBERT–Large (idf)                                                                                              | 0.671                                              | 0.707                                              | 0.829                                              | 0.738                                              | 0.745                                              | 0.746                                              | 0.729                                              |
|              | PRoBERTa–Base                                                                                                  | 0.639                                              | 0.663                                              | 0.801                                              | 0.689                                              | 0.688                                              | 0.700                                              | 0.704                                              |
|              | RRoBERTa–Base                                                                                                  | 0.648                                              | 0.652                                              | 0.768                                              | 0.651                                              | 0.669                                              | 0.684                                              | 0.734                                              |
|              | FRoBERTa–Base                                                                                                  | 0.675                                              | 0.683                                              | 0.818                                              | 0.693                                              | 0.707                                              | 0.718                                              | 0.740                                              |
|              | PRoBERTa–Base (idf)                                                                                            | 0.629                                              | 0.655                                              | 0.804                                              | 0.702                                              | 0.711                                              | 0.707                                              | 0.700                                              |
|              | RRoBERTa–Base (idf)                                                                                            | 0.652                                              | 0.646                                              | 0.773                                              | 0.667                                              | 0.676                                              | 0.689                                              | 0.734                                              |
|              | FRoBERTa–Base (idf)                                                                                            | 0.673                                              | 0.673                                              | 0.823                                              | 0.708                                              | 0.719                                              | 0.721                                              | 0.739                                              |
| Pre-Trained  | PRoBERTa–Large                                                                                                 | 0.658                                              | 0.724                                              | 0.811                                              | 0.743                                              | 0.727                                              | 0.720                                              | 0.744                                              |
|              | RRoBERTa–Large                                                                                                 | 0.685                                              | 0.714                                              | 0.778                                              | 0.711                                              | 0.718                                              | 0.713                                              | 0.759                                              |
|              | FRoBERTa–Large                                                                                                 | 0.710                                              | 0.745                                              | 0.833                                              | 0.756                                              | 0.746                                              | 0.751                                              | 0.775                                              |
|              | PRoBERTa–Large (idf)                                                                                           | 0.644                                              | 0.721                                              | 0.815                                              | 0.740                                              | 0.734                                              | 0.736                                              | 0.734                                              |
|              | RRoBERTa–Large (idf)                                                                                           | 0.683                                              | 0.705                                              | 0.783                                              | 0.718                                              | 0.720                                              | 0.726                                              | 0.751                                              |
|              | FRoBERTa–Large (idf)                                                                                           | 0.703                                              | 0.737                                              | 0.838                                              | 0.761                                              | 0.752                                              | 0.764                                              | 0.767                                              |
|              | PRoBERTa–Large–MNLI                                                                                            | 0.694                                              | 0.736                                              | 0.822                                              | 0.764                                              | 0.741                                              | 0.754                                              | 0.737                                              |
|              | RRoBERTa–Large–MNLI                                                                                            | 0.706                                              | 0.725                                              | 0.785                                              | 0.732                                              | 0.741                                              | 0.750                                              | 0.760                                              |
|              | FRoBERTa–Large–MNLI                                                                                            | 0.722                                              | 0.747                                              | 0.822                                              | 0.764                                              | 0.758                                              | 0.767                                              | 0.765                                              |
|              | PRoBERTa–Large–MNLI (idf)                                                                                      | 0.686                                              | 0.733                                              | 0.836                                              | 0.772                                              | 0.760                                              | 0.767                                              | 0.738                                              |
|              | RRoBERTa–Large–MNLI (idf)                                                                                      | 0.697                                              | 0.717                                              | 0.796                                              | 0.741                                              | 0.753                                              | 0.757                                              | 0.762                                              |
|              | FRoBERTa–Large–MNLI (idf)                                                                                      | 0.714                                              | 0.740                                              | 0.835                                              | 0.774                                              | 0.773                                              | 0.776                                              | 0.767                                              |
|              | PXLNET–Base<br>RXLNET–Base<br>FXLNET–Base<br>PXLNET–Base (idf)<br>RXLNET–Base (idf)<br>FXLNET–Base (idf)       | 0.595<br>0.603<br>0.610<br>0.616<br>0.614<br>0.627 | 0.579<br>0.560<br>0.580<br>0.603<br>0.583<br>0.603 | 0.779<br>0.746<br>0.775<br>0.795<br>0.765<br>0.795 | 0.632<br>0.617<br>0.636<br>0.665<br>0.640<br>0.663 | 0.626<br>0.624<br>0.639<br>0.659<br>0.648<br>0.665 | 0.688<br>0.689<br>0.700<br>0.693<br>0.697<br>0.707 | 0.646<br>0.677<br>0.675<br>0.649<br>0.688<br>0.684 |
|              | PXLNET–Large<br>RXLNET–Large<br>FXLNET–Large<br>PXLNET–Large (idf)<br>RXLNET–Large (idf)<br>FXLNET–Large (idf) | 0.620<br>0.622<br>0.635<br>0.635<br>0.626<br>0.646 | 0.622<br>0.601<br>0.627<br>0.633<br>0.611<br>0.636 | 0.796<br>0.758<br>0.794<br>0.808<br>0.770<br>0.809 | 0.648<br>0.628<br>0.654<br>0.673<br>0.646<br>0.675 | 0.648<br>0.645<br>0.664<br>0.672<br>0.661<br>0.682 | 0.694<br>0.684<br>0.705<br>0.688<br>0.682<br>0.700 | 0.660<br>0.701<br>0.698<br>0.649<br>0.700<br>0.695 |
|              | PXLM–En                                                                                                        | 0.565                                              | 0.594                                              | 0.769                                              | 0.631                                              | 0.649                                              | 0.672                                              | 0.643                                              |
|              | RXLM–En                                                                                                        | 0.592                                              | 0.586                                              | 0.734                                              | 0.618                                              | 0.647                                              | 0.673                                              | 0.686                                              |
|              | FXLM–En                                                                                                        | 0.595                                              | 0.605                                              | 0.768                                              | 0.641                                              | 0.664                                              | 0.686                                              | 0.683                                              |
|              | PXLM–En (idf)                                                                                                  | 0.599                                              | 0.618                                              | 0.795                                              | 0.670                                              | 0.686                                              | 0.690                                              | 0.657                                              |
|              | RXLM–En (idf)                                                                                                  | 0.624                                              | 0.605                                              | 0.768                                              | 0.652                                              | 0.680                                              | 0.684                                              | 0.698                                              |
|              | FXLM–En (idf)                                                                                                  | 0.630                                              | 0.624                                              | 0.798                                              | 0.676                                              | 0.698                                              | 0.698                                              | 0.694                                              |

<span id="page-25-0"></span>Table 13: Absolute Pearson correlations with segment-level human judgments on WMT17 to-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples.

| Setting      | Metric               | en-cs<br>32K | en-de<br>3K | en-fi<br>3K | en-lv<br>3K | en-ru<br>560 | en-tr<br>247 | en-zh<br>560 |
|--------------|----------------------|--------------|-------------|-------------|-------------|--------------|--------------|--------------|
|              |                      | τ            | τ           | τ           | τ           | r            | τ            | r            |
|              | AUTODA               | 0.041        | 0.099       | 0.204       | 0.130       | 0.511        | 0.409        | 0.609        |
|              | AUTODA-TECTO         | 0.336        | -           | -           | -           | -            | -            | -            |
|              | CHRF                 | 0.376        | 0.336       | 0.503       | 0.420       | 0.605        | 0.466        | 0.608        |
|              | CHRF+                | 0.377        | 0.325       | 0.514       | 0.421       | 0.609        | 0.474        | -            |
| Unsupervised | CHRF++               | 0.368        | 0.328       | 0.484       | 0.417       | 0.604        | 0.466        | 0.602        |
|              | MEANT 2.0            | -            | 0.350       | -           | -           | -            | -            | 0.727        |
|              | MEANT 2.0-NOSRL      | 0.395        | 0.324       | 0.565       | 0.425       | 0.636        | 0.482        | 0.705        |
|              | SENTBLEU             | 0.274        | 0.269       | 0.446       | 0.259       | 0.468        | 0.377        | 0.642        |
|              | TREEAGGREG           | 0.361        | 0.305       | 0.509       | 0.383       | 0.535        | 0.441        | 0.566        |
|              | BEER                 | 0.398        | 0.336       | 0.557       | 0.420       | 0.569        | 0.490        | 0.622        |
|              | BLEND                | -            | -           | -           | -           | 0.578        | -            | -            |
| Supervised   | BLEU2VEC             | 0.305        | 0.313       | 0.503       | 0.315       | 0.472        | 0.425        | -            |
|              | NGRAM2VEC            | -            | -           | 0.486       | 0.317       | -            | -            | -            |
|              | PBERT–Multi          | 0.412        | 0.364       | 0.561       | 0.435       | 0.606        | 0.579        | 0.759        |
|              | RBERT–Multi          | 0.443        | 0.430       | 0.587       | 0.480       | 0.663        | 0.571        | 0.804        |
|              | FBERT–Multi          | 0.440        | 0.404       | 0.587       | 0.466       | 0.653        | 0.587        | 0.806        |
|              | PBERT–Multi<br>(idf) | 0.411        | 0.328       | 0.568       | 0.444       | 0.616        | 0.555        | 0.741        |
|              | RBERT–Multi<br>(idf) | 0.449        | 0.416       | 0.591       | 0.479       | 0.665        | 0.579        | 0.796        |
| Pre-Trained  | FBERT–Multi<br>(idf) | 0.447        | 0.379       | 0.588       | 0.470       | 0.657        | 0.571        | 0.793        |
|              | PXLM–100             | 0.406        | 0.383       | 0.553       | 0.423       | 0.562        | 0.611        | 0.722        |
|              | RXLM–100             | 0.446        | 0.436       | 0.587       | 0.458       | 0.626        | 0.652        | 0.779        |
|              | FXLM–100             | 0.444        | 0.424       | 0.577       | 0.456       | 0.613        | 0.628        | 0.778        |
|              | PXLM–100<br>(idf)    | 0.419        | 0.367       | 0.557       | 0.427       | 0.571        | 0.595        | 0.719        |
|              | RXLM–100<br>(idf)    | 0.450        | 0.424       | 0.592       | 0.464       | 0.632        | 0.644        | 0.770        |
|              | FXLM–100<br>(idf)    | 0.448        | 0.419       | 0.580       | 0.459       | 0.617        | 0.644        | 0.771        |

Table 14: Absolute Pearson correlation (|r|) and Kendall correlation (τ ) with segment-level human judgments on WMT17 from-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples.

| Setting      | Metric                                                                                                                                                   | cs-en<br>4                                                                                               | de-en<br>11                                                                                              | fi-en<br>6                                                                                               | lv-en<br>9                                                                                               | ru-en<br>9                                                                                               | tr-en<br>10                                                                                              | zh-en<br>16                                                                                              |
|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| Unsupervised | BLEU<br>CDER<br>CHARACTER<br>CHRF<br>CHRF++<br>MEANT 2.0<br>MEANT 2.0-NOSRL<br>NIST<br>PER<br>TER<br>TREEAGGREG<br>UHH_TSKM                              | 0.971<br>0.989<br>0.972<br>0.939<br>0.940<br>0.926<br>0.902<br>1.000<br>0.968<br>0.989<br>0.983<br>0.996 | 0.923<br>0.930<br>0.974<br>0.968<br>0.965<br>0.950<br>0.936<br>0.931<br>0.951<br>0.906<br>0.920<br>0.937 | 0.903<br>0.927<br>0.946<br>0.938<br>0.927<br>0.941<br>0.933<br>0.931<br>0.896<br>0.952<br>0.977<br>0.921 | 0.979<br>0.985<br>0.932<br>0.968<br>0.973<br>0.970<br>0.963<br>0.960<br>0.962<br>0.971<br>0.986<br>0.990 | 0.912<br>0.922<br>0.958<br>0.952<br>0.945<br>0.962<br>0.960<br>0.912<br>0.911<br>0.912<br>0.918<br>0.914 | 0.976<br>0.973<br>0.949<br>0.944<br>0.960<br>0.932<br>0.896<br>0.971<br>0.932<br>0.954<br>0.987<br>0.987 | 0.864<br>0.904<br>0.799<br>0.859<br>0.880<br>0.838<br>0.800<br>0.849<br>0.877<br>0.847<br>0.861<br>0.902 |
| Supervised   | WER<br>AUTODA<br>BEER<br>BLEND<br>BLEU2VEC<br>NGRAM2VEC                                                                                                  | 0.987<br>0.438<br>0.972<br>0.968<br>0.989<br>0.984                                                       | 0.896<br>0.959<br>0.960<br>0.976<br>0.936<br>0.935                                                       | 0.948<br>0.925<br>0.955<br>0.958<br>0.888<br>0.890                                                       | 0.969<br>0.973<br>0.978<br>0.979<br>0.966<br>0.963                                                       | 0.907<br>0.907<br>0.936<br>0.964<br>0.907<br>0.907                                                       | 0.925<br>0.916<br>0.972<br>0.984<br>0.961<br>0.955                                                       | 0.839<br>0.734<br>0.902<br>0.894<br>0.886<br>0.880                                                       |
| Pre-Trained  | PBERT–Base<br>RBERT–Base<br>FBERT–Base<br>PBERT–Base (idf)<br>RBERT–Base (idf)<br>FBERT–Base (idf)                                                       | 0.975<br>0.995<br>0.987<br>0.983<br>0.997<br>0.992                                                       | 0.936<br>0.975<br>0.961<br>0.937<br>0.981<br>0.967                                                       | 0.991<br>0.944<br>0.979<br>0.998<br>0.962<br>0.995                                                       | 0.993<br>0.978<br>0.991<br>0.992<br>0.968<br>0.992                                                       | 0.918<br>0.953<br>0.937<br>0.939<br>0.977<br>0.960                                                       | 0.981<br>0.991<br>0.991<br>0.985<br>0.985<br>0.996                                                       | 0.892<br>0.975<br>0.953<br>0.878<br>0.949<br>0.951                                                       |
|              | PBERT–Base–MRPC<br>RBERT–Base–MRPC<br>FBERT–Base–MRPC<br>PBERT–Base–MRPC (idf)<br>RBERT–Base–MRPC (idf)<br>FBERT–Base–MRPC (idf)                         | 0.982<br>0.999<br>0.994<br>0.989<br>0.999<br>0.997                                                       | 0.926<br>0.979<br>0.957<br>0.936<br>0.987<br>0.968                                                       | 0.990<br>0.950<br>0.986<br>0.992<br>0.962<br>0.995                                                       | 0.987<br>0.982<br>0.994<br>0.979<br>0.980<br>0.997                                                       | 0.916<br>0.957<br>0.938<br>0.931<br>0.975<br>0.956                                                       | 0.970<br>0.977<br>0.980<br>0.976<br>0.979<br>0.989                                                       | 0.899<br>0.985<br>0.960<br>0.892<br>0.973<br>0.963                                                       |
|              | PBERT–Large<br>RBERT–Large<br>FBERT–Large<br>PBERT–Large (idf)<br>RBERT–Large (idf)<br>FBERT–Large (idf)                                                 | 0.981<br>0.996<br>0.990<br>0.986<br>0.997<br>0.994                                                       | 0.937<br>0.975<br>0.960<br>0.938<br>0.982<br>0.965                                                       | 0.991<br>0.953<br>0.981<br>0.998<br>0.967<br>0.993                                                       | 0.996<br>0.985<br>0.995<br>0.995<br>0.979<br>0.995                                                       | 0.921<br>0.954<br>0.938<br>0.939<br>0.974<br>0.958                                                       | 0.987<br>0.992<br>0.992<br>0.994<br>0.992<br>0.998                                                       | 0.905<br>0.977<br>0.957<br>0.897<br>0.966<br>0.959                                                       |
|              | PRoBERTa–Base<br>RRoBERTa–Base<br>FRoBERTa–Base<br>PRoBERTa–Base (idf)<br>RRoBERTa–Base (idf)<br>FRoBERTa–Base (idf)                                     | 0.987<br>0.999<br>0.996<br>0.990<br>0.998<br>0.996                                                       | 0.930<br>0.982<br>0.961<br>0.938<br>0.987<br>0.970                                                       | 0.984<br>0.947<br>0.993<br>0.980<br>0.963<br>0.999                                                       | 0.966<br>0.979<br>0.993<br>0.956<br>0.979<br>0.994                                                       | 0.916<br>0.956<br>0.937<br>0.929<br>0.971<br>0.952                                                       | 0.963<br>0.986<br>0.983<br>0.967<br>0.986<br>0.989                                                       | 0.955<br>0.984<br>0.982<br>0.962<br>0.974<br>0.982                                                       |
|              | PRoBERTa–Large<br>RRoBERTa–Large<br>FRoBERTa–Large<br>PRoBERTa–Large (idf)<br>RRoBERTa–Large (idf)<br>FRoBERTa–Large (idf)                               | 0.989<br>0.998<br>0.996<br>0.989<br>0.995<br>0.996                                                       | 0.948<br>0.988<br>0.973<br>0.959<br>0.991<br>0.982                                                       | 0.984<br>0.957<br>0.997<br>0.975<br>0.962<br>0.998                                                       | 0.949<br>0.983<br>0.991<br>0.935<br>0.979<br>0.991                                                       | 0.927<br>0.969<br>0.949<br>0.944<br>0.981<br>0.965                                                       | 0.960<br>0.982<br>0.984<br>0.968<br>0.981<br>0.991                                                       | 0.967<br>0.984<br>0.987<br>0.974<br>0.970<br>0.984                                                       |
|              | PRoBERTa–Large–MNLI<br>RRoBERTa–Large–MNLI<br>FRoBERTa–Large–MNLI<br>PRoBERTa–Large–MNLI (idf)<br>RRoBERTa–Large–MNLI (idf)<br>FRoBERTa–Large–MNLI (idf) | 0.994<br>0.995<br>0.999<br>0.995<br>0.994<br>0.999                                                       | 0.963<br>0.991<br>0.982<br>0.970<br>0.992<br>0.989                                                       | 0.995<br>0.962<br>0.992<br>0.997<br>0.967<br>0.996                                                       | 0.990<br>0.981<br>0.996<br>0.985<br>0.977<br>0.997                                                       | 0.944<br>0.973<br>0.961<br>0.955<br>0.983<br>0.972                                                       | 0.981<br>0.985<br>0.988<br>0.988<br>0.988<br>0.994                                                       | 0.974<br>0.984<br>0.989<br>0.979<br>0.972<br>0.987                                                       |
|              | PXLNET–Base<br>RXLNET–Base<br>FXLNET–Base<br>PXLNET–Base (idf)<br>RXLNET–Base (idf)<br>FXLNET–Base (idf)                                                 | 0.988<br>0.999<br>0.996<br>0.992<br>0.999<br>0.998                                                       | 0.938<br>0.978<br>0.963<br>0.951<br>0.986<br>0.974                                                       | 0.993<br>0.956<br>0.986<br>0.998<br>0.968<br>0.996                                                       | 0.993<br>0.977<br>0.991<br>0.996<br>0.973<br>0.994                                                       | 0.914<br>0.946<br>0.932<br>0.930<br>0.964<br>0.950                                                       | 0.974<br>0.981<br>0.981<br>0.982<br>0.987<br>0.990                                                       | 0.960<br>0.980<br>0.978<br>0.939<br>0.955<br>0.970                                                       |
|              | PXLNET–Large<br>RXLNET–Large<br>FXLNET–Large<br>PXLNET–Large (idf)<br>RXLNET–Large (idf)<br>FXLNET–Large (idf)                                           | 0.991<br>0.996<br>0.999<br>0.995<br>0.993<br>1.000                                                       | 0.944<br>0.981<br>0.969<br>0.955<br>0.985<br>0.978                                                       | 0.996<br>0.945<br>0.986<br>0.999<br>0.951<br>0.994                                                       | 0.995<br>0.971<br>0.992<br>0.996<br>0.960<br>0.993                                                       | 0.924<br>0.961<br>0.945<br>0.941<br>0.975<br>0.962                                                       | 0.982<br>0.986<br>0.992<br>0.985<br>0.974<br>0.994                                                       | 0.943<br>0.958<br>0.961<br>0.937<br>0.910<br>0.954                                                       |
|              | PXLM–En<br>RXLM–En<br>FXLM–En<br>PXLM–En (idf)<br>RXLM–En (idf)<br>FXLM–En (idf)                                                                         | 0.983<br>0.998<br>0.994<br>0.986<br>0.999<br>0.995                                                       | 0.933<br>0.978<br>0.960<br>0.940<br>0.983<br>0.967                                                       | 0.994<br>0.949<br>0.985<br>0.997<br>0.966<br>0.996                                                       | 0.989<br>0.983<br>0.995<br>0.992<br>0.980<br>0.998                                                       | 0.918<br>0.957<br>0.938<br>0.939<br>0.975<br>0.959                                                       | 0.973<br>0.985<br>0.984<br>0.979<br>0.991<br>0.993                                                       | 0.928<br>0.972<br>0.964<br>0.916<br>0.952<br>0.958                                                       |

Table 15: Absolute Pearson correlations with system-level human judgments on WMT17 to-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems.

| Setting      | Metric               | en-cs<br>14 | en-de<br>16 | en-lv<br>17 | en-ru<br>9 | en-tr<br>8 | en-zh<br>11 |
|--------------|----------------------|-------------|-------------|-------------|------------|------------|-------------|
|              | BLEU                 | 0.956       | 0.804       | 0.866       | 0.898      | 0.924      | –           |
|              | CDER                 | 0.968       | 0.813       | 0.930       | 0.924      | 0.957      | –           |
|              | CHARACTER            | 0.981       | 0.938       | 0.897       | 0.939      | 0.975      | 0.933       |
|              | CHRF                 | 0.976       | 0.863       | 0.955       | 0.950      | 0.991      | 0.976       |
|              | CHRF++               | 0.974       | 0.852       | 0.956       | 0.945      | 0.986      | 0.976       |
|              | MEANT 2.0            | –           | 0.858       | –           | –          | –          | 0.956       |
| Unsupervised | MEANT 2.0-NOSRL      | 0.976       | 0.770       | 0.959       | 0.957      | 0.991      | 0.943       |
|              | NIST                 | 0.962       | 0.769       | 0.935       | 0.920      | 0.986      | –           |
|              | PER                  | 0.954       | 0.687       | 0.851       | 0.887      | 0.963      | –           |
|              | TER                  | 0.955       | 0.796       | 0.909       | 0.933      | 0.967      | –           |
|              | TREEAGGREG           | 0.947       | 0.773       | 0.927       | 0.921      | 0.983      | 0.938       |
|              | UHH_TSKM             | –           | –           | –           | –          | –          | –           |
|              | WER                  | 0.954       | 0.802       | 0.906       | 0.934      | 0.956      | –           |
|              | AUTODA               | 0.975       | 0.603       | 0.729       | 0.850      | 0.601      | 0.976       |
|              | BEER                 | 0.970       | 0.842       | 0.930       | 0.944      | 0.980      | 0.914       |
| Supervised   | BLEND                | –           | –           | –           | 0.953      | –          | –           |
|              | BLEU2VEC             | 0.963       | 0.810       | 0.859       | 0.903      | 0.911      | –           |
|              | NGRAM2VEC            | –           | –           | 0.862       | –          | –          | –           |
|              | PBERT–Multi          | 0.959       | 0.798       | 0.960       | 0.946      | 0.981      | 0.970       |
|              | RBERT–Multi          | 0.982       | 0.909       | 0.957       | 0.980      | 0.979      | 0.994       |
|              | FBERT–Multi          | 0.976       | 0.859       | 0.959       | 0.966      | 0.980      | 0.992       |
|              | PBERT–Multi<br>(idf) | 0.963       | 0.760       | 0.960       | 0.947      | 0.984      | 0.971       |
|              | RBERT–Multi<br>(idf) | 0.985       | 0.907       | 0.955       | 0.981      | 0.984      | 0.982       |
| Pre-Trained  | FBERT–Multi<br>(idf) | 0.979       | 0.841       | 0.958       | 0.968      | 0.984      | 0.991       |
|              | PXLM–100             | 0.967       | 0.825       | 0.965       | 0.953      | 0.974      | 0.977       |
|              | RXLM–100             | 0.980       | 0.902       | 0.965       | 0.982      | 0.977      | 0.979       |
|              | FXLM–100             | 0.979       | 0.868       | 0.969       | 0.971      | 0.976      | 0.986       |
|              | PXLM–100<br>(idf)    | 0.968       | 0.809       | 0.965       | 0.955      | 0.980      | 0.975       |
|              | RXLM–100<br>(idf)    | 0.981       | 0.894       | 0.964       | 0.984      | 0.983      | 0.968       |
|              | FXLM–100<br>(idf)    | 0.979       | 0.856       | 0.966       | 0.973      | 0.982      | 0.979       |

<span id="page-28-0"></span>Table 16: Absolute Pearson correlations with system-level human judgments on WMT17 from-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems.

| Setting      | Metric                                                                                                                                                   | cs-en<br>5K                                        | de-en<br>78K                                       | et-en<br>57K                                       | fi-en<br>16K                                       | ru-en<br>10K                                       | tr-en<br>9K                                        | zh-en<br>33K                                       |
|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|
| Unsupervised | CHARACTER                                                                                                                                                | 0.256                                              | 0.450                                              | 0.286                                              | 0.185                                              | 0.244                                              | 0.172                                              | 0.202                                              |
|              | ITER                                                                                                                                                     | 0.198                                              | 0.396                                              | 0.235                                              | 0.128                                              | 0.139                                              | -0.029                                             | 0.144                                              |
|              | METEOR++                                                                                                                                                 | 0.270                                              | 0.457                                              | 0.329                                              | 0.207                                              | 0.253                                              | 0.204                                              | 0.179                                              |
|              | SENTBLEU                                                                                                                                                 | 0.233                                              | 0.415                                              | 0.285                                              | 0.154                                              | 0.228                                              | 0.145                                              | 0.178                                              |
|              | UHH_TSKM                                                                                                                                                 | 0.274                                              | 0.436                                              | 0.300                                              | 0.168                                              | 0.235                                              | 0.154                                              | 0.151                                              |
|              | YISI-0                                                                                                                                                   | 0.301                                              | 0.474                                              | 0.330                                              | 0.225                                              | 0.294                                              | 0.215                                              | 0.205                                              |
|              | YISI-1                                                                                                                                                   | 0.319                                              | 0.488                                              | 0.351                                              | 0.231                                              | 0.300                                              | 0.234                                              | 0.211                                              |
|              | YISI-1 SRL                                                                                                                                               | 0.317                                              | 0.483                                              | 0.345                                              | 0.237                                              | 0.306                                              | 0.233                                              | 0.209                                              |
| Supervised   | BEER                                                                                                                                                     | 0.295                                              | 0.481                                              | 0.341                                              | 0.232                                              | 0.288                                              | 0.229                                              | 0.214                                              |
|              | BLEND                                                                                                                                                    | 0.322                                              | 0.492                                              | 0.354                                              | 0.226                                              | 0.290                                              | 0.232                                              | 0.217                                              |
|              | RUSE                                                                                                                                                     | 0.347                                              | 0.498                                              | 0.368                                              | 0.273                                              | 0.311                                              | 0.259                                              | 0.218                                              |
|              | PBERT–Base                                                                                                                                               | 0.349                                              | 0.522                                              | 0.373                                              | 0.264                                              | 0.325                                              | 0.264                                              | 0.232                                              |
|              | RBERT–Base                                                                                                                                               | 0.370                                              | 0.528                                              | 0.378                                              | 0.291                                              | 0.333                                              | 0.257                                              | 0.244                                              |
|              | FBERT–Base                                                                                                                                               | 0.373                                              | 0.531                                              | 0.385                                              | 0.287                                              | 0.341                                              | 0.266                                              | 0.243                                              |
|              | PBERT–Base (idf)                                                                                                                                         | 0.352                                              | 0.524                                              | 0.382                                              | 0.27                                               | 0.326                                              | 0.277                                              | 0.235                                              |
|              | RBERT–Base (idf)                                                                                                                                         | 0.368                                              | 0.536                                              | 0.388                                              | 0.300                                              | 0.340                                              | 0.284                                              | 0.244                                              |
|              | FBERT–Base (idf)                                                                                                                                         | 0.375                                              | 0.535                                              | 0.393                                              | 0.294                                              | 0.339                                              | 0.289                                              | 0.243                                              |
|              | PBERT–Base–MRPC                                                                                                                                          | 0.343                                              | 0.520                                              | 0.365                                              | 0.247                                              | 0.333                                              | 0.25                                               | 0.227                                              |
|              | RBERT–Base–MRPC                                                                                                                                          | 0.370                                              | 0.524                                              | 0.373                                              | 0.277                                              | 0.34                                               | 0.261                                              | 0.244                                              |
|              | FBERT–Base–MRPC                                                                                                                                          | 0.366                                              | 0.529                                              | 0.377                                              | 0.271                                              | 0.342                                              | 0.263                                              | 0.242                                              |
|              | PBERT–Base–MRPC (idf)                                                                                                                                    | 0.348                                              | 0.522                                              | 0.371                                              | 0.25                                               | 0.318                                              | 0.256                                              | 0.224                                              |
|              | RBERT–Base–MRPC (idf)                                                                                                                                    | 0.379                                              | 0.531                                              | 0.383                                              | 0.285                                              | 0.339                                              | 0.266                                              | 0.242                                              |
|              | FBERT–Base–MRPC (idf)                                                                                                                                    | 0.373                                              | 0.534                                              | 0.383                                              | 0.274                                              | 0.342                                              | 0.275                                              | 0.242                                              |
|              | PBERT–LARGE                                                                                                                                              | 0.361                                              | 0.529                                              | 0.380                                              | 0.276                                              | 0.340                                              | 0.266                                              | 0.241                                              |
|              | RBERT–LARGE                                                                                                                                              | 0.386                                              | 0.532                                              | 0.386                                              | 0.297                                              | 0.347                                              | 0.268                                              | 0.247                                              |
|              | FBERT–LARGE                                                                                                                                              | 0.402                                              | 0.537                                              | 0.390                                              | 0.296                                              | 0.344                                              | 0.274                                              | 0.252                                              |
|              | PBERT–LARGE (idf)                                                                                                                                        | 0.377                                              | 0.532                                              | 0.390                                              | 0.287                                              | 0.342                                              | 0.292                                              | 0.246                                              |
|              | RBERT–LARGE (idf)                                                                                                                                        | 0.386                                              | 0.544                                              | 0.396                                              | 0.308                                              | 0.356                                              | 0.287                                              | 0.251                                              |
|              | FBERT–LARGE (idf)                                                                                                                                        | 0.388                                              | 0.545                                              | 0.399                                              | 0.309                                              | 0.358                                              | 0.300                                              | 0.257                                              |
|              | PRoBERTa–Base                                                                                                                                            | 0.368                                              | 0.53                                               | 0.371                                              | 0.274                                              | 0.318                                              | 0.265                                              | 0.235                                              |
|              | RRoBERTa–Base                                                                                                                                            | 0.383                                              | 0.536                                              | 0.376                                              | 0.283                                              | 0.336                                              | 0.253                                              | 0.245                                              |
|              | FRoBERTa–Base                                                                                                                                            | 0.391                                              | 0.540                                              | 0.383                                              | 0.273                                              | 0.339                                              | 0.270                                              | 0.249                                              |
|              | PRoBERTa–Base (idf)                                                                                                                                      | 0.379                                              | 0.528                                              | 0.372                                              | 0.261                                              | 0.314                                              | 0.265                                              | 0.232                                              |
|              | RRoBERTa–Base (idf)                                                                                                                                      | 0.389                                              | 0.539                                              | 0.384                                              | 0.288                                              | 0.332                                              | 0.267                                              | 0.245                                              |
|              | FRoBERTa–Base (idf)                                                                                                                                      | 0.400                                              | 0.540                                              | 0.385                                              | 0.274                                              | 0.337                                              | 0.277                                              | 0.247                                              |
| Pre-Trained  | PRoBERTa–LARGE<br>RRoBERTa–LARGE<br>FRoBERTa–LARGE<br>PRoBERTa–LARGE (idf)<br>RRoBERTa–LARGE (idf)<br>FRoBERTa–LARGE (idf)                               | 0.387<br>0.388<br>0.404<br>0.391<br>0.386<br>0.408 | 0.541<br>0.546<br>0.550<br>0.540<br>0.548<br>0.550 | 0.389<br>0.391<br>0.397<br>0.387<br>0.394<br>0.395 | 0.283<br>0.304<br>0.296<br>0.280<br>0.305<br>0.293 | 0.345<br>0.343<br>0.353<br>0.334<br>0.338<br>0.346 | 0.280<br>0.290<br>0.292<br>0.284<br>0.295<br>0.296 | 0.248<br>0.255<br>0.264<br>0.252<br>0.252<br>0.260 |
|              | PRoBERTa–Large–MNLI<br>RRoBERTa–Large–MNLI<br>FRoBERTa–Large–MNLI<br>PRoBERTa–Large–MNLI (idf)<br>RRoBERTa–Large–MNLI (idf)<br>FRoBERTa–Large–MNLI (idf) | 0.397<br>0.404<br>0.418<br>0.414<br>0.412<br>0.417 | 0.549<br>0.553<br>0.557<br>0.552<br>0.555<br>0.559 | 0.396<br>0.393<br>0.402<br>0.399<br>0.400<br>0.403 | 0.299<br>0.313<br>0.312<br>0.301<br>0.316<br>0.309 | 0.351<br>0.351<br>0.362<br>0.349<br>0.357<br>0.357 | 0.295<br>0.279<br>0.290<br>0.306<br>0.289<br>0.307 | 0.253<br>0.253<br>0.258<br>0.249<br>0.258<br>0.258 |
|              | PXLNet–Base<br>RXLNet–Base<br>FXLNet–Base<br>PXLNet–Base (idf)<br>RXLNet–Base (idf)<br>FXLNet–Base (idf)                                                 | 0.335<br>0.351<br>0.351<br>0.339<br>0.364<br>0.355 | 0.514<br>0.515<br>0.517<br>0.516<br>0.521<br>0.524 | 0.359<br>0.362<br>0.365<br>0.366<br>0.371<br>0.374 | 0.243<br>0.261<br>0.257<br>0.258<br>0.268<br>0.265 | 0.308<br>0.311<br>0.315<br>0.307<br>0.317<br>0.320 | 0.247<br>0.227<br>0.25<br>0.261<br>0.242<br>0.261  | 0.232<br>0.232<br>0.237<br>0.236<br>0.238<br>0.241 |
|              | PXL-NET–LARGE<br>RXL-NET–LARGE<br>FXL-NET–LARGE<br>PXL-NET–LARGE (idf)<br>RXL-NET–LARGE (idf)<br>FXL-NET–LARGE (idf)                                     | 0.344<br>0.358<br>0.357<br>0.348<br>0.366<br>0.375 | 0.522<br>0.524<br>0.530<br>0.520<br>0.529<br>0.530 | 0.371<br>0.374<br>0.380<br>0.373<br>0.378<br>0.382 | 0.252<br>0.275<br>0.265<br>0.260<br>0.278<br>0.274 | 0.316<br>0.332<br>0.334<br>0.319<br>0.331<br>0.332 | 0.264<br>0.249<br>0.263<br>0.265<br>0.266<br>0.274 | 0.233<br>0.239<br>0.238<br>0.235<br>0.241<br>0.240 |
|              | PXLM–En                                                                                                                                                  | 0.349                                              | 0.516                                              | 0.366                                              | 0.244                                              | 0.310                                              | 0.259                                              | 0.233                                              |
|              | RXLM–En                                                                                                                                                  | 0.358                                              | 0.518                                              | 0.364                                              | 0.264                                              | 0.320                                              | 0.244                                              | 0.237                                              |
|              | FXLM–En                                                                                                                                                  | 0.358                                              | 0.525                                              | 0.373                                              | 0.259                                              | 0.322                                              | 0.258                                              | 0.238                                              |
|              | PXLM–En (idf)                                                                                                                                            | 0.355                                              | 0.527                                              | 0.374                                              | 0.254                                              | 0.311                                              | 0.28                                               | 0.238                                              |
|              | RXLM–En (idf)                                                                                                                                            | 0.362                                              | 0.528                                              | 0.376                                              | 0.274                                              | 0.333                                              | 0.26                                               | 0.24                                               |
|              | FXLM–En (idf)                                                                                                                                            | 0.367                                              | 0.531                                              | 0.382                                              | 0.273                                              | 0.330                                              | 0.275                                              | 0.246                                              |

Table 17: Kendall correlations with segment-level human judgments on WMT18 to-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples.

| Setting      | Metric               | en-cs<br>5K | en-de<br>20K | en-et<br>32K | en-fi<br>10K | en-ru<br>22K | en-tr<br>1K | en-zh<br>29K |
|--------------|----------------------|-------------|--------------|--------------|--------------|--------------|-------------|--------------|
|              | CHARACTER            | 0.414       | 0.604        | 0.464        | 0.403        | 0.352        | 0.404       | 0.313        |
|              | ITER                 | 0.333       | 0.610        | 0.392        | 0.311        | 0.291        | 0.236       | -            |
| Unsupervised | SENTBLEU             | 0.389       | 0.620        | 0.414        | 0.355        | 0.330        | 0.261       | 0.311        |
|              | YISI-0               | 0.471       | 0.661        | 0.531        | 0.464        | 0.394        | 0.376       | 0.318        |
|              | YISI-1               | 0.496       | 0.691        | 0.546        | 0.504        | 0.407        | 0.418       | 0.323        |
|              | YISI-1 SRL           | -           | 0.696        | -            | -            | -            | -           | 0.310        |
| Supervised   | BEER                 | 0.518       | 0.686        | 0.558        | 0.511        | 0.403        | 0.374       | 0.302        |
|              | BLEND                | -           | -            | -            | -            | 0.394        | -           | -            |
|              | PBERT–Multi          | 0.541       | 0.715        | 0.549        | 0.486        | 0.414        | 0.328       | 0.337        |
|              | RBERT–Multi          | 0.570       | 0.728        | 0.594        | 0.565        | 0.420        | 0.411       | 0.367        |
|              | FBERT–Multi          | 0.562       | 0.728        | 0.586        | 0.546        | 0.423        | 0.399       | 0.364        |
|              | PBERT–Multi<br>(idf) | 0.525       | 0.7          | 0.54         | 0.495        | 0.423        | 0.352       | 0.338        |
|              | RBERT–Multi<br>(idf) | 0.569       | 0.727        | 0.601        | 0.561        | 0.423        | 0.420       | 0.374        |
| Pre-Trained  | FBERT–Multi<br>(idf) | 0.553       | 0.721        | 0.585        | 0.537        | 0.425        | 0.406       | 0.366        |
|              | PXLM–100             | 0.496       | 0.711        | 0.561        | 0.527        | 0.417        | 0.364       | 0.340        |
|              | RXLM–100             | 0.564       | 0.724        | 0.612        | 0.584        | 0.418        | 0.432       | 0.363        |
|              | FXLM–100             | 0.533       | 0.727        | 0.599        | 0.573        | 0.421        | 0.408       | 0.362        |
|              | PXLM–100<br>(idf)    | 0.520       | 0.710        | 0.572        | 0.546        | 0.421        | 0.370       | 0.328        |
|              | RXLM–100<br>(idf)    | 0.567       | 0.722        | 0.609        | 0.587        | 0.420        | 0.439       | 0.365        |
|              | FXLM–100<br>(idf)    | 0.554       | 0.724        | 0.601        | 0.584        | 0.422        | 0.389       | 0.355        |
|              |                      |             |              |              |              |              |             |              |

Table 18: Kendall correlations with segment-level human judgments on WMT18 from-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples.

| Setting      | Metric                                                                                                                                                   | cs-en<br>5                                         | de-en<br>16                                        | et-en<br>14                                        | fi-en<br>9                                         | ru-en<br>8                                         | tr-en<br>5                                         | zh-en<br>14                                        |
|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|
|              | BLEU                                                                                                                                                     | 0.970                                              | 0.971                                              | 0.986                                              | 0.973                                              | 0.979                                              | 0.657                                              | 0.978                                              |
|              | CDER                                                                                                                                                     | 0.972                                              | 0.980                                              | 0.990                                              | 0.984                                              | 0.980                                              | 0.664                                              | 0.982                                              |
|              | CHARACTER                                                                                                                                                | 0.970                                              | 0.993                                              | 0.979                                              | 0.989                                              | 0.991                                              | 0.782                                              | 0.950                                              |
|              | ITER                                                                                                                                                     | 0.975                                              | 0.990                                              | 0.975                                              | 0.996                                              | 0.937                                              | 0.861                                              | 0.980                                              |
|              | METEOR++                                                                                                                                                 | 0.945                                              | 0.991                                              | 0.978                                              | 0.971                                              | 0.995                                              | 0.864                                              | 0.962                                              |
|              | NIST                                                                                                                                                     | 0.954                                              | 0.984                                              | 0.983                                              | 0.975                                              | 0.973                                              | 0.970                                              | 0.968                                              |
| Unsupervised | PER                                                                                                                                                      | 0.970                                              | 0.985                                              | 0.983                                              | 0.993                                              | 0.967                                              | 0.159                                              | 0.931                                              |
|              | TER                                                                                                                                                      | 0.950                                              | 0.970                                              | 0.990                                              | 0.968                                              | 0.970                                              | 0.533                                              | 0.975                                              |
|              | UHH_TSKM                                                                                                                                                 | 0.952                                              | 0.980                                              | 0.989                                              | 0.982                                              | 0.980                                              | 0.547                                              | 0.981                                              |
|              | WER                                                                                                                                                      | 0.951                                              | 0.961                                              | 0.991                                              | 0.961                                              | 0.968                                              | 0.041                                              | 0.975                                              |
|              | YISI-0                                                                                                                                                   | 0.956                                              | 0.994                                              | 0.975                                              | 0.978                                              | 0.988                                              | 0.954                                              | 0.957                                              |
|              | YISI-1                                                                                                                                                   | 0.950                                              | 0.992                                              | 0.979                                              | 0.973                                              | 0.991                                              | 0.958                                              | 0.951                                              |
|              | YISI-1 SRL                                                                                                                                               | 0.965                                              | 0.995                                              | 0.981                                              | 0.977                                              | 0.992                                              | 0.869                                              | 0.962                                              |
| Supervised   | BEER                                                                                                                                                     | 0.958                                              | 0.994                                              | 0.985                                              | 0.991                                              | 0.982                                              | 0.870                                              | 0.976                                              |
|              | BLEND                                                                                                                                                    | 0.973                                              | 0.991                                              | 0.985                                              | 0.994                                              | 0.993                                              | 0.801                                              | 0.976                                              |
|              | RUSE                                                                                                                                                     | 0.981                                              | 0.997                                              | 0.990                                              | 0.991                                              | 0.988                                              | 0.853                                              | 0.981                                              |
|              | PBERT–Base                                                                                                                                               | 0.965                                              | 0.995                                              | 0.986                                              | 0.973                                              | 0.976                                              | 0.941                                              | 0.974                                              |
|              | RBERT–Base                                                                                                                                               | 0.994                                              | 0.991                                              | 0.979                                              | 0.992                                              | 0.991                                              | 0.067                                              | 0.988                                              |
|              | FBERT–Base                                                                                                                                               | 0.982                                              | 0.994                                              | 0.983                                              | 0.986                                              | 0.985                                              | 0.949                                              | 0.984                                              |
|              | PBERT–Base (idf)                                                                                                                                         | 0.961                                              | 0.993                                              | 0.987                                              | 0.988                                              | 0.976                                              | 0.984                                              | 0.973                                              |
|              | RBERT–Base (idf)                                                                                                                                         | 0.996                                              | 0.994                                              | 0.977                                              | 0.995                                              | 0.995                                              | 0.874                                              | 0.983                                              |
|              | FBERT–Base (idf)                                                                                                                                         | 0.981                                              | 0.995                                              | 0.984                                              | 0.995                                              | 0.988                                              | 0.994                                              | 0.981                                              |
|              | PBERT–Base–MRPC                                                                                                                                          | 0.957                                              | 0.994                                              | 0.989                                              | 0.953                                              | 0.976                                              | 0.798                                              | 0.977                                              |
|              | RBERT–Base–MRPC                                                                                                                                          | 0.992                                              | 0.994                                              | 0.983                                              | 0.988                                              | 0.993                                              | 0.707                                              | 0.990                                              |
|              | FBERT–Base–MRPC                                                                                                                                          | 0.975                                              | 0.995                                              | 0.987                                              | 0.975                                              | 0.986                                              | 0.526                                              | 0.986                                              |
|              | PBERT–Base–MRPC (idf)                                                                                                                                    | 0.957                                              | 0.997                                              | 0.989                                              | 0.967                                              | 0.975                                              | 0.894                                              | 0.980                                              |
|              | RBERT–Base–MRPC (idf)                                                                                                                                    | 0.991                                              | 0.997                                              | 0.981                                              | 0.994                                              | 0.993                                              | 0.052                                              | 0.987                                              |
|              | FBERT–Base–MRPC (idf)                                                                                                                                    | 0.975                                              | 0.998                                              | 0.987                                              | 0.985                                              | 0.987                                              | 0.784                                              | 0.987                                              |
|              | PBERT–Large                                                                                                                                              | 0.978                                              | 0.992                                              | 0.987                                              | 0.971                                              | 0.977                                              | 0.920                                              | 0.978                                              |
|              | RBERT–Large                                                                                                                                              | 0.997                                              | 0.990                                              | 0.985                                              | 0.990                                              | 0.992                                              | 0.098                                              | 0.990                                              |
|              | FBERT–Large                                                                                                                                              | 0.989                                              | 0.992                                              | 0.987                                              | 0.983                                              | 0.985                                              | 0.784                                              | 0.986                                              |
|              | PBERT–Large (idf)                                                                                                                                        | 0.977                                              | 0.992                                              | 0.988                                              | 0.986                                              | 0.976                                              | 0.980                                              | 0.977                                              |
|              | RBERT–Large (idf)                                                                                                                                        | 0.998                                              | 0.993                                              | 0.983                                              | 0.996                                              | 0.995                                              | 0.809                                              | 0.986                                              |
|              | FBERT–Large (idf)                                                                                                                                        | 0.989                                              | 0.993                                              | 0.986                                              | 0.993                                              | 0.987                                              | 0.976                                              | 0.984                                              |
|              | PRoBERTa–Base                                                                                                                                            | 0.970                                              | 0.995                                              | 0.991                                              | 0.998                                              | 0.976                                              | 0.796                                              | 0.980                                              |
|              | RRoBERTa–Base                                                                                                                                            | 0.996                                              | 0.996                                              | 0.982                                              | 0.998                                              | 0.994                                              | 0.477                                              | 0.991                                              |
|              | FRoBERTa–Base                                                                                                                                            | 0.984                                              | 0.997                                              | 0.989                                              | 0.999                                              | 0.987                                              | 0.280                                              | 0.989                                              |
|              | PRoBERTa–Base (idf)                                                                                                                                      | 0.966                                              | 0.993                                              | 0.991                                              | 0.994                                              | 0.977                                              | 0.880                                              | 0.984                                              |
|              | RRoBERTa–Base (idf)                                                                                                                                      | 0.995                                              | 0.998                                              | 0.981                                              | 0.998                                              | 0.995                                              | 0.230                                              | 0.989                                              |
|              | FRoBERTa–Base (idf)                                                                                                                                      | 0.981                                              | 0.998                                              | 0.989                                              | 0.997                                              | 0.988                                              | 0.741                                              | 0.990                                              |
| Pre-Trained  | PRoBERTa–Large                                                                                                                                           | 0.980                                              | 0.998                                              | 0.990                                              | 0.995                                              | 0.982                                              | 0.791                                              | 0.981                                              |
|              | RRoBERTa–Large                                                                                                                                           | 0.998                                              | 0.997                                              | 0.986                                              | 0.997                                              | 0.995                                              | 0.054                                              | 0.990                                              |
|              | FRoBERTa–Large                                                                                                                                           | 0.990                                              | 0.999                                              | 0.990                                              | 0.998                                              | 0.990                                              | 0.499                                              | 0.988                                              |
|              | PRoBERTa–Large (idf)                                                                                                                                     | 0.972                                              | 0.997                                              | 0.993                                              | 0.985                                              | 0.982                                              | 0.920                                              | 0.983                                              |
|              | RRoBERTa–Large (idf)                                                                                                                                     | 0.996                                              | 0.997                                              | 0.984                                              | 0.997                                              | 0.995                                              | 0.578                                              | 0.989                                              |
|              | FRoBERTa–Large (idf)                                                                                                                                     | 0.985                                              | 0.999                                              | 0.992                                              | 0.992                                              | 0.991                                              | 0.826                                              | 0.989                                              |
|              | PRoBERTa–Large–MNLI<br>RRoBERTa–Large–MNLI<br>FRoBERTa–Large–MNLI<br>PRoBERTa–Large–MNLI (idf)<br>RRoBERTa–Large–MNLI (idf)<br>FRoBERTa–Large–MNLI (idf) | 0.989<br>1.000<br>0.996<br>0.986<br>0.999<br>0.995 | 0.998<br>0.996<br>0.998<br>0.998<br>0.997<br>0.998 | 0.994<br>0.988<br>0.992<br>0.994<br>0.986<br>0.991 | 0.998<br>0.996<br>0.998<br>0.993<br>0.997<br>0.996 | 0.985<br>0.995<br>0.992<br>0.986<br>0.993<br>0.993 | 0.908<br>0.097<br>0.665<br>0.989<br>0.633<br>0.963 | 0.982<br>0.991<br>0.989<br>0.985<br>0.990<br>0.990 |
|              | PXLNET–Base<br>RXLNET–Base<br>FXLNET–Base<br>PXLNET–Base (idf)<br>RXLNET–Base (idf)<br>FXLNET–Base (idf)                                                 | 0.970<br>0.994<br>0.983<br>0.968<br>0.993<br>0.981 | 0.996<br>0.997<br>0.997<br>0.998<br>0.998<br>0.999 | 0.986<br>0.979<br>0.983<br>0.986<br>0.978<br>0.984 | 0.990<br>0.995<br>0.993<br>0.990<br>0.996<br>0.995 | 0.979<br>0.994<br>0.987<br>0.978<br>0.994<br>0.989 | 0.739<br>0.795<br>0.505<br>0.923<br>0.439<br>0.722 | 0.982<br>0.990<br>0.988<br>0.982<br>0.988<br>0.988 |
|              | PXLNET–Large<br>RXLNET–Large<br>FXLNET–Large<br>PXLNET–Large (idf)<br>RXLNET–Large (idf)<br>FXLNET–Large (idf)                                           | 0.969<br>0.995<br>0.983<br>0.963<br>0.992<br>0.978 | 0.998<br>0.997<br>0.998<br>0.996<br>0.997<br>0.997 | 0.986<br>0.977<br>0.983<br>0.986<br>0.975<br>0.983 | 0.995<br>0.997<br>0.997<br>0.995<br>0.993<br>0.996 | 0.979<br>0.995<br>0.988<br>0.978<br>0.996<br>0.990 | 0.880<br>0.430<br>0.713<br>0.939<br>0.531<br>0.886 | 0.981<br>0.988<br>0.988<br>0.979<br>0.982<br>0.984 |
|              | PXLM–En                                                                                                                                                  | 0.965                                              | 0.996                                              | 0.990                                              | 0.978                                              | 0.980                                              | 0.946                                              | 0.981                                              |
|              | RXLM–En                                                                                                                                                  | 0.990                                              | 0.995                                              | 0.984                                              | 0.996                                              | 0.996                                              | 0.286                                              | 0.987                                              |
|              | FXLM–En                                                                                                                                                  | 0.978                                              | 0.997                                              | 0.988                                              | 0.990                                              | 0.989                                              | 0.576                                              | 0.987                                              |
|              | PXLM–En (idf)                                                                                                                                            | 0.960                                              | 0.996                                              | 0.990                                              | 0.987                                              | 0.980                                              | 0.989                                              | 0.981                                              |
|              | RXLM–En (idf)                                                                                                                                            | 0.991                                              | 0.997                                              | 0.983                                              | 0.996                                              | 0.998                                              | 0.612                                              | 0.985                                              |
|              | FXLM–En (idf)                                                                                                                                            | 0.976                                              | 0.998                                              | 0.988                                              | 0.994                                              | 0.992                                              | 0.943                                              | 0.985                                              |

Table 19: Absolute Pearson correlations with system-level human judgments on WMT18 to-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems.

| Setting      | Metric               | en-cs | en-de | en-et | en-fi | en-ru | en-tr | en-zh |
|--------------|----------------------|-------|-------|-------|-------|-------|-------|-------|
|              |                      | 5     | 16    | 14    | 12    | 9     | 8     | 14    |
|              | BLEU                 | 0.995 | 0.981 | 0.975 | 0.962 | 0.983 | 0.826 | 0.947 |
|              | CDER                 | 0.997 | 0.986 | 0.984 | 0.964 | 0.984 | 0.861 | 0.961 |
|              | CHARACTER            | 0.993 | 0.989 | 0.956 | 0.974 | 0.983 | 0.833 | 0.983 |
|              | ITER                 | 0.915 | 0.984 | 0.981 | 0.973 | 0.975 | 0.865 | –     |
|              | METEOR++             | –     | –     | –     | –     | –     | –     | –     |
|              | NIST                 | 0.999 | 0.986 | 0.983 | 0.949 | 0.990 | 0.902 | 0.950 |
| Unsupervised | PER                  | 0.991 | 0.981 | 0.958 | 0.906 | 0.988 | 0.859 | 0.964 |
|              | TER                  | 0.997 | 0.988 | 0.981 | 0.942 | 0.987 | 0.867 | 0.963 |
|              | UHH_TSKM             | –     | –     | –     | –     | –     | –     | –     |
|              | WER                  | 0.997 | 0.986 | 0.981 | 0.945 | 0.985 | 0.853 | 0.957 |
|              | YISI-0               | 0.973 | 0.985 | 0.968 | 0.944 | 0.990 | 0.990 | 0.957 |
|              | YISI-1               | 0.987 | 0.985 | 0.979 | 0.940 | 0.992 | 0.976 | 0.963 |
|              | YISI-1 SRL           | –     | 0.990 | –     | –     | –     | –     | 0.952 |
|              | BEER                 | 0.992 | 0.991 | 0.980 | 0.961 | 0.988 | 0.965 | 0.928 |
| Supervised   | BLEND                | –     | –     | –     | –     | 0.988 | –     | –     |
|              | RUSE                 | –     | –     | –     | –     | –     | –     | –     |
|              | PBERT–Multi          | 0.994 | 0.988 | 0.981 | 0.957 | 0.990 | 0.935 | 0.954 |
|              | RBERT–Multi          | 0.997 | 0.990 | 0.980 | 0.980 | 0.989 | 0.879 | 0.976 |
|              | FBERT–Multi          | 0.997 | 0.989 | 0.982 | 0.972 | 0.990 | 0.908 | 0.967 |
|              | PBERT–Multi<br>(idf) | 0.992 | 0.986 | 0.974 | 0.954 | 0.991 | 0.969 | 0.954 |
|              | RBERT–Multi<br>(idf) | 0.997 | 0.993 | 0.982 | 0.982 | 0.992 | 0.901 | 0.984 |
| Pre-Trained  | FBERT–Multi<br>(idf) | 0.995 | 0.990 | 0.981 | 0.972 | 0.991 | 0.941 | 0.973 |
|              | PXLM–100             | 0.984 | 0.992 | 0.993 | 0.972 | 0.993 | 0.962 | 0.965 |
|              | RXLM–100             | 0.991 | 0.992 | 0.992 | 0.989 | 0.992 | 0.895 | 0.983 |
|              | FXLM–100             | 0.988 | 0.993 | 0.993 | 0.986 | 0.993 | 0.935 | 0.976 |
|              | PXLM–100<br>(idf)    | 0.982 | 0.992 | 0.994 | 0.975 | 0.993 | 0.968 | 0.964 |
|              | RXLM–100<br>(idf)    | 0.993 | 0.993 | 0.991 | 0.989 | 0.993 | 0.911 | 0.986 |
|              | FXLM–100<br>(idf)    | 0.989 | 0.993 | 0.994 | 0.985 | 0.993 | 0.945 | 0.979 |

Table 20: Absolute Pearson correlations with system-level human judgments on WMT18 from-English translations. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems.

| Setting      | Metric                                                                                                                                                   | cs-en<br>10K                                                         | de-en<br>10K                                                         | et-en<br>10K                                                         | fi-en<br>10K                                                         | ru-en<br>10K                                                         | tr-en<br>10K                                                         | zh-en<br>10K                                                         |
|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|
| Unsupervised | BLEU<br>CDER<br>CHARACTER<br>ITER<br>METEOR++<br>NIST                                                                                                    | 0.956<br>0.964<br>0.960<br>0.966<br>0.937<br>0.942                   | 0.969<br>0.980<br>0.992<br>0.990<br>0.990<br>0.982                   | 0.981<br>0.988<br>0.975<br>0.975<br>0.975<br>0.980                   | 0.962<br>0.976<br>0.979<br>0.989<br>0.962<br>0.965                   | 0.972<br>0.974<br>0.984<br>0.943<br>0.989<br>0.965                   | 0.586<br>0.577<br>0.680<br>0.742<br>0.787<br>0.862                   | 0.968<br>0.973<br>0.942<br>0.978<br>0.954<br>0.959                   |
|              | PER<br>TER<br>UHH_TSKM<br>WER<br>YISI-0<br>YISI-1<br>YISI-1 SRL                                                                                          | 0.937<br>0.942<br>0.943<br>0.942<br>0.947<br>0.942<br>0.957          | 0.982<br>0.970<br>0.979<br>0.961<br>0.992<br>0.991<br>0.994          | 0.978<br>0.988<br>0.987<br>0.989<br>0.972<br>0.976<br>0.978          | 0.983<br>0.960<br>0.974<br>0.953<br>0.969<br>0.964<br>0.968          | 0.955<br>0.963<br>0.973<br>0.962<br>0.982<br>0.985<br>0.986          | 0.043<br>0.450<br>0.443<br>0.072<br>0.863<br>0.881<br>0.785          | 0.923<br>0.967<br>0.972<br>0.967<br>0.950<br>0.943<br>0.954          |
| Supervised   | BEER<br>BLEND<br>RUSE                                                                                                                                    | 0.950<br>0.965<br>0.974                                              | 0.993<br>0.990<br>0.996                                              | 0.983<br>0.982<br>0.988                                              | 0.982<br>0.985<br>0.983                                              | 0.976<br>0.986<br>0.982                                              | 0.723<br>0.724<br>0.780                                              | 0.968<br>0.969<br>0.973                                              |
|              | PBERT–Base<br>RBERT–Base<br>FBERT–Base<br>PBERT–Base (idf)<br>RBERT–Base (idf)<br>FBERT–Base (idf)<br>PBERT–Base–MRPC<br>RBERT–Base–MRPC                 | 0.954<br>0.988<br>0.973<br>0.957<br>0.986<br>0.974<br>0.949<br>0.983 | 0.992<br>0.994<br>0.994<br>0.994<br>0.990<br>0.993<br>0.995<br>0.997 | 0.984<br>0.974<br>0.981<br>0.983<br>0.976<br>0.980<br>0.986<br>0.979 | 0.980<br>0.987<br>0.987<br>0.966<br>0.984<br>0.978<br>0.960<br>0.986 | 0.970<br>0.988<br>0.982<br>0.970<br>0.984<br>0.978<br>0.969<br>0.986 | 0.917<br>0.801<br>0.924<br>0.875<br>0.019<br>0.853<br>0.832<br>0.099 | 0.965<br>0.975<br>0.973<br>0.966<br>0.980<br>0.976<br>0.972<br>0.980 |
|              | FBERT–Base–MRPC<br>PBERT–Base–MRPC (idf)<br>RBERT–Base–MRPC (idf)<br>FBERT–Base–MRPC (idf)                                                               | 0.967<br>0.949<br>0.984<br>0.967                                     | 0.997<br>0.994<br>0.994<br>0.995                                     | 0.984<br>0.986<br>0.980<br>0.984                                     | 0.978<br>0.946<br>0.980<br>0.968                                     | 0.981<br>0.969<br>0.986<br>0.979                                     | 0.722<br>0.743<br>0.541<br>0.464                                     | 0.979<br>0.969<br>0.982<br>0.978                                     |
| Pre-Trained  | PBERT–Large<br>RBERT–Large<br>FBERT–Large<br>PBERT–Large (idf)<br>RBERT–Large (idf)<br>FBERT–Large (idf)                                                 | 0.969<br>0.990<br>0.982<br>0.970<br>0.989<br>0.981                   | 0.991<br>0.993<br>0.993<br>0.991<br>0.990<br>0.991                   | 0.985<br>0.980<br>0.984<br>0.984<br>0.982<br>0.984                   | 0.979<br>0.988<br>0.986<br>0.963<br>0.982<br>0.976                   | 0.970<br>0.988<br>0.981<br>0.971<br>0.985<br>0.978                   | 0.915<br>0.745<br>0.909<br>0.858<br>0.047<br>0.722                   | 0.969<br>0.978<br>0.976<br>0.970<br>0.982<br>0.978                   |
|              | PRoBERTa–Base<br>RRoBERTa–Base<br>FRoBERTa–Base<br>PRoBERTa–Base (idf)<br>RRoBERTa–Base (idf)<br>FRoBERTa–Base (idf)                                     | 0.959<br>0.987<br>0.973<br>0.963<br>0.988<br>0.976                   | 0.992<br>0.997<br>0.997<br>0.994<br>0.996<br>0.997                   | 0.988<br>0.978<br>0.987<br>0.988<br>0.979<br>0.986                   | 0.986<br>0.989<br>0.989<br>0.989<br>0.989<br>0.990                   | 0.971<br>0.988<br>0.982<br>0.970<br>0.987<br>0.980                   | 0.809<br>0.238<br>0.674<br>0.711<br>0.353<br>0.277                   | 0.976<br>0.981<br>0.982<br>0.972<br>0.983<br>0.980                   |
|              | PRoBERTa–Large<br>RRoBERTa–Large<br>FRoBERTa–Large<br>PRoBERTa–Large (idf)<br>RRoBERTa–Large (idf)<br>FRoBERTa–Large (idf)                               | 0.965<br>0.989<br>0.978<br>0.972<br>0.990<br>0.982                   | 0.995<br>0.997<br>0.998<br>0.997<br>0.996<br>0.998                   | 0.990<br>0.982<br>0.989<br>0.988<br>0.983<br>0.988                   | 0.976<br>0.989<br>0.983<br>0.986<br>0.989<br>0.989                   | 0.976<br>0.988<br>0.985<br>0.976<br>0.989<br>0.983                   | 0.846<br>0.540<br>0.760<br>0.686<br>0.096<br>0.453                   | 0.975<br>0.981<br>0.981<br>0.973<br>0.982<br>0.980                   |
|              | PRoBERTa–Large–MNLI<br>RRoBERTa–Large–MNLI<br>FRoBERTa–Large–MNLI<br>PRoBERTa–Large–MNLI (idf)<br>RRoBERTa–Large–MNLI (idf)<br>FRoBERTa–Large–MNLI (idf) | 0.978<br>0.991<br>0.987<br>0.982<br>0.992<br>0.989                   | 0.997<br>0.996<br>0.998<br>0.998<br>0.996<br>0.998                   | 0.991<br>0.984<br>0.989<br>0.992<br>0.985<br>0.990                   | 0.984<br>0.989<br>0.988<br>0.990<br>0.988<br>0.990                   | 0.980<br>0.987<br>0.986<br>0.978<br>0.988<br>0.985                   | 0.914<br>0.566<br>0.873<br>0.822<br>0.022<br>0.583                   | 0.977<br>0.982<br>0.982<br>0.974<br>0.983<br>0.980                   |
|              | PXLNET–Base<br>RXLNET–Base<br>FXLNET–Base<br>PXLNET–Base (idf)<br>RXLNET–Base (idf)<br>FXLNET–Base (idf)                                                 | 0.960<br>0.985<br>0.974<br>0.962<br>0.986<br>0.975                   | 0.997<br>0.997<br>0.998<br>0.995<br>0.996<br>0.996                   | 0.984<br>0.975<br>0.981<br>0.983<br>0.976<br>0.980                   | 0.982<br>0.988<br>0.986<br>0.982<br>0.987<br>0.985                   | 0.972<br>0.988<br>0.982<br>0.972<br>0.987<br>0.981                   | 0.849<br>0.303<br>0.628<br>0.657<br>0.666<br>0.259                   | 0.974<br>0.980<br>0.980<br>0.974<br>0.982<br>0.980                   |
|              | PXLNET–Large<br>RXLNET–Large<br>FXLNET–Large<br>PXLNET–Large (idf)<br>RXLNET–Large (idf)<br>FXLNET–Large (idf)                                           | 0.955<br>0.984<br>0.971<br>0.961<br>0.987<br>0.976                   | 0.995<br>0.996<br>0.996<br>0.997<br>0.996<br>0.997                   | 0.983<br>0.972<br>0.980<br>0.983<br>0.975<br>0.980                   | 0.986<br>0.984<br>0.987<br>0.987<br>0.989<br>0.989                   | 0.972<br>0.989<br>0.984<br>0.973<br>0.988<br>0.982                   | 0.875<br>0.491<br>0.821<br>0.816<br>0.320<br>0.623                   | 0.970<br>0.975<br>0.976<br>0.973<br>0.981<br>0.980                   |
|              | PXLM–En<br>RXLM–En<br>FXLM–En<br>PXLM–En (idf)<br>RXLM–En (idf)<br>FXLM–En (idf)                                                                         | 0.953<br>0.983<br>0.969<br>0.957<br>0.982<br>0.970                   | 0.995<br>0.996<br>0.997<br>0.996<br>0.995<br>0.996                   | 0.988<br>0.980<br>0.986<br>0.987<br>0.981<br>0.985                   | 0.979<br>0.988<br>0.986<br>0.970<br>0.988<br>0.982                   | 0.974<br>0.991<br>0.985<br>0.974<br>0.989<br>0.982                   | 0.918<br>0.561<br>0.869<br>0.862<br>0.213<br>0.519                   | 0.972<br>0.977<br>0.977<br>0.973<br>0.980<br>0.978                   |

Table 21: Absolute Pearson correlations with human judgments on WMT18 to-English language pairs for 10K hybrid systems. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems.

| Setting      | Metric                                                                                                                                                | en-cs<br>10K                                                                  | en-de<br>10K                                                                  | en-et<br>10K                                                                  | en-fi<br>10K                                                                  | en-ru<br>10K                                                                  | en-tr<br>10K                                                                  | en-zh<br>10K                                                                  |
|--------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| Unsupervised | BLEU<br>CDER<br>CHARACTER<br>ITER<br>METEOR++<br>NIST                                                                                                 | 0.993<br>0.995<br>0.990<br>0.865<br>–<br>0.997                                | 0.977<br>0.984<br>0.986<br>0.978<br>–<br>0.984                                | 0.971<br>0.981<br>0.950<br>0.982<br>–<br>0.980                                | 0.958<br>0.961<br>0.963<br>0.966<br>–<br>0.944                                | 0.977<br>0.982<br>0.981<br>0.965<br>–<br>0.988                                | 0.796<br>0.832<br>0.775<br>0.872<br>–<br>0.870                                | 0.941<br>0.956<br>0.978<br>–<br>–<br>0.944                                    |
|              | PER<br>TER<br>UHH_TSKM<br>WER<br>YISI-0<br>YISI-1<br>YISI-1 SRL                                                                                       | 0.987<br>0.995<br>–<br>0.994<br>0.971<br>0.985<br>–                           | 0.979<br>0.986<br>–<br>0.984<br>0.983<br>0.983<br>0.988                       | 0.954<br>0.977<br>–<br>0.977<br>0.965<br>0.976<br>–                           | 0.904<br>0.939<br>–<br>0.942<br>0.942<br>0.938<br>–                           | 0.986<br>0.985<br>–<br>0.983<br>0.988<br>0.989<br>–                           | 0.829<br>0.837<br>–<br>0.824<br>0.953<br>0.942<br>–                           | 0.950<br>0.959<br>–<br>0.954<br>0.951<br>0.957<br>0.948                       |
| Supervised   | BEER<br>BLEND<br>RUSE                                                                                                                                 | 0.990<br>–<br>–                                                               | 0.989<br>–<br>–                                                               | 0.978<br>–<br>–                                                               | 0.959<br>–<br>–                                                               | 0.986<br>0.986<br>–                                                           | 0.933<br>–<br>–                                                               | 0.925<br>–<br>–                                                               |
| Pre-Trained  | PBERT–Multi<br>RBERT–Multi<br>FBERT–Multi<br>PBERT–Multi<br>(idf)<br>RBERT–Multi<br>(idf)<br>FBERT–Multi<br>(idf)<br>PXLM–100<br>RXLM–100<br>FXLM–100 | 0.989<br>0.995<br>0.993<br>0.992<br>0.995<br>0.995<br>0.980<br>0.991<br>0.987 | 0.983<br>0.991<br>0.988<br>0.986<br>0.988<br>0.988<br>0.990<br>0.990<br>0.990 | 0.970<br>0.979<br>0.978<br>0.978<br>0.977<br>0.979<br>0.991<br>0.989<br>0.991 | 0.951<br>0.977<br>0.969<br>0.954<br>0.976<br>0.969<br>0.972<br>0.985<br>0.981 | 0.988<br>0.989<br>0.989<br>0.988<br>0.987<br>0.987<br>0.991<br>0.991<br>0.991 | 0.936<br>0.872<br>0.910<br>0.903<br>0.850<br>0.877<br>0.936<br>0.882<br>0.915 | 0.950<br>0.980<br>0.969<br>0.950<br>0.972<br>0.963<br>0.959<br>0.981<br>0.974 |
|              | PXLM–100<br>(idf)<br>RXLM–100<br>(idf)<br>FXLM–100<br>(idf)                                                                                           | 0.982<br>0.989<br>0.986                                                       | 0.990<br>0.990<br>0.991                                                       | 0.990<br>0.990<br>0.991                                                       | 0.968<br>0.985<br>0.982                                                       | 0.991<br>0.990<br>0.991                                                       | 0.931<br>0.867<br>0.905                                                       | 0.960<br>0.978<br>0.972                                                       |

<span id="page-34-0"></span>Table 22: Absolute Pearson correlations with human judgments on WMT18 from-English language pairs for 10K hybrid systems. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems.

| Setting      | Metric                                         | cs-en          | de-en          | et-en          | fi-en          | ru-en          | tr-en          | zh-en          |
|--------------|------------------------------------------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|
|              | BLEU                                           | 0.135          | 0.804          | 0.757          | 0.460          | 0.230          | 0.096          | 0.661          |
|              | CDER                                           | 0.162          | 0.795          | 0.764          | 0.493          | 0.234          | 0.087          | 0.660          |
|              | CHARACTER                                      | 0.146          | 0.737          | 0.696          | 0.496          | 0.201          | 0.082          | 0.584          |
|              | ITER                                           | 0.152          | 0.814          | 0.746          | 0.474          | 0.234          | 0.100          | 0.673          |
|              | METEOR++                                       | 0.172          | 0.804          | 0.646          | 0.456          | 0.253          | 0.052          | 0.597          |
| Unsupervised | NIST<br>PER                                    | 0.136<br>0.121 | 0.802<br>0.764 | 0.739<br>0.602 | 0.469<br>0.455 | 0.228<br>0.218 | 0.135<br>0.000 | 0.665<br>0.602 |
|              | TER                                            | 0.139          | 0.789          | 0.768          | 0.470          | 0.232          | 0.001          | 0.652          |
|              | UHH_TSKM                                       | 0.191          | 0.803          | 0.768          | 0.469          | 0.240          | 0.002          | 0.642          |
|              | WER                                            | 0.149          | 0.776          | 0.760          | 0.471          | 0.227          | 0.000          | 0.654          |
|              | YISI-0                                         | 0.148          | 0.780          | 0.703          | 0.483          | 0.229          | 0.106          | 0.629          |
|              | YISI-1                                         | 0.157          | 0.808          | 0.752          | 0.466          | 0.250          | 0.110          | 0.613          |
|              | YISI-1 SRL                                     | 0.159          | 0.814          | 0.763          | 0.484          | 0.243          | 0.008          | 0.620          |
|              | BEER                                           | 0.165          | 0.811          | 0.765          | 0.485          | 0.237          | 0.030          | 0.675          |
| Supervised   | BLEND                                          | 0.184          | 0.820          | 0.779          | 0.484          | 0.254          | 0.003          | 0.611          |
|              | RUSE                                           | 0.213          | 0.823          | 0.788          | 0.487          | 0.250          | 0.109          | 0.672          |
|              | PBERT–Base                                     | 0.190          | 0.815          | 0.778          | 0.468          | 0.261          | 0.130          | 0.655          |
|              | RBERT–Base                                     | 0.189          | 0.813          | 0.775          | 0.481          | 0.266          | 0.014          | 0.663          |
|              | FBERT–Base                                     | 0.194          | 0.819          | 0.778          | 0.474          | 0.265          | 0.144          | 0.670          |
|              | PBERT–Base (idf)                               | 0.189          | 0.817          | 0.775          | 0.477          | 0.255          | 0.131          | 0.650          |
|              | RBERT–Base (idf)                               | 0.192<br>0.193 | 0.808<br>0.817 | 0.771<br>0.774 | 0.484<br>0.483 | 0.248<br>0.262 | 0.005<br>0.081 | 0.674<br>0.669 |
|              | FBERT–Base (idf)                               |                |                |                |                |                |                |                |
|              | PBERT–Base–MRPC                                | 0.190          | 0.701          | 0.766          | 0.487          | 0.254          | 0.126          | 0.653          |
|              | RBERT–Base–MRPC                                | 0.199          | 0.826          | 0.765          | 0.493          | 0.258          | 0.000          | 0.671          |
|              | FBERT–Base–MRPC                                | 0.197          | 0.824          | 0.767          | 0.491          | 0.260          | 0.147          | 0.668          |
|              | PBERT–Base–MRPC (idf)<br>RBERT–Base–MRPC (idf) | 0.186<br>0.200 | 0.806<br>0.823 | 0.765<br>0.760 | 0.492<br>0.495 | 0.247<br>0.258 | 0.125<br>0.000 | 0.661<br>0.680 |
|              | FBERT–Base–MRPC (idf)                          | 0.196          | 0.821          | 0.763          | 0.497          | 0.254          | 0.031          | 0.676          |
|              |                                                |                |                |                |                |                |                |                |
|              | PBERT–Large                                    | 0.200          | 0.815          | 0.778          | 0.474          | 0.261          | 0.137          | 0.661          |
|              | RBERT–Large<br>FBERT–Large                     | 0.194<br>0.199 | 0.809<br>0.810 | 0.779<br>0.782 | 0.493<br>0.484 | 0.270<br>0.266 | 0.006<br>0.142 | 0.672<br>0.672 |
|              | PBERT–Large (idf)                              | 0.200          | 0.813          | 0.772          | 0.485          | 0.256          | 0.136          | 0.657          |
|              | RBERT–Large (idf)                              | 0.197          | 0.806          | 0.769          | 0.495          | 0.262          | 0.005          | 0.675          |
|              | FBERT–Large (idf)                              | 0.199          | 0.811          | 0.772          | 0.494          | 0.262          | 0.006          | 0.673          |
|              | PRoBERTa–Base                                  | 0.173          | 0.675          | 0.757          | 0.502          | 0.258          | 0.126          | 0.654          |
|              | RRoBERTa–Base                                  | 0.165          | 0.816          | 0.764          | 0.483          | 0.266          | 0.000          | 0.674          |
|              | FRoBERTa–Base                                  | 0.173          | 0.820          | 0.764          | 0.498          | 0.262          | 0.090          | 0.669          |
|              | PRoBERTa–Base (idf)                            | 0.172          | 0.691          | 0.755          | 0.503          | 0.252          | 0.123          | 0.661          |
|              | RRoBERTa–Base (idf)                            | 0.172          | 0.809          | 0.758          | 0.490          | 0.268          | 0.000          | 0.678          |
|              | FRoBERTa–Base (idf)                            | 0.178          | 0.820          | 0.758          | 0.501          | 0.260          | 0.001          | 0.674          |
| Pre-Trained  | PRoBERTa–Large                                 | 0.174          | 0.704          | 0.765          | 0.497          | 0.255          | 0.140          | 0.663          |
|              | RRoBERTa–Large                                 | 0.163          | 0.805          | 0.770          | 0.491          | 0.263          | 0.005          | 0.679          |
|              | FRoBERTa–Large                                 | 0.175          | 0.825          | 0.770          | 0.499          | 0.262          | 0.143          | 0.675          |
|              | PRoBERTa–Large (idf)                           | 0.181<br>0.165 | 0.821<br>0.787 | 0.758<br>0.763 | 0.500<br>0.495 | 0.256<br>0.270 | 0.089<br>0.000 | 0.669<br>0.684 |
|              | RRoBERTa–Large (idf)<br>FRoBERTa–Large (idf)   | 0.179          | 0.824          | 0.761          | 0.502          | 0.265          | 0.004          | 0.679          |
|              | PRoBERTa–Large–MNLI                            | 0.185          | 0.828          | 0.780          | 0.504          | 0.263          | 0.133          | 0.654          |
|              | RRoBERTa–Large–MNLI                            | 0.179          | 0.779          | 0.775          | 0.494          | 0.266          | 0.004          | 0.670          |
|              | FRoBERTa–Large–MNLI                            | 0.186          | 0.827          | 0.778          | 0.502          | 0.267          | 0.113          | 0.669          |
|              | PRoBERTa–Large–MNLI (idf)                      | 0.190          | 0.820          | 0.771          | 0.504          | 0.261          | 0.102          | 0.661          |
|              | RRoBERTa–Large–MNLI (idf)                      | 0.181          | 0.769          | 0.766          | 0.494          | 0.266          | 0.004          | 0.674          |
|              | FRoBERTa–Large–MNLI (idf)                      | 0.188          | 0.822          | 0.768          | 0.501          | 0.265          | 0.004          | 0.671          |
|              | PXLNET–Base                                    | 0.186          | 0.771          | 0.762          | 0.496          | 0.247          | 0.153          | 0.658          |
|              | RXLNET–Base                                    | 0.182          | 0.823          | 0.764          | 0.496          | 0.256          | 0.000          | 0.671          |
|              | FXLNET–Base                                    | 0.186          | 0.824          | 0.765          | 0.499          | 0.253          | 0.049          | 0.673          |
|              | PXLNET–Base (idf)                              | 0.178          | 0.819          | 0.756          | 0.506          | 0.241          | 0.130          | 0.656          |
|              | RXLNET–Base (idf)<br>FXLNET–Base (idf)         | 0.183<br>0.182 | 0.817<br>0.821 | 0.754<br>0.755 | 0.501<br>0.505 | 0.256<br>0.250 | 0.000<br>0.000 | 0.673<br>0.670 |
|              |                                                |                |                |                |                |                |                |                |
|              | PXLNET–Large                                   | 0.195          | 0.721          | 0.767          | 0.493          | 0.152          | 0.144          | 0.661          |
|              | RXLNET–Large<br>FXLNET–Large                   | 0.192<br>0.196 | 0.821<br>0.824 | 0.766<br>0.773 | 0.494<br>0.496 | 0.260<br>0.261 | 0.001<br>0.155 | 0.659<br>0.675 |
|              | PXLNET–Large (idf)                             | 0.191          | 0.811          | 0.765          | 0.500          | 0.167          | 0.144          | 0.657          |
|              | RXLNET–Large (idf)                             | 0.196          | 0.815          | 0.762          | 0.495          | 0.259          | 0.000          | 0.673          |
|              | FXLNET–Large (idf)                             | 0.195          | 0.822          | 0.764          | 0.499          | 0.256          | 0.046          | 0.674          |
|              | PXLM–En                                        | 0.192          | 0.796          | 0.779          | 0.486          | 0.255          | 0.131          | 0.665          |
|              | RXLM–En                                        | 0.202          | 0.818          | 0.772          | 0.495          | 0.261          | 0.005          | 0.662          |
|              | FXLM–En                                        | 0.199          | 0.827          | 0.778          | 0.491          | 0.262          | 0.086          | 0.674          |
|              | PXLM–En (idf)                                  | 0.189          | 0.818          | 0.770          | 0.485          | 0.259          | 0.116          | 0.662          |
|              | RXLM–En (idf)                                  | 0.202          | 0.812          | 0.761          | 0.490          | 0.250          | 0.003          | 0.668          |
|              | FXLM–En (idf)                                  | 0.196          | 0.821          | 0.766          | 0.490          | 0.263          | 0.003          | 0.672          |

<span id="page-35-0"></span>Table 23: Model selection accuracies (Hits@1) on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 confidence intervals are below 10−<sup>3</sup> . We bold the highest numbers for each language pair and direction.

| Setting      | Metric                                         | cs-en          | de-en          | et-en          | fi-en          | ru-en          | tr-en          | zh-en          |
|--------------|------------------------------------------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|
|              | BLEU                                           | 0.338          | 0.894          | 0.866          | 0.666          | 0.447          | 0.265          | 0.799          |
|              | CDER                                           | 0.362          | 0.890          | 0.870          | 0.689          | 0.451          | 0.256          | 0.799          |
|              | CHARACTER                                      | 0.349          | 0.854          | 0.814          | 0.690          | 0.429          | 0.254          | 0.739          |
|              | ITER                                           | 0.356          | 0.901          | 0.856          | 0.676          | 0.454          | 0.278          | 0.811          |
|              | METEOR++                                       | 0.369          | 0.895          | 0.798          | 0.662          | 0.470          | 0.174          | 0.757          |
|              | NIST                                           | 0.338          | 0.894          | 0.857          | 0.672          | 0.446          | 0.323          | 0.803          |
| Unsupervised | PER<br>TER                                     | 0.325<br>0.342 | 0.866<br>0.885 | 0.771<br>0.873 | 0.663<br>0.673 | 0.435<br>0.447 | 0.021<br>0.063 | 0.754<br>0.792 |
|              | UHH_TSKM                                       | 0.387          | 0.894          | 0.873          | 0.671          | 0.460          | 0.063          | 0.788          |
|              | WER                                            | 0.353          | 0.876          | 0.868          | 0.674          | 0.443          | 0.034          | 0.790          |
|              | YISI-0                                         | 0.344          | 0.881          | 0.834          | 0.681          | 0.452          | 0.275          | 0.776          |
|              | YISI-1                                         | 0.352          | 0.896          | 0.864          | 0.671          | 0.470          | 0.285          | 0.765          |
|              | YISI-1 SRL                                     | 0.351          | 0.901          | 0.871          | 0.682          | 0.464          | 0.086          | 0.770          |
|              | BEER                                           | 0.364          | 0.899          | 0.871          | 0.684          | 0.460          | 0.125          | 0.811          |
| Supervised   | BLEND                                          | 0.382          | 0.904          | 0.880          | 0.681          | 0.473          | 0.077          | 0.767          |
|              | RUSE                                           | 0.417          | 0.906          | 0.885          | 0.686          | 0.468          | 0.273          | 0.809          |
|              | PBERT–Base                                     | 0.386          | 0.901          | 0.880          | 0.674          | 0.481          | 0.318          | 0.793          |
|              | RBERT–Base                                     | 0.383          | 0.899          | 0.877          | 0.683          | 0.486          | 0.100          | 0.804          |
|              | FBERT–Base                                     | 0.388          | 0.903          | 0.879          | 0.678          | 0.484          | 0.331          | 0.808          |
|              | PBERT–Base (idf)                               | 0.390          | 0.902          | 0.877          | 0.681          | 0.475          | 0.318          | 0.786          |
|              | RBERT–Base (idf)<br>FBERT–Base (idf)           | 0.390<br>0.393 | 0.896<br>0.902 | 0.874<br>0.876 | 0.686<br>0.685 | 0.475<br>0.483 | 0.077<br>0.225 | 0.811<br>0.806 |
|              |                                                |                |                |                |                |                |                |                |
|              | PBERT–Base–MRPC                                | 0.392          | 0.832          | 0.872          | 0.686          | 0.475          | 0.319          | 0.791          |
|              | RBERT–Base–MRPC                                | 0.397          | 0.908          | 0.870          | 0.691          | 0.478          | 0.025          | 0.811          |
|              | FBERT–Base–MRPC                                | 0.398          | 0.907          | 0.872          | 0.690          | 0.481          | 0.335          | 0.806          |
|              | PBERT–Base–MRPC (idf)<br>RBERT–Base–MRPC (idf) | 0.392<br>0.400 | 0.896<br>0.906 | 0.870<br>0.867 | 0.689<br>0.691 | 0.467<br>0.479 | 0.316<br>0.018 | 0.797<br>0.817 |
|              | FBERT–Base–MRPC (idf)                          | 0.400          | 0.905          | 0.869          | 0.693          | 0.475          | 0.097          | 0.812          |
|              | PBERT–Large                                    | 0.398          | 0.901          | 0.880          | 0.678          | 0.481          | 0.327          | 0.799          |
|              | RBERT–Large                                    | 0.391          | 0.897          | 0.879          | 0.690          | 0.490          | 0.085          | 0.810          |
|              | FBERT–Large                                    | 0.397          | 0.898          | 0.882          | 0.684          | 0.486          | 0.328          | 0.810          |
|              | PBERT–Large (idf)                              | 0.398          | 0.900          | 0.875          | 0.685          | 0.475          | 0.323          | 0.794          |
|              | RBERT–Large (idf)                              | 0.395          | 0.895          | 0.873          | 0.692          | 0.488          | 0.080          | 0.813          |
|              | FBERT–Large (idf)                              | 0.398          | 0.899          | 0.875          | 0.691          | 0.482          | 0.086          | 0.810          |
|              | PRoBERTa–Base                                  | 0.372          | 0.814          | 0.866          | 0.697          | 0.475          | 0.313          | 0.795          |
|              | RRoBERTa–Base                                  | 0.366          | 0.902          | 0.870          | 0.683          | 0.483          | 0.026          | 0.813          |
|              | FRoBERTa–Base                                  | 0.374          | 0.904          | 0.870          | 0.694          | 0.480          | 0.224          | 0.808          |
|              | PRoBERTa–Base (idf)                            | 0.373          | 0.825          | 0.865          | 0.697          | 0.470          | 0.303          | 0.802          |
|              | RRoBERTa–Base (idf)                            | 0.374          | 0.898          | 0.866          | 0.688          | 0.486          | 0.028          | 0.816          |
|              | FRoBERTa–Base (idf)                            | 0.380          | 0.904          | 0.866          | 0.696          | 0.479          | 0.037          | 0.812          |
| Pre-Trained  | PRoBERTa–Large                                 | 0.375          | 0.833          | 0.871          | 0.693          | 0.474          | 0.327          | 0.800          |
|              | RRoBERTa–Large                                 | 0.366          | 0.895          | 0.874          | 0.689          | 0.480          | 0.039          | 0.816          |
|              | FRoBERTa–Large                                 | 0.378          | 0.907          | 0.874          | 0.694          | 0.480          | 0.324          | 0.811          |
|              | PRoBERTa–Large (idf)                           | 0.384<br>0.368 | 0.905<br>0.885 | 0.866<br>0.869 | 0.694<br>0.692 | 0.475<br>0.487 | 0.220<br>0.030 | 0.806<br>0.819 |
|              | RRoBERTa–Large (idf)<br>FRoBERTa–Large (idf)   | 0.382          | 0.907          | 0.868          | 0.696          | 0.484          | 0.048          | 0.815          |
|              |                                                | 0.383          | 0.909          | 0.880          | 0.698          | 0.480          | 0.323          | 0.795          |
|              | PRoBERTa–Large–MNLI<br>RRoBERTa–Large–MNLI     | 0.378          | 0.880          | 0.877          | 0.692          | 0.481          | 0.078          | 0.811          |
|              | FRoBERTa–Large–MNLI                            | 0.385          | 0.909          | 0.879          | 0.697          | 0.484          | 0.286          | 0.809          |
|              | PRoBERTa–Large–MNLI (idf)                      | 0.389          | 0.905          | 0.874          | 0.698          | 0.478          | 0.268          | 0.803          |
|              | RRoBERTa–Large–MNLI (idf)                      | 0.380          | 0.874          | 0.870          | 0.691          | 0.483          | 0.079          | 0.814          |
|              | FRoBERTa–Large–MNLI (idf)                      | 0.387          | 0.906          | 0.872          | 0.696          | 0.482          | 0.082          | 0.811          |
|              | PXLNET–Base                                    | 0.385          | 0.875          | 0.869          | 0.692          | 0.469          | 0.342          | 0.796          |
|              | RXLNET–Base                                    | 0.381          | 0.907          | 0.869          | 0.693          | 0.477          | 0.026          | 0.809          |
|              | FXLNET–Base                                    | 0.385          | 0.907          | 0.871          | 0.694          | 0.476          | 0.128          | 0.810          |
|              | PXLNET–Base (idf)                              | 0.381          | 0.904          | 0.864          | 0.699          | 0.464          | 0.289          | 0.794          |
|              | RXLNET–Base (idf)                              | 0.384          | 0.903          | 0.863          | 0.696          | 0.479          | 0.013          | 0.812          |
|              | FXLNET–Base (idf)                              | 0.384          | 0.905          | 0.864          | 0.699          | 0.472          | 0.032          | 0.809          |
|              | PXLNET–Large                                   | 0.392          | 0.844          | 0.873          | 0.689          | 0.367          | 0.338          | 0.799          |
|              | RXLNET–Large<br>FXLNET–Large                   | 0.389<br>0.393 | 0.905<br>0.907 | 0.871<br>0.876 | 0.690<br>0.691 | 0.482<br>0.483 | 0.031<br>0.348 | 0.800<br>0.812 |
|              | PXLNET–Large (idf)                             | 0.393          | 0.899          | 0.870          | 0.694          | 0.387          | 0.333          | 0.794          |
|              | RXLNET–Large (idf)                             | 0.395          | 0.901          | 0.868          | 0.690          | 0.483          | 0.023          | 0.810          |
|              | FXLNET–Large (idf)                             | 0.396          | 0.906          | 0.870          | 0.693          | 0.478          | 0.128          | 0.811          |
|              | PXLM–En                                        | 0.394          | 0.891          | 0.880          | 0.685          | 0.476          | 0.322          | 0.802          |
|              | RXLM–En                                        | 0.401          | 0.903          | 0.875          | 0.692          | 0.483          | 0.082          | 0.803          |
|              | FXLM–En                                        | 0.400          | 0.909          | 0.878          | 0.689          | 0.483          | 0.234          | 0.811          |
|              | PXLM–En (idf)                                  | 0.391          | 0.903          | 0.874          | 0.684          | 0.480          | 0.293          | 0.797          |
|              | RXLM–En (idf)                                  | 0.402          | 0.900          | 0.868          | 0.688          | 0.477          | 0.068          | 0.806          |
|              | FXLM–En (idf)                                  | 0.398          | 0.905          | 0.871          | 0.688          | 0.487          | 0.079          | 0.809          |

Table 24: Mean Reciprocal Rank (MRR) of the top metric-rated system on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 confidence intervals are below 10−<sup>3</sup> . We bold the highest numbers for each language pair and direction.

| Setting      | Metric                                                                                                                                                   | cs-en                                                                                | de-en                                                                                | et-en                                                                                | fi-en                                                                                | ru-en                                                                                | tr-en                                                                                | zh-en                                                                                |
|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| Unsupervised | BLEU<br>CDER<br>CHARACTER<br>ITER<br>METEOR++<br>NIST<br>PER<br>TER<br>UHH_TSKM<br>WER<br>YISI-0                                                         | 3.85<br>3.88<br>3.77<br>3.55<br>3.70<br>3.93<br>2.02<br>3.86<br>3.98<br>3.85<br>3.81 | 0.45<br>0.43<br>0.49<br>0.46<br>0.41<br>0.49<br>0.46<br>0.43<br>0.40<br>0.44<br>0.48 | 1.01<br>0.87<br>0.94<br>1.25<br>0.69<br>1.10<br>1.71<br>1.14<br>1.27<br>1.48<br>0.72 | 2.17<br>1.33<br>2.07<br>1.43<br>1.13<br>1.19<br>1.49<br>1.14<br>1.10<br>1.18<br>1.20 | 2.34<br>2.30<br>2.25<br>4.65<br>2.28<br>2.36<br>2.25<br>4.34<br>2.23<br>4.87<br>1.75 | 4.48<br>4.58<br>4.07<br>3.11<br>1.40<br>1.42<br>4.22<br>5.18<br>4.26<br>5.96<br>1.40 | 3.19<br>3.43<br>3.37<br>2.92<br>3.50<br>3.92<br>3.20<br>3.82<br>3.47<br>3.72<br>3.44 |
| Supervised   | YISI-1<br>YISI-1 SRL<br>BEER<br>BLEND                                                                                                                    | 3.88<br>3.67<br>3.82<br>3.77                                                         | 0.44<br>0.41<br>0.41<br>0.41                                                         | 0.65<br>0.64<br>0.79<br>0.66                                                         | 1.13<br>1.20<br>1.08<br>1.09                                                         | 2.17<br>2.15<br>1.92<br>2.21                                                         | 1.32<br>1.31<br>1.96<br>1.28                                                         | 3.40<br>3.55<br>3.43<br>3.46                                                         |
|              | RUSE                                                                                                                                                     | 3.13                                                                                 | 0.32                                                                                 | 0.64                                                                                 | 1.03                                                                                 | 1.51                                                                                 | 1.94                                                                                 | 3.15                                                                                 |
|              | PBERT–Base<br>RBERT–Base<br>FBERT–Base<br>PBERT–Base (idf)<br>RBERT–Base (idf)<br>FBERT–Base (idf)                                                       | 3.97<br>1.51<br>3.70<br>3.94<br>1.54<br>2.75                                         | 0.36<br>0.43<br>0.36<br>0.36<br>0.43<br>0.39                                         | 0.72<br>0.60<br>0.59<br>0.64<br>0.63<br>0.60                                         | 1.16<br>1.65<br>1.08<br>1.18<br>1.87<br>1.10                                         | 2.20<br>1.33<br>1.92<br>2.06<br>1.12<br>1.38                                         | 1.25<br>1.34<br>1.27<br>2.55<br>5.96<br>1.26                                         | 3.26<br>3.50<br>3.38<br>3.54<br>3.38<br>3.51                                         |
|              | PBERT–Base–MRPC<br>RBERT–Base–MRPC<br>FBERT–Base–MRPC<br>PBERT–Base–MRPC (idf)<br>RBERT–Base–MRPC (idf)<br>FBERT–Base–MRPC (idf)                         | 4.02<br>2.66<br>3.89<br>4.02<br>1.63<br>3.86                                         | 0.35<br>0.43<br>0.36<br>0.35<br>0.43<br>0.38                                         | 0.74<br>0.62<br>0.60<br>0.67<br>0.65<br>0.61                                         | 1.15<br>1.75<br>1.09<br>1.18<br>1.93<br>1.11                                         | 1.09<br>1.10<br>1.08<br>1.48<br>1.13<br>1.14                                         | 3.33<br>5.64<br>3.82<br>3.30<br>7.26<br>4.24                                         | 3.06<br>3.34<br>3.23<br>3.49<br>3.13<br>3.28                                         |
| Pre-Trained  | PBERT–Large<br>RBERT–Large<br>FBERT–Large<br>PBERT–Large (idf)<br>RBERT–Large (idf)<br>FBERT–Large (idf)                                                 | 3.82<br>1.49<br>1.71<br>3.74<br>1.51<br>1.49                                         | 0.34<br>0.40<br>0.35<br>0.35<br>0.42<br>0.38                                         | 0.66<br>0.59<br>0.58<br>0.65<br>0.62<br>0.60                                         | 1.12<br>1.56<br>1.08<br>1.12<br>1.86<br>1.17                                         | 2.10<br>1.17<br>1.65<br>1.90<br>1.10<br>1.24                                         | 1.31<br>1.35<br>1.29<br>1.98<br>5.84<br>1.96                                         | 3.60<br>3.61<br>3.60<br>3.77<br>3.21<br>3.53                                         |
|              | PRoBERTa–Base<br>RRoBERTa–Base<br>FRoBERTa–Base<br>PRoBERTa–Base (idf)<br>RRoBERTa–Base (idf)<br>FRoBERTa–Base (idf)                                     | 3.89<br>1.92<br>3.56<br>3.89<br>1.61<br>3.18                                         | 0.37<br>0.39<br>0.37<br>0.38<br>0.42<br>0.38                                         | 0.75<br>0.64<br>0.59<br>0.67<br>0.67<br>0.60                                         | 1.18<br>1.57<br>1.10<br>1.20<br>1.65<br>1.11                                         | 1.07<br>1.11<br>1.08<br>1.30<br>1.14<br>1.13                                         | 3.45<br>5.75<br>3.79<br>3.27<br>6.55<br>6.54                                         | 2.62<br>3.13<br>2.90<br>3.47<br>2.95<br>3.11                                         |
|              | PRoBERTa–Large<br>RRoBERTa–Large<br>FRoBERTa–Large<br>PRoBERTa–Large (idf)<br>RRoBERTa–Large (idf)<br>FRoBERTa–Large (idf)                               | 3.64<br>1.60<br>2.38<br>2.70<br>1.55<br>1.68                                         | 0.36<br>0.37<br>0.35<br>0.36<br>0.39<br>0.37                                         | 0.71<br>0.64<br>0.58<br>0.69<br>0.66<br>0.59                                         | 1.10<br>1.51<br>1.06<br>1.13<br>1.59<br>1.08                                         | 1.03<br>1.09<br>1.05<br>1.08<br>1.10<br>1.08                                         | 2.69<br>3.91<br>3.57<br>3.18<br>6.66<br>5.58                                         | 2.57<br>3.27<br>2.95<br>2.89<br>3.18<br>2.91                                         |
|              | PRoBERTa–Large–MNLI<br>RRoBERTa–Large–MNLI<br>FRoBERTa–Large–MNLI<br>PRoBERTa–Large–MNLI (idf)<br>RRoBERTa–Large–MNLI (idf)<br>FRoBERTa–Large–MNLI (idf) | 2.14<br>1.45<br>1.42<br>1.55<br>1.45<br>1.42                                         | 0.35<br>0.37<br>0.35<br>0.35<br>0.39<br>0.36                                         | 0.61<br>0.64<br>0.59<br>0.60<br>0.64<br>0.60                                         | 1.07<br>1.49<br>1.07<br>1.08<br>1.65<br>1.10                                         | 1.09<br>1.10<br>1.07<br>1.12<br>1.09<br>1.08                                         | 1.21<br>4.42<br>1.27<br>1.54<br>5.89<br>3.80                                         | 3.35<br>3.55<br>3.41<br>3.87<br>3.32<br>3.45                                         |
|              | PXLNET–Base<br>RXLNET–Base<br>FXLNET–Base<br>PXLNET–Base (idf)<br>RXLNET–Base (idf)<br>FXLNET–Base (idf)                                                 | 3.90<br>1.71<br>3.78<br>3.90<br>1.51<br>3.67                                         | 0.37<br>0.45<br>0.39<br>0.46<br>0.45<br>0.42                                         | 0.68<br>0.72<br>0.62<br>0.65<br>0.82<br>0.66                                         | 1.07<br>1.58<br>1.05<br>1.08<br>1.78<br>1.11                                         | 1.16<br>1.07<br>1.07<br>2.93<br>1.12<br>1.22                                         | 2.47<br>6.29<br>3.60<br>3.30<br>10.77<br>7.13                                        | 2.91<br>3.36<br>3.20<br>3.39<br>3.13<br>3.23                                         |
|              | PXLNET–Large<br>RXLNET–Large<br>FXLNET–Large<br>PXLNET–Large (idf)<br>RXLNET–Large (idf)<br>FXLNET–Large (idf)                                           | 3.94<br>2.23<br>3.84<br>3.92<br>1.60<br>3.80                                         | 0.37<br>0.41<br>0.36<br>0.41<br>0.43<br>0.38                                         | 0.71<br>0.69<br>0.60<br>0.64<br>0.78<br>0.63                                         | 1.10<br>1.34<br>1.03<br>1.12<br>1.70<br>1.06                                         | 21.10<br>1.07<br>1.07<br>21.10<br>1.09<br>1.09                                       | 1.85<br>4.46<br>3.38<br>3.24<br>6.13<br>3.72                                         | 2.90<br>3.40<br>3.22<br>3.37<br>3.20<br>3.25                                         |
|              | PXLM–En<br>RXLM–En<br>FXLM–En<br>PXLM–En (idf)<br>RXLM–En (idf)<br>FXLM–En (idf)                                                                         | 3.88<br>1.98<br>3.78<br>3.84<br>1.70<br>3.72                                         | 0.33<br>0.41<br>0.36<br>0.36<br>0.42<br>0.40                                         | 0.75<br>0.60<br>0.61<br>0.69<br>0.63<br>0.62                                         | 1.16<br>1.41<br>1.09<br>1.17<br>1.55<br>1.14                                         | 2.16<br>1.21<br>1.71<br>1.86<br>1.11<br>1.32                                         | 1.28<br>3.30<br>1.30<br>1.33<br>5.87<br>4.15                                         | 3.29<br>3.47<br>3.40<br>3.47<br>3.36<br>3.43                                         |

Table 25: Absolute Difference (×100) of the top metric-rated and the top human-rated system on to-English WMT18 hybrid systems. Smaller difference signify higher agreement with human scores. We report the average of 100K samples and the 0.95 confidence intervals are below 10−<sup>3</sup> . We bold the lowest numbers for each language pair and direction.

| Setting      | Metric               | en-cs | en-de | en-et | en-fi | en-ru | en-tr | en-zh |
|--------------|----------------------|-------|-------|-------|-------|-------|-------|-------|
|              | BLEU                 | 0.151 | 0.611 | 0.617 | 0.087 | 0.519 | 0.029 | 0.515 |
|              | CDER                 | 0.163 | 0.663 | 0.731 | 0.081 | 0.541 | 0.032 | 0.552 |
|              | CHARACTER            | 0.135 | 0.737 | 0.639 | 0.492 | 0.543 | 0.027 | 0.667 |
|              | ITER                 | 0.000 | 0.691 | 0.734 | 0.112 | 0.534 | 0.031 | –     |
|              | METEOR++             | –     | –     | –     | –     | –     | –     | –     |
|              | NIST                 | 0.182 | 0.662 | 0.549 | 0.083 | 0.537 | 0.033 | 0.553 |
| Unsupervised | PER                  | 0.179 | 0.555 | 0.454 | 0.062 | 0.535 | 0.032 | 0.539 |
|              | TER                  | 0.175 | 0.657 | 0.550 | 0.065 | 0.545 | 0.029 | 0.551 |
|              | UHH_TSKM             | –     | –     | –     | –     | –     | –     | –     |
|              | WER                  | 0.155 | 0.643 | 0.552 | 0.067 | 0.538 | 0.029 | 0.546 |
|              | YISI-0               | 0.154 | 0.674 | 0.622 | 0.356 | 0.523 | 0.383 | 0.600 |
|              | YISI-1               | 0.178 | 0.670 | 0.674 | 0.230 | 0.548 | 0.396 | 0.595 |
|              | YISI-1 SRL           | –     | 0.708 | –     | –     | –     | –     | 0.537 |
|              | BEER                 | 0.174 | 0.670 | 0.662 | 0.113 | 0.555 | 0.296 | 0.531 |
| Supervised   | BLEND                | –     | –     | –     | –     | 0.559 | –     | –     |
|              | RUSE                 | –     | –     | –     | –     | –     | –     | –     |
|              | PBERT–Multi          | 0.181 | 0.665 | 0.771 | 0.077 | 0.550 | 0.373 | 0.550 |
|              | RBERT–Multi          | 0.184 | 0.728 | 0.722 | 0.146 | 0.544 | 0.031 | 0.657 |
|              | FBERT–Multi          | 0.185 | 0.703 | 0.764 | 0.081 | 0.548 | 0.032 | 0.629 |
|              | PBERT–Multi<br>(idf) | 0.175 | 0.713 | 0.769 | 0.080 | 0.542 | 0.031 | 0.549 |
|              | RBERT–Multi<br>(idf) | 0.177 | 0.725 | 0.752 | 0.178 | 0.538 | 0.031 | 0.628 |
| Pre-Trained  | FBERT–Multi<br>(idf) | 0.178 | 0.721 | 0.766 | 0.081 | 0.543 | 0.030 | 0.594 |
|              | PXLM–100             | 0.175 | 0.669 | 0.748 | 0.079 | 0.550 | 0.314 | 0.582 |
|              | RXLM–100             | 0.195 | 0.671 | 0.770 | 0.222 | 0.555 | 0.034 | 0.658 |
|              | FXLM–100             | 0.187 | 0.670 | 0.775 | 0.099 | 0.552 | 0.034 | 0.615 |
|              | PXLM–100<br>(idf)    | 0.163 | 0.664 | 0.750 | 0.091 | 0.550 | 0.288 | 0.578 |
|              | RXLM–100<br>(idf)    | 0.191 | 0.681 | 0.770 | 0.231 | 0.548 | 0.033 | 0.645 |
|              | FXLM–100<br>(idf)    | 0.180 | 0.672 | 0.774 | 0.127 | 0.550 | 0.033 | 0.616 |

Table 26: Model selection accuracies (Hits@1) on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 confidence intervals are below 10−<sup>3</sup> . We bold the highest numbers for each language pair and direction.

| Setting      | Metric               | en-cs | en-de | en-et | en-fi | en-ru | en-tr | en-zh |
|--------------|----------------------|-------|-------|-------|-------|-------|-------|-------|
|              | BLEU                 | 0.363 | 0.764 | 0.766 | 0.323 | 0.714 | 0.205 | 0.666 |
|              | CDER                 | 0.371 | 0.803 | 0.851 | 0.319 | 0.729 | 0.210 | 0.700 |
|              | CHARACTER            | 0.346 | 0.853 | 0.781 | 0.667 | 0.732 | 0.205 | 0.809 |
|              | ITER                 | 0.044 | 0.825 | 0.853 | 0.365 | 0.717 | 0.210 | –     |
|              | METEOR++             | –     | –     | –     | –     | –     | –     | –     |
|              | NIST                 | 0.393 | 0.803 | 0.710 | 0.326 | 0.726 | 0.211 | 0.698 |
| Unsupervised | PER                  | 0.387 | 0.719 | 0.624 | 0.301 | 0.725 | 0.211 | 0.678 |
|              | TER                  | 0.384 | 0.798 | 0.708 | 0.305 | 0.733 | 0.209 | 0.695 |
|              | UHH_TSKM             | –     | –     | –     | –     | –     | –     | –     |
|              | WER                  | 0.367 | 0.787 | 0.710 | 0.308 | 0.728 | 0.209 | 0.696 |
|              | YISI-0               | 0.370 | 0.811 | 0.775 | 0.553 | 0.715 | 0.602 | 0.753 |
|              | YISI-1               | 0.390 | 0.808 | 0.811 | 0.439 | 0.735 | 0.612 | 0.750 |
|              | YISI-1 SRL           | –     | 0.835 | –     | –     | –     | –     | 0.691 |
|              | BEER                 | 0.388 | 0.808 | 0.804 | 0.353 | 0.739 | 0.507 | 0.683 |
| Supervised   | BLEND                | –     | –     | –     | –     | 0.742 | –     | –     |
|              | RUSE                 | –     | –     | –     | –     | –     | –     | –     |
|              | PBERT–Multi          | 0.395 | 0.805 | 0.876 | 0.314 | 0.736 | 0.586 | 0.694 |
|              | RBERT–Multi          | 0.401 | 0.849 | 0.844 | 0.368 | 0.732 | 0.212 | 0.802 |
|              | FBERT–Multi          | 0.400 | 0.832 | 0.872 | 0.317 | 0.735 | 0.214 | 0.775 |
|              | PBERT–Multi<br>(idf) | 0.390 | 0.839 | 0.875 | 0.320 | 0.730 | 0.213 | 0.691 |
|              | RBERT–Multi<br>(idf) | 0.395 | 0.847 | 0.864 | 0.398 | 0.727 | 0.212 | 0.776 |
| Pre-Trained  | FBERT–Multi<br>(idf) | 0.395 | 0.844 | 0.873 | 0.319 | 0.730 | 0.212 | 0.739 |
|              | PXLM–100             | 0.391 | 0.808 | 0.862 | 0.316 | 0.735 | 0.522 | 0.733 |
|              | RXLM–100             | 0.413 | 0.809 | 0.876 | 0.435 | 0.738 | 0.216 | 0.803 |
|              | FXLM–100             | 0.404 | 0.809 | 0.878 | 0.333 | 0.737 | 0.216 | 0.767 |
|              | PXLM–100<br>(idf)    | 0.377 | 0.805 | 0.863 | 0.326 | 0.735 | 0.497 | 0.729 |
|              | RXLM–100<br>(idf)    | 0.409 | 0.816 | 0.876 | 0.444 | 0.733 | 0.214 | 0.793 |
|              | FXLM–100<br>(idf)    | 0.396 | 0.810 | 0.878 | 0.355 | 0.735 | 0.214 | 0.767 |

Table 27: Mean Reciprocal Rank (MRR) of the top metric-rated system on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 confidence intervals are below 10−<sup>3</sup> . We bold the highest numbers for each language pair and direction.

| Setting      | Metric               | en-cs | en-de | en-et | en-fi | en-ru | en-tr | en-zh |
|--------------|----------------------|-------|-------|-------|-------|-------|-------|-------|
|              | BLEU                 | 1.26  | 6.36  | 2.59  | 0.92  | 0.76  | 9.40  | 3.01  |
|              | CDER                 | 1.25  | 6.70  | 1.90  | 1.41  | 0.87  | 9.37  | 1.75  |
|              | CHARACTER            | 1.23  | 6.90  | 2.19  | 4.35  | 0.93  | 5.22  | 1.64  |
|              | ITER                 | 1.25  | 9.14  | 2.52  | 1.52  | 1.35  | 7.33  | –     |
|              | METEOR++             | –     | –     | –     | –     | –     | –     | –     |
|              | NIST                 | 1.24  | 5.28  | 2.55  | 1.02  | 0.75  | 8.82  | 3.34  |
| Unsupervised | PER                  | 1.25  | 6.62  | 4.92  | 7.43  | 0.68  | 9.76  | 2.31  |
|              | TER                  | 1.21  | 6.02  | 4.34  | 2.17  | 0.73  | 8.80  | 1.43  |
|              | UHH_TSKM             | –     | –     | –     | –     | –     | –     | –     |
|              | WER                  | 1.22  | 6.15  | 4.19  | 2.43  | 0.72  | 9.28  | 1.49  |
|              | YISI-0               | 1.25  | 6.62  | 1.53  | 1.46  | 0.75  | 3.47  | 2.87  |
|              | YISI-1               | 1.22  | 6.27  | 1.21  | 1.13  | 0.71  | 3.51  | 3.33  |
|              | YISI-1 SRL           | –     | 6.57  | –     | –     | –     | –     | 3.71  |
|              | BEER                 | 1.21  | 5.96  | 1.84  | 0.77  | 0.74  | 3.36  | 1.96  |
| Supervised   | BLEND                | –     | –     | –     | –     | 0.71  | –     | –     |
|              | RUSE                 | –     | –     | –     | –     | –     | –     | –     |
|              | PBERT–Multi          | 1.17  | 3.27  | 1.38  | 1.24  | 0.75  | 4.14  | 2.08  |
|              | RBERT–Multi          | 1.16  | 6.68  | 0.77  | 0.94  | 0.68  | 3.22  | 1.31  |
|              | FBERT–Multi          | 1.15  | 5.17  | 0.90  | 0.98  | 0.71  | 3.26  | 1.62  |
|              | PBERT–Multi<br>(idf) | 1.14  | 3.82  | 1.66  | 1.27  | 0.76  | 4.57  | 2.04  |
|              | RBERT–Multi<br>(idf) | 1.15  | 6.97  | 0.83  | 3.65  | 0.68  | 3.32  | 1.37  |
| Pre-Trained  | FBERT–Multi<br>(idf) | 1.14  | 5.63  | 1.13  | 1.19  | 0.71  | 3.38  | 1.58  |
|              | PXLM–100             | 1.22  | 6.30  | 1.14  | 0.79  | 0.74  | 3.73  | 2.21  |
|              | RXLM–100             | 1.18  | 6.89  | 0.76  | 0.77  | 0.66  | 3.26  | 1.68  |
|              | FXLM–100             | 1.19  | 6.44  | 0.82  | 0.76  | 0.69  | 3.21  | 1.57  |
|              | PXLM–100<br>(idf)    | 1.21  | 6.61  | 1.07  | 0.78  | 0.72  | 5.59  | 2.02  |
|              | RXLM–100<br>(idf)    | 1.19  | 7.07  | 0.77  | 0.77  | 0.66  | 3.33  | 1.60  |
|              | FXLM–100<br>(idf)    | 1.20  | 6.57  | 0.86  | 0.76  | 0.68  | 3.28  | 1.56  |

<span id="page-40-0"></span>Table 28: Absolute Difference (×100) of the top metric-rated and the top human-rated system on to-English WMT18 hybrid systems. Smaller difference indicate higher agreement with human scores. We report the average of 100K samples and the 0.95 confidence intervals are below 10−<sup>3</sup> . We bold the lowest numbers for each language pair and direction.

| Metric                    | M1      | M2      |
|---------------------------|---------|---------|
|                           |         |         |
| BLEU-1                    | 0.124∗  | 0.135∗  |
| BLEU-2                    | 0.037∗  | 0.048∗  |
| BLEU-3                    | 0.004∗  | 0.016∗  |
| BLEU-4                    | -0.019∗ | -0.005∗ |
| METEOR                    | 0.606∗  | 0.594∗  |
| ROUGE-L                   | 0.090∗  | 0.096∗  |
|                           | 0.438∗  | 0.440∗  |
| CIDER                     |         |         |
| SPICE                     | 0.759∗  | 0.750∗  |
| LEIC                      | 0.939∗  | 0.949∗  |
| BEER                      | 0.491   | 0.562   |
| EED                       | 0.545   | 0.599   |
| CHRF++                    | 0.702   | 0.729   |
| CHARACTER                 | 0.800   | 0.801   |
| PBERT–Base                | 0.313   | 0.344   |
| RBERT–Base                | 0.679   | 0.622   |
|                           | 0.531   | 0.519   |
| FBERT–Base                |         |         |
| PBERT–Base (idf)          | 0.243   | 0.286   |
| RBERT–Base (idf)          | 0.834   | 0.783   |
| FBERT–Base (idf)          | 0.579   | 0.581   |
| PBERT–Base–MRPC           | 0.252   | 0.331   |
| RBERT–Base–MRPC           | 0.644   | 0.641   |
| FBERT–Base–MRPC           | 0.470   | 0.512   |
| PBERT–Base–MRPC (idf)     | 0.264   | 0.300   |
| RBERT–Base–MRPC (idf)     | 0.794   | 0.767   |
|                           |         |         |
| FBERT–Base–MRPC (idf)     | 0.575   | 0.583   |
| PBERT–Large               | 0.454   | 0.486   |
| RBERT–Large               | 0.756   | 0.697   |
| FBERT–Large               | 0.649   | 0.634   |
| PBERT–Large (idf)         | 0.327   | 0.372   |
| RBERT–Large (idf)         | 0.873   | 0.821   |
| FBERT–Large (idf)         | 0.645   | 0.647   |
|                           |         |         |
| PRoBERTa–Base             | -0.223  | -0.179  |
| RRoBERTa–Base             | 0.827   | 0.800   |
| FRoBERTa–Base             | 0.176   | 0.191   |
| PRoBERTa–Base (idf)       | -0.256  | -0.267  |
| RRoBERTa–Base (idf)       | 0.901   | 0.869   |
| FRoBERTa–Base (idf)       | 0.188   | 0.157   |
|                           |         |         |
| PRoBERTa–Large            | -0.105  | -0.041  |
| RRoBERTa–Large            | 0.888   | 0.863   |
| FRoBERTa–Large            | 0.322   | 0.350   |
| PRoBERTa–Large (idf)      | 0.063   | -0.011  |
| RRoBERTa–Large (idf)      | 0.917   | 0.889   |
| FRoBERTa–Large (idf)      | 0.519   | 0.453   |
| PRoBERTa–Large–MNLI       | 0.129   | 0.208   |
| RRoBERTa–Large–MNLI       | 0.820   | 0.823   |
|                           | 0.546   | 0.592   |
| FRoBERTa–Large–MNLI       |         |         |
| PRoBERTa–Large–MNLI (idf) | 0.081   | 0.099   |
| RRoBERTa–Large–MNLI (idf) | 0.906   | 0.875   |
| FRoBERTa–Large–MNLI (idf) | 0.605   | 0.596   |
| PXLNet–Base               | -0.046  | 0.080   |
| RXLNet–Base               | 0.409   | 0.506   |
| FXLNet–Base               | 0.146   | 0.265   |
| PXLNet–Base (idf)         | 0.006   | 0.145   |
| RXLNet–Base (idf)         | 0.655   | 0.720   |
| FXLNet–Base (idf)         | 0.270   | 0.391   |
|                           |         |         |
| PXLNet–Large              | -0.188  | -0.115  |
| RXLNet–Large              | 0.178   | 0.195   |
| FXLNet–Large              | -0.014  | 0.036   |
| PXLNet–Large (idf)        | -0.186  | -0.072  |
| RXLNet–Large (idf)        | 0.554   | 0.555   |
| FXLNet–Large (idf)        | 0.151   | 0.234   |
| PXLM–En                   | 0.230   | 0.220   |
|                           | 0.333   | 0.263   |
| RXLM–En                   |         |         |
| FXLM–En                   | 0.297   | 0.243   |
| PXLM–En (idf)             | 0.266   | 0.275   |
| RXLM–En (idf)             | 0.700   | 0.640   |
| FXLM–En (idf)             | 0.499   | 0.470   |

<span id="page-41-0"></span>Table 29: Pearson correlation on the 2015 COCO Captioning Challenge. The M1 and M2 measures are described in Section [4.](#page-4-0) We bold the best correlating task-specific and task-agnostic metrics in each setting LEIC uses images as additional inputs. Numbers with <sup>∗</sup> are cited from [Cui et al.](#page-9-11) [\(2018\)](#page-9-11).

| Type                                   | Method                    | QQP    | PAWSQQP |
|----------------------------------------|---------------------------|--------|---------|
| Trained on QQP (supervised)            | DecAtt                    | 0.939* | 0.263   |
|                                        | DIIN                      | 0.952* | 0.324   |
|                                        | BERT                      | 0.963* | 0.351   |
| Trained on QQP + PAWSQQP (supervised)  | DecAtt                    | -      | 0.511   |
|                                        | DIIN                      | -      | 0.778   |
|                                        | BERT                      | -      | 0.831   |
|                                        | BLEU-1                    | 0.737  | 0.402   |
|                                        | BLEU-2                    | 0.720  | 0.548   |
|                                        | BLEU-3                    | 0.712  | 0.527   |
|                                        | BLEU-4                    | 0.707  | 0.527   |
|                                        | METEOR                    | 0.755  | 0.532   |
|                                        | ROUGE-L                   | 0.740  | 0.536   |
|                                        | CHRF++                    | 0.577  | 0.608   |
|                                        | BEER                      | 0.741  | 0.564   |
|                                        | EED                       | 0.743  | 0.611   |
|                                        | CHARACTER                 | 0.698  | 0.650   |
|                                        | PBERT–Base                | 0.750  | 0.654   |
|                                        | RBERT–Base                | 0.739  | 0.655   |
|                                        | FBERT–Base                | 0.755  | 0.654   |
|                                        | PBERT–Base (idf)          | 0.766  | 0.665   |
|                                        | RBERT–Base (idf)          | 0.752  | 0.665   |
|                                        | FBERT–Base (idf)          | 0.770  | 0.664   |
|                                        | PBERT–Base–MRPC           | 0.742  | 0.615   |
|                                        | RBERT–Base–MRPC           | 0.729  | 0.617   |
|                                        | FBERT–Base–MRPC           | 0.746  | 0.614   |
|                                        | PBERT–Base–MRPC (idf)     | 0.752  | 0.618   |
|                                        | RBERT–Base–MRPC (idf)     | 0.737  | 0.619   |
|                                        | FBERT–Base–MRPC (idf)     | 0.756  | 0.617   |
|                                        | PBERT–Large               | 0.752  | 0.706   |
|                                        | RBERT–Large               | 0.740  | 0.710   |
|                                        | FBERT–Large               | 0.756  | 0.707   |
|                                        | PBERT–Large (idf)         | 0.766  | 0.713   |
|                                        | RBERT–Large (idf)         | 0.751  | 0.718   |
|                                        | FBERT–Large (idf)         | 0.769  | 0.714   |
| Metric (Not trained on QQP or PAWSQQP) | PRoBERTa–Base             | 0.746  | 0.657   |
|                                        | RRoBERTa–Base             | 0.736  | 0.656   |
|                                        | FRoBERTa–Base             | 0.751  | 0.654   |
|                                        | PRoBERTa–Base (idf)       | 0.760  | 0.666   |
|                                        | RRoBERTa–Base (idf)       | 0.745  | 0.666   |
|                                        | FRoBERTa–Base (idf)       | 0.765  | 0.664   |
|                                        | PRoBERTa–Large            | 0.757  | 0.687   |
|                                        | RRoBERTa–Large            | 0.744  | 0.685   |
|                                        | FRoBERTa–Large            | 0.761  | 0.685   |
|                                        | PRoBERTa–Large (idf)      | 0.773  | 0.691   |
|                                        | RRoBERTa–Large (idf)      | 0.757  | 0.697   |
|                                        | FRoBERTa–Large (idf)      | 0.777  | 0.693   |
|                                        | PRoBERTa–Large–MNLI       | 0.763  | 0.767   |
|                                        | RRoBERTa–Large–MNLI       | 0.750  | 0.772   |
|                                        | FRoBERTa–Large–MNLI       | 0.766  | 0.770   |
|                                        | PRoBERTa–Large–MNLI (idf) | 0.783  | 0.756   |
|                                        | RRoBERTa–Large–MNLI (idf) | 0.767  | 0.764   |
|                                        | FRoBERTa–Large–MNLI (idf) | 0.784  | 0.759   |
|                                        | PXLNet–Base               | 0.737  | 0.603   |
|                                        | RXLNet–Base               | 0.731  | 0.607   |
|                                        | FXLNet–Base               | 0.739  | 0.605   |
|                                        | PXLNet–Base (idf)         | 0.751  | 0.625   |
|                                        | RXLNet–Base (idf)         | 0.743  | 0.630   |
|                                        | FXLNet–Base (idf)         | 0.751  | 0.626   |
|                                        | PXLNet–Large              | 0.742  | 0.593   |
|                                        | RXLNet–Large              | 0.734  | 0.598   |
|                                        | FXLNet–Large              | 0.744  | 0.596   |
|                                        | PXLNet–Large (idf)        | 0.759  | 0.604   |
|                                        | RXLNet–Large (idf)        | 0.749  | 0.610   |
|                                        | FXLNet–Large (idf)        | 0.760  | 0.606   |
|                                        | PXLM–En                   | 0.734  | 0.600   |
|                                        | RXLM–En                   | 0.725  | 0.604   |
|                                        | FXLM–En                   | 0.737  | 0.602   |
|                                        | PXLM–En (idf)             | 0.757  | 0.596   |
|                                        | RXLM–En (idf)             | 0.745  | 0.603   |
|                                        | FXLM–En (idf)             | 0.759  | 0.600   |

<span id="page-42-0"></span>Table 30: Area under ROC curve (AUC) on QQP and PAWSQQP datasets. The scores of trained DecATT [\(Parikh et al., 2016\)](#page-11-14), DIIN [\(Gong et al., 2018\)](#page-9-15), and fine-tuned BERT are reported by [Zhang](#page-13-0) [et al.](#page-13-0) [\(2019\)](#page-13-0). We bold the best task-specific and task-agnostic metrics. Numbers with <sup>∗</sup> are scores on the held-out test set of QQP.