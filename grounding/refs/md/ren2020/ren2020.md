# CodeBLEU: a Method for Automatic Evaluation of Code Synthesis

Shuo Ren<sup>1</sup> , Daya Guo<sup>2</sup> , Shuai Lu<sup>3</sup> , Long Zhou<sup>4</sup> , Shujie Liu<sup>4</sup> , Duyu Tang<sup>4</sup> , Neel Sundaresan<sup>4</sup> , Ming Zhou<sup>4</sup> , Ambrosio Blanco<sup>4</sup> , Shuai Ma<sup>1</sup>

<sup>1</sup>SKLSDE Lab, Beihang University; Beijing Advanced Innovation Center for Big Data and Brain Computing

<sup>2</sup>Sun Yat-sen University <sup>3</sup>Peking University <sup>4</sup>Microsoft

<sup>1</sup>{shuoren, mashuai}@buaa.edu.cn <sup>2</sup>guody5@mail2.sysu.edu.cn <sup>3</sup> lushuai96@pku.edu.cn

<sup>4</sup>{Long.Zhou, shujliu, dutang, neels, mingzhou, ambrob}@microsoft.com

#### Abstract

Evaluation metrics play a vital role in the growth of an area as it defines the standard of distinguishing between good and bad models. In the area of code synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but they are not suitable enough to evaluate codes, because BLEU is originally designed to evaluate natural language, neglecting important syntactic and semantic features of codes, and perfect accuracy is too strict thus it underestimates different outputs with the same semantic logic. To remedy this, we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the strength of BLEU in the n-gram match, and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow. We conduct experiments by evaluating the correlation coefficient between CodeBLEU and quality scores assigned by the programmers on three code synthesis tasks, i.e., text-to-code, code translation, and code refinement. Experimental results show that, our proposed Code-BLEU can achieve a better correlation with programmer assigned scores compared with BLEU and accuracy.

### 1 Introduction

A suitable evaluation metric is important to push forward the research of an area, such as BLEU [\(Papineni et al. 2002\)](#page-7-0) and ROUGE [\(Lin 2004\)](#page-7-1) for machine translation and text summarization. Along with the rapid progress of code synthesis such as text-to-code synthesis, code translation and code change prediction [\(Karaivanov, Raychev, and Vechev](#page-7-2) [2014;](#page-7-2) [Oda et al. 2015;](#page-7-3) [Barone and Sennrich 2017;](#page-7-4) [Chen,](#page-7-5) [Liu, and Song 2018;](#page-7-5) [Kanade et al. 2019;](#page-7-6) [Husain et al. 2019;](#page-7-7) [Feng et al. 2020;](#page-7-8) [Dinella et al. 2020;](#page-7-9) [Lachaux et al. 2020\)](#page-7-10), different automatic evaluation methods for code synthesis are leveraged, including n-gram accuracy [\(Karaivanov, Ray](#page-7-2)[chev, and Vechev 2014\)](#page-7-2), perfect accuracy [\(Chen, Liu, and](#page-7-5) [Song 2018\)](#page-7-5), and computational accuracy [\(Lachaux et al.](#page-7-10) [2020\)](#page-7-10). The n-gram accuracy (e.g. 4-gram BLEU) is the most popular evaluation method for code synthesis [\(Karaivanov,](#page-7-2) [Raychev, and Vechev 2014;](#page-7-2) [Barone and Sennrich 2017\)](#page-7-4), based on the token overlapping between the hypothesis and the reference. The perfect accuracy calculates the percentage of the predicted target programs that are exactly the same as the ground truth [\(Chen, Liu, and Song 2018\)](#page-7-5). The recently proposed computational accuracy [\(Lachaux et al.](#page-7-10) [2020\)](#page-7-10), evaluates whether the hypothesis function generates the same outputs as the reference given the same inputs.

However, the above evaluation approaches still face many drawbacks. First, the n-gram accuracy does not take into account the grammatical and logical correctness, resulting in favoring candidates with high n-gram accuracy and serious logical errors. Second, the perfect accuracy is too strict, and underestimates different outputs with the same semantic logic. Third, the computational accuracy is weak in universality and practicability, since it should be designed for different programming languages, as well as specific compilers and the desired computing resource.

In order to deal with that, in this paper, we propose a new evaluation metric CodeBLEU, considering information from not only the shallow (n-gram) match, but also the syntactic match and the semantic match. More specifically, the n-gram match assigns different weights for different n-grams, the syntactic match considers the abstract syntax tree (AST) information in the evaluation score by matching the sub-trees, and the semantic match uses data-flow structure to measure the semantic similarity. CodeBLEU is a weighted combination of the original BLEU, the weighted n-gram match, the syntactic AST match, and the semantic data-flow match.

We conduct massive experiments to evaluate the effectiveness of CodeBLEU and the correlation coefficient between CodeBLEU scores and human evaluation scores in three code synthesis tasks including text-to-code synthesis, code translation, and code refinement. Experimental results demonstrate that CodeBLEU can significantly differentiate the systems' performance and achieve better correlation with the quality scores given by programmers than the popularly used BLEU. We hope that our proposed CodeBLEU can accelerate the R&D cycle of code synthesis tasks.

### 2 Why not BLEU?

In this section we will briefly introduce BLEU, and analyze its merits and demerits when applying it to code synthesis.

#### 2.1 BLEU for Machine Translation

Machine translation, which uses computers to realize automatic translation between languages, is first proposed by Warren Weaver as early as 1949 [\(Weaver 1955\)](#page-7-11). Since then, machine translation quality has not significantly improved

Copyright c 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

until the automatic evaluation metric (BLEU) is proposed in 2002 [\(Papineni et al. 2002\)](#page-7-0). The appearance of BLEU makes it possible to automatically train and optimize the machine translation systems and speeds up the research process of machine translation.

BLEU measures how well a candidate translation matches a set of translation references by calculating the percentage of n-grams overlapped between them. Besides, the brevity penalty is introduced to punish the candidates with a very short length, so it is hard for the MT system to cheat the evaluation metric by finding a way to change the output that the BLEU score goes up, but the translation quality doesn't.

#### 2.2 Code vs Natural Language

Although the BLEU achieves great success in the evaluation of machine translation and greatly encourages the research in this area, BLEU is not suitable for the evaluation of code synthesis without considering the characteristics of the programming language. A natural language is any language that has evolved naturally in humans through use and repetition, but code is artificially designed to produce various kinds of output. There are three big differences between them.

(1) Limited keywords vs. millions of words. Different from natural languages with a huge vocabulary, code is designed by humans and uses a small number of keywords, i.e., the reserved words of programming languages. Intuitively, keywords are more important than other words and the keywords match should gain a higher score.

(2) Tree structure vs. sequential structure. Humans usually speak and write from left to right, and the current mainstream models usually process natural languages as a sequence [\(Zhou et al. 2019\)](#page-7-12), such as end-to-end neural machine translation [\(Sutskever, Vinyals, and Le 2014;](#page-7-13) [Bah](#page-7-14)[danau, Cho, and Bengio 2014;](#page-7-14) [Vaswani et al. 2017\)](#page-7-15). In contrast, code has a natural tree structure and needs to be compiled according to their abstract syntax tree [\(Rabinovich,](#page-7-16) [Stern, and Klein 2017\)](#page-7-16). Therefore, how to evaluate the syntactic structure of code becomes particularly important.

(3) Unique instructions vs. ambiguous semantic. Word sense disambiguation is a basic research problem in natural language processing, because natural languages usually have ambiguous and variable semantic. However, code design is required to be unique, standardized and systematic, with unique and fixed instructions. This feature makes it possible to evaluate the semantics of the code.

In summary, code is significantly different from natural languages, and BLEU is not suitable for code synthesis evaluation only considering the token match and ignoring the importance of keywords, syntactic accuracy, and semantic correctness. Therefore, we propose a new evaluation metric CodeBLEU, which will be introduced in the following.

### 3 CodeBLEU

In order to pay attention to the keywords, leverage the tree structure and consider the semantic logic information, we propose a new evaluation metric CodeBLEU defined as the weighted combination of four parts as shown in Figure [1:](#page-2-0)

<span id="page-1-1"></span>
$$\begin{aligned} \text{CodeBLEU} &= \alpha \cdot \text{BLEU} + \beta \cdot \text{BLEU}\_{\text{weight}} \\ &+ \gamma \cdot \text{Match}\_{\text{ast}} + \delta \cdot \text{Match}\_{\text{df}} \end{aligned} \quad (1)$$

where BLEU is calculated by standard BLEU [\(Papineni](#page-7-0) [et al. 2002\)](#page-7-0), BLEUweight is the weighted n-gram match, obtained by comparing the hypothesis code and the reference code tokens with different weights (Sec. [3.1\)](#page-1-0), Matchast is the syntactic AST match, exploring the syntactic information of code (Sec. [3.2\)](#page-2-1), and Matchdf is the semantic dataflow match, considering the semantic similarity between the hypothesis and the reference (Sec. [3.3\)](#page-2-2). The weighted ngram match and the syntactic AST match are used to measure grammatical correctness, and the semantic data-flow match is used to calculate logic correctness.

#### <span id="page-1-0"></span>3.1 Weighted N-Gram Match

The original BLEU compares n-grams between the candidate and the reference, and calculates the ratio of matched n-grams. Compared with natural languages which a huge vocabulary and a free word order, programming languages are manually designed and have only a few keywords such as "int", "public" and so on. Applying the traditional BLEU directly to code synthesis will ignore the importance of the keywords. Hence, we introduce the weighted n-gram match to assign different weights for different n-grams, so that the keywords may have higher weights, as shown in Figure [1.](#page-2-0)

The weighted n-gram match precision is computed as:

$$p\_n = \frac{\sum\_{C \in \text{Candidates}} \sum\_{i=1}^l \mu\_n^i \cdot \text{Count}\_{\text{clip}}(C(i, i+n))}{\sum\_{C' \in \text{Candidates}} \sum\_{i=1}^l \mu\_n^i \cdot \text{Count}(C'(i, i+n))} \quad (2)$$

where n means the length of the n-gram, C(i, i + n) is the n-gram from the position i to the position i + n, and Countclip(C(i, i + n)) is the maximum number of n-grams co-occurring in a candidate code and a set of reference codes. µ i <sup>n</sup> denotes the weights of different keywords or ngram. In this paper, µ i <sup>n</sup> of the keywords is 5 times the weights of other tokens. Next, following the brevity penalty of original BLEU, we also compute the brevity penalty BP:

$$\text{BP} = \begin{cases} 1 & \text{if } c > r \\ e^{1 - r/c} & \text{if } c \le r \end{cases}$$

where c is the length of the candidate code and r is the effective reference corpus length. The weighted n-gram match score is calculated as:

$$\text{BLEU}\_{\text{weight}} = \text{BP} \cdot \exp(\sum\_{n=1}^{N} w\_n \log p\_n) \tag{3}$$

In our paper, the keywords are only considered in the unigrams, so N and w<sup>n</sup> are equal to 1. Note that a keywords list is predefined for each programming language.

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

Figure 1: The proposed CodeBLEU, a weighted syntactic and semantic BLEU for code synthesis evaluation, consists of the original BLEU, the weighted n-gram match, the syntactic AST match, and the semantic data-flow match.

#### <span id="page-2-1"></span>3.2 Syntactic AST Match

In addition to the sequence-level matching, we also consider the syntactic information in CodeBLEU by matching the tree structure. Different from natural language, programming language has natural tree structures, such as the abstract syntax tree (AST). AST is a tree representation of the abstract syntactic structure of programming languages. We can obtain all the sub-trees of the tree-sitter parsing result[1](#page-2-3) , then calculate the accuracy by comparing the candidate and reference sub-trees. In AST, each node denotes a construct occurring in the source code. The leaves of AST represent the names of the function and all the variables. However, we just want to use the syntactic structure of the codes, and the naming is not important, thus we leave out all the leave nodes in the original AST trees.

As shown in the middle part of Figure 1, we extract all the sub-trees of the candidate and the reference ASTs respectively. Then we calculate the syntactic AST match score as:

$$\text{Match}\_{\text{ast}} = \text{Count}\_{\text{clip}}(\text{T}\_{\text{cand}}) / \text{Count}(\text{T}\_{\text{ref}}) \qquad (4)$$

where Count(Tref) is the total number of the reference subtrees, and Countclip(Tcand) is the number of the candidate subtrees that are matched the reference. This score can evaluate code quality from a syntactic perspective, because grammatical errors such as token missing, data type errors can be captured by the difference between their ASTs.

#### <span id="page-2-2"></span>3.3 Semantic Data-flow Match

In programming languages, the semantic of source code is highly relevant to the dependency relations among variables. Taking Figure [2](#page-2-4) as an example, the function is to calculate the mean value of an array. Although the difference between the candidate and the reference is subtle (return y → return x), their semantics are completely different. However, the weighted n-gram match and the syntactic AST match still give a high score since the two pieces of codes have the same AST and their tokens are highly overlapped. Therefore, we also consider the semantic information in CodeBLEU. We use data-flow [\(Guo et al. 2020\)](#page-7-17) to represent a source code as a graph, in which nodes represent variables and edges represent where the value of each variable comes from. Unlike AST, data-flows of the two codes are different in Figure [2](#page-2-4) since their return values come from x and y respectively. Such a semantic graph can be used to measure the semantic match between the candidate and the reference.

<span id="page-2-4"></span>

Figure 2: BLEU: 95.47; Matchast: 100.

Based on the above, there are three steps to compute the semantic data-flow match score.

Step 1: Obtain the data-flow graphs for the candidate and the reference. Based on AST, we first utilize the leaves to identify variable sequence, denoted as V = {v0, v1, ..., vm}. We then take each variable as a node of the graph and a directed edge = hv<sup>i</sup> , v<sup>j</sup> i from v<sup>i</sup> to v<sup>j</sup> refers that the value of j-th variable comes from i-th variable. The graph G(C) = (V ; E) is used to represent relations among variables of the code C, as shown by the red arrows in Figure [1.](#page-2-0)

Step 2: Normalize data-flow items. For simplicity and unity, we ignore the variable position and normalize their names. We collect all the variables in the data-flow items and rename them var i, where i is the order of the variables appearing in all data-flow items.

Step 3: Calculate the semantic data-flow match score as:

Matchdf = Countclip(DFcand)/Count(DFref) (5) where Count(DFref) is the total number of the reference data-flows, and Countclip(DFcand) is the number of

<span id="page-2-3"></span><sup>1</sup> https://github.com/tree-sitter/tree-sitter

matched candidate data-flows.

<span id="page-3-0"></span>

Figure 3: Example 1. BLEU: 75.43; CodeBLEU: 69.73.

### 3.4 Two Examples

Here we will give two toy examples to show how to calculate CodeBLEU. Meanwhile, we show the qualitative advantages of CodeBLEU compared with the traditional BLEU score.

Example 1 The output candidate of a code synthesis system and the according reference are shown in Figure [3.](#page-3-0)

In this example, there are four differences between the candidate and the reference, which are stressed with the red color. They are (1) the conversion type of the return value ("float" vs. "int"); (2) the variable naming ("c" vs. "d"); (3) the type of a constant ("0.0" and "0"); (4) the missing token ("}") in the candidate. This toy example is designed based on the background that the data type, the variable naming and the token missing tend to cause problems in reality.

The CodeBLEU is calculated as follows: (1) First, we calculate the n-gram match score (BLEU, which is 75.43) given the candidate and the reference. (2) Then, we calculate the weighted n-gram match score for it. The weight assigned to the keywords "public, static, int, return, double" in the reference are 4 times more than that of the rest tokens. The resulting score is 74.91, lower than the BLEU score, penalizing the keyword error ("float" vs. "int"). (3) The number of all sub-trees of the reference AST generated by treesitter is 21 and the hit number for the candidate is 13, so the syntactic AST match score is 13/21 ∗ 100 = 61.90(%). The data type errors in the candidate are penalized by the AST mismatch. (4) Three data-flows can be extracted from the reference AST, which are "[('var 0', 'comesFrom', []), ('var 0', 'comesFrom', ['var 0'])], ('var 0', 'comesFrom', ['var 0'])]", corresponding to the three variables "d" in the reference. The first "d" comes from no parent because it is in the parameter list. The second and the third "d" come from the first "d". The variable names are normalized and their positions are ignored according to Section [3.3.](#page-2-2) However, we can only extract two data-flows from the candidate AST , i.e., "[('var 0', 'comesFrom', []), ('var 0', 'comesFrom', ['var 0'])]" corresponding to the two "d"s in this code. The variable "c" is used before declaration so no data-flow is extracted for it. Therefore the data-flow match score is 2/3 ∗ 100 = 66.67(%). With α, β, γ, δ = 0.25, 0.25, 0.25, 0.25, the final CodeBLEU score is 69.73, which is lower than BLEU because CodeBLEU penalizes the keyword and semantic errors for the programming languages.

Example 2 As shown in Figure [4,](#page-3-1) in this example, there is no difference between the candidate and the reference except for the names of the local variables ("c" vs. "d"). In the real scenario, the candidate is correct without doubt, and a human expert would give a score of 100. However, its

<span id="page-3-1"></span>

Figure 4: Example 2. BLEU: 68.14; CodeBLEU: 83.97.

BLEU score is only 75.71, which underestimates the quality of the candidate. With CodeBLEU, we have the weight ngram match score of 76.46, the syntactic AST match score of 100 and the semantic data-flow match score of 100, the final CodeBLEU score being 88.04, which makes up for the underestimation of BLEU.

From the two examples, we find that in some typical scenarios, CodeBLEU gives more reasonable scores than BLEU to evaluate the code synthesis output. In the experiment section, we will give the quantitative analysis, further showing the effectiveness of CodeBLEU.

## 4 Experiments

We conduct experiments on three code synthesis tasks, i.e., text-to-code (Java), code translation (from Java to C#) and code refinement (Java). Previous work of these tasks uses BLEU or perfect accuracy (exactly match) for evaluation. In this paper, we will take the proposed CodeBLEU as the evaluation metric to see if CodeBLEU is more reasonable. For each task, we calculate the Pearson correlation coefficient to check the correlation between the scores given by our proposed CodeBLEU and the scores assigned by programmers (human evaluation scores). In the following subsections, we will first introduce the three tasks we used. Then we will give details of our experiment settings. Next, the experimental results will be shown and discussed. Finally, we will do an ablation study and investigate the influence of different components of CodeBLEU to the final results.

### 4.1 Task Introduction

The three tasks we choose for the experiment are text-tocode, code translation, and code refinement.

Text-to-code Text-to-code [\(Iyer et al. 2018\)](#page-7-18) is the task of generating class member functions given the function documentation and the programmatic context. The inputs are the natural language documentation, and the class environment the code resides in. The environment comprises two lists of entities: (1) class member variable names with their data types, and (2) member function names together with their return types. The output is a piece of code of the desired class member function. We use the same dataset released by [Iyer et al.](#page-7-18) [\(2018\)](#page-7-18), which consists of 100k training samples, 2k validation samples and 2k test samples.

Code Translation Code translation aims to migrate legacy software from one programming language in a platform to another. Following [Nguyen, Nguyen, and Nguyen](#page-7-19) [\(2015\)](#page-7-19) and [Chen, Liu, and Song](#page-7-5) [\(2018\)](#page-7-5), we conduct experiments on a dataset crawled from several open-source projects, i.e.,

<span id="page-4-4"></span>

| Task | Text-to-code          | Code translation      | Code refinement       |
|------|-----------------------|-----------------------|-----------------------|
|      | Sys1 Seq2Seq          | PBSMT                 | LSTM                  |
|      | Sys2 Seq2Action+MAML1 | Transformer           | Transformer           |
|      | Sys3 GPT22            | Transformer+CodeBERT4 | Transformer+CodeBERT4 |
|      | Sys4 CodeGPT3         | Human                 | -                     |

Table 1: The systems we choose for each task. Note that "Human" in this table means the output is given by human programming experts. <sup>1</sup> [\(Guo et al. 2019\)](#page-7-20); <sup>2</sup> Fine-tune with GPT-2 [\(Radford et al. 2019\)](#page-7-21); <sup>3</sup> Pre-trained GPT-2 with the Java data of Codesearchnet [\(Husain et al. 2019\)](#page-7-7) and then fine-tuning; <sup>4</sup> Fine-tune with CodeBERT [\(Feng et al. 2020\)](#page-7-8).

Lucene[2](#page-4-0) , POI[3](#page-4-1) , JGit[4](#page-4-2) , and Antlr[5](#page-4-3) . Those projects have both Java and C# implementation. We paired the methods in the two languages based on their file names and method names. After removing duplication, the total number of method pairs is 11.8k, and we split 0.5k pairs from them as the development set and another 1k pairs for test. We will release the code translation dataset with our scripts.

Code Refinement Code refinement aims to automatically fix bugs in the code, which can contribute to reducing the cost of bug-fixing for developers. We use the dataset released by [Tufano et al.](#page-7-22) [\(2019\)](#page-7-22). The source is buggy Java functions while the target is the according fixed ones. Their dataset contains two subsets ( i.e. *small* and *medium*) based on the code length. For the *small* dataset, the function numbers of training, development and test samples are 46,680, 5,835 and 5,835. For the *medium* dataset, the function numbers are 52,364, 6,545 and 6,545 respectively.

#### 4.2 Settings

For each task, we prepare 3 to 4 standard systems as shown in Table [1.](#page-4-4) We randomly choose 500 samples from each test set for evaluation. As for human evaluation, we have a group of human judges consisting of 10 people who are familiar with Java and C#. The humans judge our four systems on a subset of 50 samples extracted randomly from our test set. We pair each input with its 4 outputs, resulting in a total of 200 pairs of the given inputs and the output codes. We prepare a UI software with these input-output pairs randomly ordered to disperse the 4 outputs of each input. All judges use this same software and see the pairs in the same order. They rated each output from 1 (very bad) to 5 (very good).

#### 4.3 Results

Main Results The main results are shown in Table [2.](#page-4-5) In this table, we calculate BLEU scores, perfect accuracy, CodeBLEU and human evaluation scores for all systems of each task on the selected test set. Note that the former three metrics are ranging from 0 to 100 and the last one is ranging from 1 (very bad) to 5 (very good). We find that some of the systems are very close in terms of BLEU and CodeBLEU scores. Hence, some questions are raised.

<span id="page-4-5"></span>

| Text-to-code     |       |       |       |                                             |  |  |
|------------------|-------|-------|-------|---------------------------------------------|--|--|
|                  |       |       |       | System BLEU Acc (100%) CodeBLEU Human score |  |  |
| Sys1             | 12.02 | 3.05  | 18.04 | 1.888                                       |  |  |
| Sys2             | 16.82 | 10.50 | 21.71 | 1.99                                        |  |  |
| Sys3             | 21.18 | 17.35 | 24.95 | 2.558                                       |  |  |
| Sys4             | 26.45 | 20.10 | 30.96 | 3.125                                       |  |  |
| Code translation |       |       |       |                                             |  |  |
|                  |       |       |       | System BLEU Acc (100%) CodeBLEU Human score |  |  |
| Sys1             | 44.53 | 13.2  | 45.71 | 3.25                                        |  |  |
| Sys2             | 54.84 | 31.75 | 61.14 | 3.771                                       |  |  |
| Sys3             | 80.18 | 60.2  | 82.74 | 4.036                                       |  |  |
| Sys4             | 81.14 | 63.5  | 84.75 | 4.252                                       |  |  |
| Code refinement  |       |       |       |                                             |  |  |
|                  |       |       |       | System BLEU Acc (100%) CodeBLEU Human score |  |  |
| Sys1             | 90.35 | 3.00  | 80.81 | 1.378                                       |  |  |
| Sys2             | 91.40 | 7.01  | 82.16 | 1.545                                       |  |  |
| Sys3             | 92.80 | 17.6  | 83.85 | 2.022                                       |  |  |
|                  |       |       |       |                                             |  |  |

Table 2: The results of all baselines of the given three tasks evaluated by BLEU, accuracy (exactly match), CodeBLEU and human evaluation scores.

- Is the difference in CodeBLEU metric reliable?
- What is the variance of the CodeBLEU score?
- Is CodeBLEU more correlated with human scores than BLEU and accuracy?

To answer these questions, first, following [Papineni et al.](#page-7-0) [\(2002\)](#page-7-0), we divided the test set into 20 blocks of 25 sentences each, and computed CodeBLEU on these blocks individually. We thus have 20 samples of these metrics for each system. We computed the means, variances, and paired tstatistics for them, which is displayed in Table [3.](#page-5-0)

From Table [3,](#page-5-0) as expected, these two sets of results are close for each system and differ only by small finite block size effects. Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems' scores are statistically very significant. The reported variance on 25 sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus. Therefore, we conclude that the difference in the CodeBLEU metric is reliable, and the variance of it is within a reasonable range.

Next, we compare the correlation of BLEU, accuracy and

<span id="page-4-0"></span><sup>2</sup> http://lucene.apache.org/

<span id="page-4-1"></span><sup>3</sup> http://poi.apache.org/

<span id="page-4-2"></span><sup>4</sup> https://github.com/eclipse/jgit/

<span id="page-4-3"></span><sup>5</sup> https://github.com/antlr/

<span id="page-5-0"></span>

|        | Text-to-code |        |     | Code translation |        |     | Code refinement |        |     |
|--------|--------------|--------|-----|------------------|--------|-----|-----------------|--------|-----|
| System | Mean         | StdDev | t   | Mean             | StdDev | t   | Mean            | StdDev | t   |
| Sys1   | 17.93        | 1.8    | -   | 44.62            | 5.2    | -   | 79.21           | 5.6    | -   |
| Sys2   | 20.67        | 2.9    | 7.4 | 60.04            | 5.8    | 30  | 81.04           | 5.8    | 2.1 |
| Sys3   | 23.92        | 3.4    | 7   | 81.55            | 6.1    | 38  | 82.52           | 6.4    | 3.4 |
| Sys4   | 30.13        | 4.2    | 12  | 83.26            | 6.7    | 5.2 | -               | -      | -   |

Table 3: The mean, standard deviation and paired t-statistic of all baselines of the given three tasks. The t-statistic compares each system with the neighbor above it in the table.

<span id="page-5-1"></span>

|                  | Text-to-code Code trans Code ref |        |        |
|------------------|----------------------------------|--------|--------|
| BLEU & human     | 0.967                            | 0.940  | 0.923  |
| Acc & human      | 0.912                            | 0.968  | 0.999  |
| CodeBLEU & human | 0.977                            | 0.970  | 0.979  |
|                  | (+1.0)                           | (+3.0) | (+5.6) |

Table 4: Comparison of the Pearson correlation coefficients between human evaluation scores and three different metrics. The numbers in the brackets in the last row are the improvements in percent compared with BLEU.

CodeBLEU to human evaluation scores respectively. The Pearson correlation coefficients are listed in Table [4.](#page-5-1)

From the table, we see CodeBLEU scores are more correlated with human evaluation scores in all the three tasks. The improvements are significant compared with the traditional MT metric BLEU. The results verify the effectiveness of our proposed metric. For text-to-code and code translation tasks, CodeBLEU scores are also more correlated with human scores than accuracy (Acc), but there is an exception that the Acc is more correlated for code refinement. This is because the data of refinement task is just fixing small bugs in a given Java function. The output is usually unique, and the humans score the outputs based on the unique refinement way, so that the Acc here correlates more with human evaluation scores. However, we also believe that in the more general code synthesis scenarios, CodeBLEU is more reasonable in terms of the correlation with human scores.

Figure [5](#page-5-2) shows the comparable regression results for each metric to human scores on the text-to-code and code translation tasks. The R<sup>2</sup> values of the linear regression are also shown in the figure. From the figure, we find CodeBLEU is more linearly correlated with human evaluation scores than BLEU, which is consistent with the results in Table [4.](#page-5-1)

Based on the above results and analysis, we conclude that:

- The difference in CodeBLEU metric is reliable. Code-BLEU is capable to differentiate code synthesis systems.
- CodeBLEU is reliable, and its variance is within a reasonable range.
- CodeBLEU is more correlated with human evaluation scores than traditional BLEU scores on all the three tasks, and more correlated than Acc on the two tasks.

<span id="page-5-2"></span>![](_page_5_Figure_11.jpeg)

Figure 5: BLEU and CodeBLEU predict human evaluation scores. (a) Text-to-code; (b) Code translation.

Ablation Study To investigate the influence of the different components of CodeBLEU, we conduct the following experiment to calculate the respective Pearson correlation between the human evaluation scores and the scores given by different components. The results are reported in Table [5.](#page-5-3)

<span id="page-5-3"></span>

|            | Components Text-to-code Code trans Code ref |       |       |
|------------|---------------------------------------------|-------|-------|
| BLEU       | 0.967                                       | 0.940 | 0.923 |
| BLEUweight | 0.960                                       | 0.934 | 0.985 |
| Matchast   | 0.985                                       | 0.977 | 0.967 |
| Matchdf    | 0.978                                       | 0.974 | 0.983 |
| CodeBLEU   | 0.977                                       | 0.970 | 0.979 |

Table 5: The Pearson correlation coefficients between different components of CodeBLEU and humans.

From the table, we find that, for the text-to-code and code translation tasks, the scores of the last two components, i.e., syntactic AST match and semantic data-flow match, are more relevant to human evaluation scores compared with the n-gram and weight n-gram match scores. For the code refinement task, the scores given by the weighted n-gram match and the semantic data-flow are more relevant to human evaluation. This may be because many bugs in the refinement training data are wrong variable naming or keywords errors, while the weighted n-gram and semantic data-flow match scores could evaluate them better. The above result verifies the effectiveness of our three proposed components, i.e., weighted n-gram match, syntactic AST match and semantic data-flow match, for code synthesis evaluation. Besides, the results are inspiring for us to change the hyper-parameters α, β, γ, δ in Eq. [\(1\)](#page-1-1) to get better evaluation whose results are more correlated with humans. For example, to achieve this, we can increase γ and δ to improve the weights of the last two components in the final CodeBLEU scores. In the next section, we will conduct experiments to investigate the influence of the four hyper-parameters.

#### 4.4 Influence of hyper-parameters

In the above subsection, we find different components have a different influence on the final results of CodeBLEU in terms of the correlation with human evaluation scores. Therefore, we can change the weights of those components to achieve a higher correlation between CodeBLEU and human evaluation. We gradually increase the weights of the last two components (as in Table [6\)](#page-6-0) and record the correlation coefficients between CodeBLEU and human evaluation scores for the three tasks. The results are shown in Figure [6.](#page-6-1)

From the figure, we find that increasing the weights of the last two components improves the correlation between CodeBLEU and human scores for all of the three tasks. The performance starts to converge after the combination [4] and the combination [7], i.e., α, β, γ, δ = 0.1, 0.1, 0.4, 0.4, achieves the best result among all the combinations in Figure [6](#page-6-1) (0.981, 0.975, 0.980 for the three tasks respectively). Of course, [7] is not the best combination all the time. For example, α, β, γ, δ = 0.1, 0.4, 0.1, 0.4 achieves the better result (the correlation coefficient is 0.984) than the combination [7] (the correlation coefficient is 0.980) for the code refinement task. In spite of this, we recommend to choose the combination [7] when calculating CodeBLEU for general code synthesis tasks, because the last two components are more likely to be more correlated with human evaluation scores from the instinct given by Table [4.](#page-5-1)

<span id="page-6-1"></span>![](_page_6_Figure_4.jpeg)

Figure 6: The correlation coefficients between CodeBLEU and human scores with different hyper-parameters. The hyper-parameter setting of each combination is in Table [6.](#page-6-0)

<span id="page-6-0"></span>

| Combination | α, β, γ, δ             |
|-------------|------------------------|
| [1]         | 0.40, 0.40, 0.10, 0.10 |
| [2]         | 0.35, 0.35, 0.15, 0.15 |
| [3]         | 0.30, 0.30, 0.20, 0.20 |
| [4]         | 0.25, 0.25, 0.25, 0.25 |
| [5]         | 0.20, 0.20, 0.30, 0.30 |
| [6]         | 0.15, 0.15, 0.35, 0.35 |
| [7]         | 0.10, 0.10, 0.40, 0.40 |

Table 6: The settings of each combination in Figure [6.](#page-6-1)

### 5 Related Work

As code artificial intelligence receives more and more attention [\(Allamanis et al. 2015;](#page-7-23) [Yin and Neubig 2017;](#page-7-24) [Al](#page-6-2)[lamanis et al. 2018;](#page-6-2) [Monperrus 2018;](#page-7-25) [Alon et al. 2019;](#page-7-26) [Svyatkovskiy et al. 2020\)](#page-7-27), the evaluation of code synthesis becomes critical to promote its development. Although there are several automatic evaluation methods, which can be used to evaluate code synthesis [\(Karaivanov, Raychev,](#page-7-2) [and Vechev 2014;](#page-7-2) [Chen, Liu, and Song 2018;](#page-7-5) [Lachaux et al.](#page-7-10) [2020\)](#page-7-10), these approaches still suffer from many weakness and are not suitable to evaluate code.

The widely used 4-gram BLEU [\(Papineni et al. 2002\)](#page-7-0) evaluates the code quality by using the relative overlap between the tokens in the hypothesis and reference [\(Karaivanov, Raychev, and Vechev 2014;](#page-7-2) [Barone and Sen](#page-7-4)[nrich 2017\)](#page-7-4). Nevertheless, BLEU ignores the grammatical correctness and logic correctness. The perfect accuracy [\(Ra](#page-7-16)[binovich, Stern, and Klein 2017;](#page-7-16) [Chen, Liu, and Song 2018\)](#page-7-5) is too strict and it is an underestimation of the true accuracy based on semantic equivalence. Additionally, the computational accuracy [\(Lachaux et al. 2020\)](#page-7-10), evaluating whether the hypothesis function generates the same outputs given the same inputs by performing code, locks universality and practicability. To overcome the limitation, our proposed simple and effective CodeBLEU can not only consider the surface match similar with the original BLEU, but can also consider the grammatical correctness and the logic correctness.

### 6 Conclusion

In this paper, we propose a novel metric CodeBLEU for code synthesis evaluation. CodeBLEU evaluates the candidate code pieces considering not only the shallow match, but also the syntactic match and the semantic match. The results of three real-world tasks, i.e. text-to-code, code translation and code refinement, demonstrate the rationality and effectiveness of CodeBLEU by analyzing the correlation with human evaluation scores from different granularity. In the future work, we will delve more into the evaluation of syntactic and semantic match and try more tasks with CodeBLEU to show its practicality.

### References

<span id="page-6-2"></span>Allamanis, M.; Barr, E. T.; Devanbu, P.; and Sutton, C. 2018. A survey of machine learning for big code and naturalness. *ACM Computing Surveys (CSUR)* 51(4): 1–37.

<span id="page-7-23"></span>Allamanis, M.; Tarlow, D.; Gordon, A.; and Wei, Y. 2015. Bimodal modelling of source code and natural language. In *International conference on machine learning*, 2123–2132.

<span id="page-7-26"></span>Alon, U.; Zilberstein, M.; Levy, O.; and Yahav, E. 2019. code2vec: Learning distributed representations of code. *Proceedings of the ACM on Programming Languages* 3(POPL): 1–29.

<span id="page-7-14"></span>Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. *arXiv preprint arXiv:1409.0473* .

<span id="page-7-4"></span>Barone, A. V. M.; and Sennrich, R. 2017. A parallel corpus of Python functions and documentation strings for automated code documentation and code generation. *arXiv preprint arXiv:1707.02275* .

<span id="page-7-5"></span>Chen, X.; Liu, C.; and Song, D. 2018. Tree-to-tree neural networks for program translation. In *Advances in neural information processing systems*, 2547–2557.

<span id="page-7-9"></span>Dinella, E.; Dai, H.; Li, Z.; Naik, M.; Song, L.; and Wang, K. 2020. Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs. In *International Conference on Learning Representations*.

<span id="page-7-8"></span>Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.; Shou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. Codebert: A pre-trained model for programming and natural languages. *arXiv preprint arXiv:2002.08155* .

<span id="page-7-17"></span>Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Liu, S.; Zhou, L.; Duan, N.; Yin, J.; Jiang, D.; et al. 2020. GraphCode-BERT: Pre-training Code Representations with Data Flow. *arXiv preprint arXiv:2009.08366* .

<span id="page-7-20"></span>Guo, D.; Tang, D.; Duan, N.; Zhou, M.; and Yin, J. 2019. Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing. *arXiv preprint arXiv:1906.07108* .

<span id="page-7-7"></span>Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and Brockschmidt, M. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. *arXiv preprint arXiv:1909.09436* .

<span id="page-7-18"></span>Iyer, S.; Konstas, I.; Cheung, A.; and Zettlemoyer, L. 2018. Mapping Language to Code in Programmatic Context. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, 1643–1652.

<span id="page-7-6"></span>Kanade, A.; Maniatis, P.; Balakrishnan, G.; and Shi, K. 2019. Pre-trained contextual embedding of source code. *arXiv preprint arXiv:2001.00059* .

<span id="page-7-2"></span>Karaivanov, S.; Raychev, V.; and Vechev, M. 2014. Phrasebased statistical translation of programming languages. In *Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software*, 173–184.

<span id="page-7-10"></span>Lachaux, M.-A.; Roziere, B.; Chanussot, L.; and Lample, G. 2020. Unsupervised Translation of Programming Languages. *arXiv preprint arXiv:2006.03511* .

<span id="page-7-1"></span>Lin, C.-Y. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In *Text Summarization Branches Out*, 74–81. Barcelona, Spain: Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/W04-](https://www.aclweb.org/anthology/W04-1013) [1013.](https://www.aclweb.org/anthology/W04-1013)

<span id="page-7-25"></span>Monperrus, M. 2018. Automatic software repair: a bibliography. *ACM Computing Surveys (CSUR)* 51(1): 1–24.

<span id="page-7-19"></span>Nguyen, A. T.; Nguyen, T. T.; and Nguyen, T. N. 2015. Divide-and-conquer approach for multi-phase statistical migration for source code (t). In *2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)*, 585–596. IEEE.

<span id="page-7-3"></span>Oda, Y.; Fudaba, H.; Neubig, G.; Hata, H.; Sakti, S.; Toda, T.; and Nakamura, S. 2015. Learning to generate pseudocode from source code using statistical machine translation (t). In *2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)*, 574–584. IEEE.

<span id="page-7-0"></span>Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. BLEU: a method for automatic evaluation of machine translation. In *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*, 311–318.

<span id="page-7-16"></span>Rabinovich, M.; Stern, M.; and Klein, D. 2017. Abstract syntax networks for code generation and semantic parsing. *arXiv preprint arXiv:1704.07535* .

<span id="page-7-21"></span>Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsupervised multitask learners. *OpenAI Blog* 1(8): 9.

<span id="page-7-13"></span>Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In *Advances in neural information processing systems*, 3104–3112.

<span id="page-7-27"></span>Svyatkovskiy, A.; Deng, S. K.; Fu, S.; and Sundaresan, N. 2020. IntelliCode Compose: Code Generation Using Transformer. *arXiv preprint arXiv:2005.08025* .

<span id="page-7-22"></span>Tufano, M.; Watson, C.; Bavota, G.; Penta, M. D.; White, M.; and Poshyvanyk, D. 2019. An empirical study on learning bug-fixing patches in the wild via neural machine translation. *ACM Transactions on Software Engineering and Methodology (TOSEM)* 28(4): 1–29.

<span id="page-7-15"></span>Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In *Advances in Neural Information Processing Systems*, 6000–6010.

<span id="page-7-11"></span>Weaver, W. 1955. Translation. *Machine translation of languages* 14(15-23): 10.

<span id="page-7-24"></span>Yin, P.; and Neubig, G. 2017. A Syntactic Neural Model for General-Purpose Code Generation. In *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 440–450. Vancouver, Canada: Association for Computational Linguistics.

<span id="page-7-12"></span>Zhou, L.; Zhang, J.; Zong, C.; and Yu, H. 2019. Sequence generation: From both sides to the middle. In *Proceedings of IJCAI 2019*.